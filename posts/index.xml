<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Oppenheimer&#39;s BLOG</title>
    <link>https://russellgeum.github.io/posts/</link>
    <description>Recent content in Posts on Oppenheimer&#39;s BLOG</description>
    <image>
      <title>Oppenheimer&#39;s BLOG</title>
      <url>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 13 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://russellgeum.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기</title>
      <link>https://russellgeum.github.io/posts/2024-02-13/</link>
      <pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2024-02-13/</guid>
      <description>Orion-14B 본 포스팅은 Orion-14B-Chat을 기준으로 한다.
llama.cpp Orion-14B 모델 Orion-14B-Chat in HuggingFace 추론 환경 CMake 설치 brew install cmake llama.cpp 환경 클론 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp llama.cpp 환경 빌드 mkdir build cd build cmake .. cmake --build . --config Release Orion-14B 모델 다운로드 허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation.utils import GenerationConfig tokenizer = AutoTokenizer.from_pretrained(&amp;#34;OrionStarAI/Orion-14B&amp;#34;, use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM.</description>
    </item>
    <item>
      <title>[논문] Survey: Large Multimodal Models</title>
      <link>https://russellgeum.github.io/posts/2024-02-04/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2024-02-04/</guid>
      <description>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장</description>
    </item>
    <item>
      <title>[논문] Survey: Efficient Large Language Models</title>
      <link>https://russellgeum.github.io/posts/2024-01-07/</link>
      <pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2024-01-07/</guid>
      <description>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023</description>
    </item>
    <item>
      <title>[생각] 2023년 회고</title>
      <link>https://russellgeum.github.io/posts/2023-12-11/</link>
      <pubDate>Mon, 11 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-12-11/</guid>
      <description>1. 졸업과 취업 졸업 (1분기) 석사 디펜스를 끝내고, 완전히 학교를 떠났다. 포항과 서울을 2주마다 오가곤 했으니, 타지에서 지내는 외로움이 컸다. 그렇지만 개개인 퍼포먼스가 훌륭한 연구실 친구들은 나에게 큰 자산이다. 연구적으로 디스커션도 많이하고, 같은 업계에 있으니 서로 동향을 알기 좋은 사람들이다.
일전에도 말한바 있지만, 학위로 얻은 것은 다음과 같이 정리한다.
내가 무엇을 공부를 하든 스스로 커리큘럼을 설계하고, 학습할 수 있는 능력 체계적, 논리적으로 고민하고 그 결과를 계층적 구조로 작문하는 능력 나는 연구가 잘 풀린 사람은 아니다.</description>
    </item>
    <item>
      <title>[생각] 재능있는 척 하지 않기</title>
      <link>https://russellgeum.github.io/posts/2023-11-12/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-11-12/</guid>
      <description>재능있는 척 하지 않기 재능 있는 척 하지 않기
이 글은 브랜치의 &amp;ldquo;향로&amp;quot;님의 글을 보고 나에게 대입하여 재정리하였다.
요약하자면 작가는 개발을 잘 하지 못했던 시절에,
따로 공부했다는 사실을 주변에 알리고 싶지 않아했다.
왜냐하면 못한 성과에 대해서 재능이 없는 것이 아니라,
노력이 부족했다는 면피성 명분을 만들수 있기 때문이었다.
나의 이야기 올 여름~가을 나는 회사에서 (개인적으로 스스로가 너무 절망적이었던)
퍼포먼스가 너무 좋지 못한 태스크를 수행중이었다.
나는 &amp;ldquo;내가 잘 모른다. 어렵다. 그러니 나를 도와주었으면 좋겠다.</description>
    </item>
    <item>
      <title>[논문] ICCV 2023 관심 논문 리스트업</title>
      <link>https://russellgeum.github.io/posts/2023-10-03/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-10-03/</guid>
      <description>ICCV 2023 ICCV 2023 Link
Papers ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.
(일부 특이한 연구도 포함)
Neural Radiance Fields NeRF-MS: Neural Radiance Fields with Multi-Sequence
Peihao Li et al.
Re-ReND: Real-time Rendering of NeRFs across Devices
Sara Rojas et al.
CLNeRF: Continual Learning Meets NeRF
Zhipeng Cai, Matthias Muller
Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
Hansheng Chen et al.
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models Compression</title>
      <link>https://russellgeum.github.io/posts/2023-08-29/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-29/</guid>
      <description>대규모 언어 모델의 경량화 동향 Abstract LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬 LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함 Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌 압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함 Introduction &amp;amp; Method 대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다.</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models</title>
      <link>https://russellgeum.github.io/posts/2023-08-28/</link>
      <pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-28/</guid>
      <description>LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models</description>
    </item>
    <item>
      <title>[생각] 나의 문제점</title>
      <link>https://russellgeum.github.io/posts/2023-08-12/</link>
      <pubDate>Sat, 12 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-12/</guid>
      <description>나의 문제점 0. 왜 이 글을 쓰는가? 짧고, 긴 시간 동안 나하고 같이 지낸 소중한 사람들로부터 몇 가지 피드백과 조언을 얻을 때가 있다. 그러한 조언 중 공통된 문장을 정리해서 내가 어떤 단점이 있는지 생각하고 이를 개선할 방법을 고민한다.
1. 나는 다른 사람에게 관심이 없다. 나는 다른 사람에게 관심이 없다. 이것은 타고나는 성향일 가능성이 크다. 갑자기 남에게 관심을 많이 가지라고 하는 것은 고문에 가깝다. 그렇지만 다른 사람과의 지속적인 커뮤니케이션을 위한다면, &amp;ldquo;관심있는 척&amp;rdquo; 이라도 해본다.</description>
    </item>
    <item>
      <title>[독서] 최재천의 공부</title>
      <link>https://russellgeum.github.io/posts/2023-07-29/</link>
      <pubDate>Sat, 29 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-29/</guid>
      <description>어떻게 배우며 살 것인가? 이 책에 담긴 최재천 교수와 안희경 저널리스트 간의 대화록은 두 가지 관점으로 이야기가 흐른다. 하나는 &amp;ldquo;개인 차원에서의 학습&amp;rdquo; 이고, 둘째는 &amp;ldquo;사회 차원에서의 교육&amp;rdquo; 이다. 사회 차원에서의 교육도 겉으로는 &amp;ldquo;어떻게 가르칠까?&amp;rdquo; 의 미래적 고민을 담지만, 그렇다고 명확한 교수법을 딱 가이드라인을 세울 수는 없다. 하지만 어느정도 적절한 방법은 있을 수 있다. 그 중 저자가 추구하는 방법은 여러 사람의 생각과 경험이 한데 섞여, 어울려 학습하는 현장을 만드는 것이다.
하지만 올바른 교육법도 교육 받는 주체가 &amp;ldquo;바른 학습&amp;rdquo; 이 가능하다는 가정에서, 어떻게 그것을 지속할지의 해답에 불과하다고 생각한다.</description>
    </item>
    <item>
      <title>[논문] ICML 2023 추론 최적화 관련 논문 리스트업</title>
      <link>https://russellgeum.github.io/posts/2023-07-25/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-25/</guid>
      <description>ICML 2023 Papers ICML 2023이 열리고 있다.
Distillation, Quantization, HW-aware Deep Learning 위주로 트래킹 중이다.
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models
Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan
DIVISION: Memory Efficient Training via Dual Activation Precision
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu
Fast Private Kernel Density Estimation via Locality Sensitive Quantization
Tal Wagner, Yonatan Naamad, Nina Mishra</description>
    </item>
    <item>
      <title>[개발] Hugo 테마에서 마크다운 텍스트 양쪽 정렬</title>
      <link>https://russellgeum.github.io/posts/2023-07-24/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-24/</guid>
      <description>Hugo 텍스트 양쪽 정렬 기본적으로 마크다운 문법은 텍스트 양쪽 정렬을 지원하지 않는다.
다만 .scss 파일에 몇 줄 코드 추가로 강제 양쪽 정렬을 할 수 있다.
먼저 아래의 경로로 들어가자.
&amp;lt;blog folder&amp;gt;/assets/themes/_main.scss &amp;lt;blog folder&amp;gt;/assets/themes/_markdown.scss 그리고 아래의 코드를 추가하여 저장한다.
// 글 양쪽 정렬 p { text-align: justify; word-break: break-all; } 다시 리빌드를 하면 텍스트 양쪽 정렬이 된 것을 확인할 수 있다.</description>
    </item>
    <item>
      <title>[논문] Pruning vs Quantization: Which is Better?</title>
      <link>https://russellgeum.github.io/posts/2023-07-23/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-23/</guid>
      <description>Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motivation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다.</description>
    </item>
    <item>
      <title>[생각] 나의 자아상</title>
      <link>https://russellgeum.github.io/posts/2023-07-21/</link>
      <pubDate>Fri, 21 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-21/</guid>
      <description>의미박탈자와 의미부여자 얼마 전, 모임의 어느 전문가 분에게 짤막한 자아상 분석을 받았다.
몇 가지 질문에 대하여 나는 답을 내려야 했고, 앞으로 고민할 것이 생겼다.
나는 주변을 통제하려는 성향이 있다. -&amp;gt; 왜? 내가 자아상을 엄격하게 바라보기도 한다. 무슨 의미일까? 내가 자아상을 유연하게 바라보기도 한다. 무슨 의미일까? 몇 가지 개념을 들엇다. 그 중 기억에 남는 것이 곧 제목이다.
&amp;ldquo;의미박탈자라 함은 내 인생에 득이 되지않고 거리를 둬야하는&amp;rdquo;
&amp;ldquo;의미부여자라 함는 내 인생을 더 발전시켜 빛이 나도록 해주는&amp;rdquo;</description>
    </item>
    <item>
      <title>[논문] SqueezeLLM: Dense-and-Sparse Quantization</title>
      <link>https://russellgeum.github.io/posts/2023-07-19/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-19/</guid>
      <description>Paper Link Sehoon Kim et al (UC Berkeley)
Introduction This paper proposes a Psuedo-PTQ method considering the weight distribution of LLM and outliers.
Motivation Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. Deploying LLMs for inference has been a significant challenge due to their unprecedented resource requirements. AThis has forced existing deployment frameworks to use multi-GPU inference pipelines, or to use smaller and less performant models.</description>
    </item>
    <item>
      <title>[회고] 2023년 상반기 회고</title>
      <link>https://russellgeum.github.io/posts/2023-07-14/</link>
      <pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-14/</guid>
      <description>1. 작년 졸업을 준비할 쯤, 큰 부상을 당해 팔 수술을 하였다. 팔목에 영구 후유가 생겼는데, 지금도 키보드를 오래 잡으면 조금 시큰하다. 어쨋든 작년에는 취업을 못할 것 같아서 반년 쉬고 준비를 다시 해야할 것 같았다. (회사 지원을 폭 넓게 많이 못했다. 꼭 하고 싶은 분야의 회사로만 지원을 할 수 밖에 없었다.) 그래도 운이 좋게 원하던 회사에 합격하였고, 어느덧 재직 딱 반년 차 주니어이다.
2. (큰 이변이 없는 한) 다시 학교로 돌아가지는 않을 것 같다.</description>
    </item>
    <item>
      <title>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</title>
      <link>https://russellgeum.github.io/posts/2021-09-31/</link>
      <pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-09-31/</guid>
      <description>Motivation 기존 뎁스 추정은 correspondence estimation으로 풀었다. 그러나 이 과정에는 문제가 있음. 어떤 문제?
conventional 방법은 텍스쳐가 약하거나, non-Lambertian 표면에서 문제가 생김
딥러닝 기반은 뎁스 consistency가 일정하지 않고, photometric consistency에서 3D 정보를 제대로 반영하지 못하는 문제
이 논문은 NeRF의 힘을 빌려, 멀티 뷰 스테레오 뎁스 추정을 하고자 함. 어떻게?
correspondence estimation과 corr view depth reprojection 최적화 대신에, 이 논문은 다이렉트로 부피를 최적화함
→ 그런데 NeRF에서는 shape-radiance ambiguity 문제가 있음. 이를 해결하기 위해 뎁스 프라이어 기반의 NeRF 훈련 가이던스를 제안함</description>
    </item>
    <item>
      <title>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</title>
      <link>https://russellgeum.github.io/posts/2022-07-31/</link>
      <pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2022-07-31/</guid>
      <description>개요 호모그래피는 스테레오 비전의 근본이다. 영상이 대략 회전 모션이거나 장면이 평면 표면에 가까우면 호모그래피 행렬을 근사할 수 있다. 장면이 제약 조건을 만족하면 직접 호모그래피를 계산할 수 있다. 시맨텍 어웨어하고 러버스트한 호모그래피 추정 딥러닝 알고리즘을 개발 이전 연구의 한계 생략
제안 방법 두 이미지를 인코더
레즈넷34 백본을 받아서 3x3, 8DoF의 호모그래피 행렬을 추정
호모그래피 추정을 위해 Triplet Loss를 사용
호모그래피 추정이 완벽하다면 호모그래피를 통한 와핑이 잘 되어야 함 그래서 와핑한 피처 혹은 이미지가 타겟 피처 또는 이미지와 잘 얼라인 되어야 함 두 번째 로스 텀은 잘 모르겠음 호모그래피 a→b에서 b→a는 identity로 레귤라이저를 추가함 Content-aware prob map</description>
    </item>
    <item>
      <title>[논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception</title>
      <link>https://russellgeum.github.io/posts/2022-04-30/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2022-04-30/</guid>
      <description>Motivation CL이나 특정한 Pretext task는 비디오에서 중요하지 않은 배경에 집중하는 문제가 발생 비디오에는 모션이 있음 이 모션에 집중하기 위한 학습 방법을 제안해야함 Related Work Pretext task 방법
지오메트릭한 정보를 배우는 spatial learning clip order를 학습하는 temporal learning space-time 정보를 학습하는 spatiotemporal learning 그러나 이 방법의 단점은 비디오에 리더던시 정보가 많아서 불필요한 학습을 야기함
배경에 대하여 정적이거나 무관한 정보는 모델의 판단성을 저해할 수 있음
배경 때문에 모델의 비디오 이해도가 낮아질 수 있음</description>
    </item>
    <item>
      <title>[논문] Deep Video Prior for Consistency and Propagation</title>
      <link>https://russellgeum.github.io/posts/2022-01-31/</link>
      <pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2022-01-31/</guid>
      <description>Motivation 비디오 프레임간 시간 불일치성을 해결하기 위해 DVP를 implcit하게 DNN에 주는 방법을 제안 DVP가 무엇인가?
비디오를 사용한 멀티모달 태스크에서는 성능의 흔들림이 심함 → 이터레티브하게 중요도를 재할당하는 전략으로 해결 Related Work 이전 비디오 연구들은 구축된 대규모 비디오 데이터셋이 필요했음 옵티컬 플로우 같은 정보나, 단순 프레임 간 유사도를 비교하는 것만으로는 롱-텀 비디오에 적합하지 않음 이전 비디오 연구들은 멀티 모달 태스크에서 좋은 성능을 골고루 보이기 어려웠음 Contribution DVP가 무엇인가?
DVP는 비디오 처리에서 임플리싯하게 비디오 일관성을 주기 위해 사용되는 성질들을 일컬음</description>
    </item>
    <item>
      <title>[논문] Masked Autoencoders Are Scalable Vision Learners</title>
      <link>https://russellgeum.github.io/posts/2021-12-31/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-12-31/</guid>
      <description>Motivation 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이 복원할 수 있을까?
비대칭 형태의 인코더 - 디코더
인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이 latent vector를 가지고 원래의 이미지를 복원
인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성
Related Works 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만, 비전에서 오토인코딩으로의 진전 X
저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가?
자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다.</description>
    </item>
    <item>
      <title>[논문] SOFT: Softmax-free Transformer with Linear Complexity</title>
      <link>https://russellgeum.github.io/posts/2021-10-31/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-10-31/</guid>
      <description>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -&amp;gt; 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?</description>
    </item>
    <item>
      <title>[논문] Video Object Segmentation with Compressed Video</title>
      <link>https://russellgeum.github.io/posts/2021-08-10/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-08-10/</guid>
      <description>개요 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
이전 연구의 한계 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 제안 방법 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함</description>
    </item>
    <item>
      <title>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</title>
      <link>https://russellgeum.github.io/posts/2021-06-30/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-06-30/</guid>
      <description>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인</description>
    </item>
    <item>
      <title>[논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations</title>
      <link>https://russellgeum.github.io/posts/2021-06-25/</link>
      <pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-06-25/</guid>
      <description>Motivation ViT, MLP 믹서가 어떤 경우에 레즈넷의 성능을 능가할 수 있을까? 의 고찰
ViT, MLP 믹서는 라지 스케일 트레이닝이나, 강한 데이터 arguments를 주어야 했음 모델이 인덕티브 바이어스를 포괄하기 힘들기 때문 그런데 이러한 기법 없이 레즈넷 보다 성능을 올리는 방법을 고민 Related Works 생략
Contribution ViT와 MLP 믹서의 그래디언트 필드는 매우 날카로운 로컬 미니마에 수렴한다는 것을 보여준다. (이는 레즈넷보다 몇 배 더 큼) 이러한 필드는 백프롭때 그래디언트가 누적되고, 초기 임베딩 레이어가 굉장히 큰 헤시안 행렬의 고유값을 가지면서 문제가 될 수 있음 네트워크들은 상대적으로 작은 훈련 에러를 가지고, 특히 MLP 믹서는 ViT보다 오버피팅 가능성이 있다.</description>
    </item>
    <item>
      <title>[논문] When Does Contrastive Visual Representation Learning Work</title>
      <link>https://russellgeum.github.io/posts/2021-05-31/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-05-31/</guid>
      <description>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task </description>
    </item>
    <item>
      <title>[논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal</title>
      <link>https://russellgeum.github.io/posts/2021-05-30/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-05-30/</guid>
      <description>Motivation Video Instance Segmentation 문제는 동시에 classify, segment, track을 하는 것이다. 이 태스크는 프레임 레벨 VIS보다 성능이 좋다. 그러나 리얼 타임이 아니다. VisTR이 이 문제를 해결하려 했으나, 훈련 시간이 길었다. 그리고 hand-crafted data association이 많이 필요해서 비효율적이다.
Related Works 프레임 레벨 VIS tracking by segmentation 방법 복잡한 data association 알고리즘이 필요 temporal context를 추출하는게 한계가 있음 object occlusion을 핸들링하지 못함 클립 레벨 VIS clip by clip으로 segmentation and tracking 프레임 레벨 VIS보다 long range temporal context를 추출 가능 그러나 실시간성이 부족해서 속도가 느림 Contribution EfficientVIS</description>
    </item>
    <item>
      <title>[논문] Skip-Convolutions for Efficient Video Processing</title>
      <link>https://russellgeum.github.io/posts/2021-04-02/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-04-02/</guid>
      <description>Motivation 비디오는 정지된 이미지의 연속일수도 있고, 변화하는 이미지의 연속일수도 있다.
우리는 세상을 비디오로 인지 → 즉, 변화를 인지 → 변화를 느낀다는 건, 프레임간 차이 (residual)이 누적되면서 어느 임계를 넘어가서 알아채는 것. 이러한 동기로 몇 가지 연구들이 있다. (뉴로모픽, 이벤트 카메라, SNN 등등) 그러나 아직까지가 주류가 아님.
Related Works 기존의 비디오 처리는 픽셀 레벨의 dense prediction을 요구하는 경우가 많음 → 모든 프레임을 모델에 넣어서 연산
프레임 수가 증가할수록 연산량 오버헤드가 리니어하게 증가 → 심지어 새로운 변화가 없어도 계산을 해야만 함</description>
    </item>
    <item>
      <title>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
      <link>https://russellgeum.github.io/posts/2021-04-01/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-04-01/</guid>
      <description>Motivation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)</description>
    </item>
    <item>
      <title>[논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples</title>
      <link>https://russellgeum.github.io/posts/2021-03-30/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-30/</guid>
      <description>Motivation MoCo 구조를 비디오 도메인으로 확장
Related Works 생략
Contribution Propose temporallly adversarial learning to improve the feature representation of the encoder
ConvLSTM을 통해 프레임 마스크를 출력
→ Discriminator(encoder)를 통해 쿼리 피처와 프레임 피처를 출력
→ 프레임이 같으면 0, 마스킹된 것은 차이가 최대
마스킹 프레임의 피처를 잘 배울 수 있도록 이 차이가 최대가 되도록 학습
Propose temporal decay to reduce the effect from historical keys in the memory queyes during contrastive learning</description>
    </item>
    <item>
      <title>[논문] ViViT, A Video Vision Transformer</title>
      <link>https://russellgeum.github.io/posts/2021-03-05/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-05/</guid>
      <description>Motivation 비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
Contribution 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론 regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌 비디오 임베딩 ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing 다른 하나는 토큰 차원을 temporal로 확장해서 사용 세 가지 구조 모델 1</description>
    </item>
    <item>
      <title>[논문] Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLR v2)</title>
      <link>https://russellgeum.github.io/posts/2021-03-02/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-02/</guid>
      <description>용어의 정리 task-agnostic: 태스크에 구애받지 않는 fine-tuning 할 때 사용하는 태스크는 최종 태스크 (specific task) Motivation 레이블이 없는 방대한 데이터를 잘 활용하면서, 몇 가지 레이블로만 학습 효율을 높이는 방법론 중 하나는 비지도 학습 기반의 사전 훈련과 fine-tuning이다. 즉, 레이블이 없는 방대한 데이터를 통한 비지도 학습으로 좋은 representation을 얻은 후, 이를 통해 적은 레이블의 데이터만으로 fine-tuning을 하는 것 이러한 방법론을 컴퓨터 비전에서는 어떻게 할 수 있을지에 대한 연구이다.
Related Work 이미 자연어 처리에서는 지배적인 방법이다.</description>
    </item>
    <item>
      <title>[논문] A Simple Framework for Contrastive Learning of Visual Representations</title>
      <link>https://russellgeum.github.io/posts/2021-03-01/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-01/</guid>
      <description>용어의 정의 Pretext task: represenation learning을 위해 수행되는 태스크 Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는 문제를 푸는 것 Motivation 모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?
Related Work Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음
Generative
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음 Discriminative supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등) Contribution representation learning에서 data augmentation에 대한 체계적인 고민이 없었음</description>
    </item>
    <item>
      <title>[논문] Video Object Segmentation using Space-Time Memory Networks</title>
      <link>https://russellgeum.github.io/posts/2021-05-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-05-12/</guid>
      <description>Motivation 준지도 학습에서 문제에 따라, 중간단계 예측과 함께 사용 가능한 정보는 풍부하다. 기존의 방법에서는 이러한 아이디어를 활용할 수 없었다. 논문은 메모리 네트워크를 통해, 가능한 모든 소수로부터 관련된 정보를 읽어 학습하고자 한다. 과거 프레임과 마스크가 외부 메모리를 형성하고, 현재 프레임이 쿼리로서 메모리 속 마스크 정보를 사용하여 세그멘테이션을 수행함 구체적으로 쿼리와 메모리는 피처 스페이스에서 매칭이 된다. (모든 space-time 픽셀 지점에서) Related Works 이전 프레임에서 형상을 추출하고 전파하는 방식 외관 변화에 더 잘 대처하나, 오클루션이나 에러 드리프트의 러버스트가 낮을 수 있다.</description>
    </item>
  </channel>
</rss>
