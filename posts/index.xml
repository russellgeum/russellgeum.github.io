<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Oppenheimer&#39;s BLOG</title>
    <link>https://russellgeum.github.io/posts/</link>
    <description>Recent content in Posts on Oppenheimer&#39;s BLOG</description>
    <image>
      <title>Oppenheimer&#39;s BLOG</title>
      <url>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 04 Apr 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://russellgeum.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[논문] Layer Sharing for Parameter-Efficient Trasnformer</title>
      <link>https://russellgeum.github.io/posts/review/2024-04-04/</link>
      <pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2024-04-04/</guid>
      <description>개요 이 글은 Qualcomm의 심규홍 박사님이 발표해주신 자료를 토대로 작성한다.
대상: 트랜스포머를 이해하고 있는 개발자들을 위한 세미나
Motivation 트랜스포머 애플리케이션은 서버 베이스 모델에서는 활발하게 사용되고 있다. 이제 모바일 베이스로 들어갈려고 한다. 트랜스포머는 scaling-law를 따른다. 더 크고, 더 많이 쌓을수록 더 좋은 성능이 나온다. 따라서 돈을 들이면 성능이 보장된다. 그 예시가 LLM이다. 그러나 Efficiecy 관점에서 충분히 고민을 해봐야할 문제가 많다. RAM 사이즈, NPU 퍼포먼스, Cache 사이즈 등 고려해야 할 사항이 많다. On-device LLM에 대한 사이즈가 어느정도 적절할까?</description>
    </item>
    <item>
      <title>[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기</title>
      <link>https://russellgeum.github.io/posts/devops/2024-02-13/</link>
      <pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/devops/2024-02-13/</guid>
      <description>Orion-14B 본 포스팅은 Orion-14B-Chat을 기준으로 한다.
llama.cpp Orion-14B 모델 Orion-14B-Chat in HuggingFace 추론 환경 CMake 설치 brew install cmake llama.cpp 환경 클론 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp llama.cpp 환경 빌드 mkdir build cd build cmake .. cmake --build . --config Release Orion-14B 모델 다운로드 허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation.utils import GenerationConfig tokenizer = AutoTokenizer.from_pretrained(&amp;#34;OrionStarAI/Orion-14B&amp;#34;, use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM.</description>
    </item>
    <item>
      <title>[논문] Survey: Large Multimodal Models</title>
      <link>https://russellgeum.github.io/posts/review/2024-02-04/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2024-02-04/</guid>
      <description>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장</description>
    </item>
    <item>
      <title>[논문] Survey: Efficient Large Language Models</title>
      <link>https://russellgeum.github.io/posts/review/2024-01-07/</link>
      <pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2024-01-07/</guid>
      <description>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023</description>
    </item>
    <item>
      <title>[생각] 2023년 회고</title>
      <link>https://russellgeum.github.io/posts/essay/2023-12-11/</link>
      <pubDate>Mon, 11 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-12-11/</guid>
      <description>1. 졸업과 취업 졸업 (1분기) 석사 디펜스를 끝내고, 완전히 학교를 떠났다. 포항과 서울을 2주마다 오가곤 했으니, 타지에서 지내는 외로움이 컸다. 그렇지만 개개인 퍼포먼스가 훌륭한 연구실 친구들은 나에게 큰 자산이다. 연구적으로 디스커션도 많이하고, 같은 업계에 있으니 서로 동향을 알기 좋은 사람들이다.
일전에도 말한바 있지만, 학위로 얻은 것은 다음과 같이 정리한다.
내가 무엇을 공부를 하든 스스로 커리큘럼을 설계하고, 학습할 수 있는 능력 체계적, 논리적으로 고민하고 그 결과를 계층적 구조로 작문하는 능력 나는 연구가 잘 풀린 사람은 아니다.</description>
    </item>
    <item>
      <title>[생각] 재능있는 척 하지 않기</title>
      <link>https://russellgeum.github.io/posts/essay/2023-11-12/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-11-12/</guid>
      <description>재능있는 척 하지 않기 재능 있는 척 하지 않기
이 글은 브랜치의 &amp;ldquo;향로&amp;quot;님의 글을 보고 나에게 대입하여 재정리하였다.
요약하자면 작가는 개발을 잘 하지 못했던 시절에,
따로 공부했다는 사실을 주변에 알리고 싶지 않아했다.
왜냐하면 못한 성과에 대해서 재능이 없는 것이 아니라,
노력이 부족했다는 면피성 명분을 만들수 있기 때문이었다.
나의 이야기 올 여름~가을 나는 회사에서 (개인적으로 스스로가 너무 절망적이었던)
퍼포먼스가 너무 좋지 못한 태스크를 수행중이었다.
나는 &amp;ldquo;내가 잘 모른다. 어렵다. 그러니 나를 도와주었으면 좋겠다.</description>
    </item>
    <item>
      <title>[논문] ICCV 2023 관심 논문 리스트업</title>
      <link>https://russellgeum.github.io/posts/review/2023-10-03/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-10-03/</guid>
      <description>ICCV 2023 ICCV 2023 Link
Papers ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.
(일부 특이한 연구도 포함)
Neural Radiance Fields NeRF-MS: Neural Radiance Fields with Multi-Sequence
Peihao Li et al.
Re-ReND: Real-time Rendering of NeRFs across Devices
Sara Rojas et al.
CLNeRF: Continual Learning Meets NeRF
Zhipeng Cai, Matthias Muller
Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
Hansheng Chen et al.
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models Compression</title>
      <link>https://russellgeum.github.io/posts/review/2023-08-29/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-08-29/</guid>
      <description>대규모 언어 모델의 경량화 동향 Abstract LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬 LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함 Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌 압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함 Introduction &amp;amp; Method 대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다.</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models</title>
      <link>https://russellgeum.github.io/posts/review/2023-08-28/</link>
      <pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-08-28/</guid>
      <description>LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models</description>
    </item>
    <item>
      <title>[생각] 나의 문제점</title>
      <link>https://russellgeum.github.io/posts/essay/2023-08-12/</link>
      <pubDate>Sat, 12 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-08-12/</guid>
      <description>나의 문제점 0. 왜 이 글을 쓰는가? 짧고, 긴 시간 동안 나하고 같이 지낸 소중한 사람들로부터 몇 가지 피드백과 조언을 얻을 때가 있다. 그러한 조언 중 공통된 문장을 정리해서 내가 어떤 단점이 있는지 생각하고 이를 개선할 방법을 고민한다.
1. 나는 다른 사람에게 관심이 없다. 나는 다른 사람에게 관심이 없다. 이것은 타고나는 성향일 가능성이 크다. 갑자기 남에게 관심을 많이 가지라고 하는 것은 고문에 가깝다. 그렇지만 다른 사람과의 지속적인 커뮤니케이션을 위한다면, &amp;ldquo;관심있는 척&amp;rdquo; 이라도 해본다.</description>
    </item>
    <item>
      <title>[독서] 최재천의 공부</title>
      <link>https://russellgeum.github.io/posts/essay/2023-07-29/</link>
      <pubDate>Sat, 29 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-07-29/</guid>
      <description>어떻게 배우며 살 것인가? 이 책에 담긴 최재천 교수와 안희경 저널리스트 간의 대화록은 두 가지 관점으로 이야기가 흐른다. 하나는 &amp;ldquo;개인 차원에서의 학습&amp;rdquo; 이고, 둘째는 &amp;ldquo;사회 차원에서의 교육&amp;rdquo; 이다. 사회 차원에서의 교육도 겉으로는 &amp;ldquo;어떻게 가르칠까?&amp;rdquo; 의 미래적 고민을 담지만, 그렇다고 명확한 교수법을 딱 가이드라인을 세울 수는 없다. 하지만 어느정도 적절한 방법은 있을 수 있다. 그 중 저자가 추구하는 방법은 여러 사람의 생각과 경험이 한데 섞여, 어울려 학습하는 현장을 만드는 것이다.
하지만 올바른 교육법도 교육 받는 주체가 &amp;ldquo;바른 학습&amp;rdquo; 이 가능하다는 가정에서, 어떻게 그것을 지속할지의 해답에 불과하다고 생각한다.</description>
    </item>
    <item>
      <title>[논문] ICML 2023 관심 논문 리스트업</title>
      <link>https://russellgeum.github.io/posts/review/2023-07-25/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-07-25/</guid>
      <description>ICML 2023 Papers ICML 2023이 열리고 있다.
Distillation, Quantization, HW-aware Deep Learning 위주로 트래킹 중이다.
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models
Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan
DIVISION: Memory Efficient Training via Dual Activation Precision
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu
Fast Private Kernel Density Estimation via Locality Sensitive Quantization
Tal Wagner, Yonatan Naamad, Nina Mishra</description>
    </item>
    <item>
      <title>[개발] Hugo 테마에서 마크다운 텍스트 양쪽 정렬</title>
      <link>https://russellgeum.github.io/posts/devops/2023-07-24/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/devops/2023-07-24/</guid>
      <description>Hugo 텍스트 양쪽 정렬 기본적으로 마크다운 문법은 텍스트 양쪽 정렬을 지원하지 않는다.
다만 .scss 파일에 몇 줄 코드 추가로 강제 양쪽 정렬을 할 수 있다.
먼저 아래의 경로로 들어가자.
&amp;lt;blog folder&amp;gt;/assets/themes/_main.scss &amp;lt;blog folder&amp;gt;/assets/themes/_markdown.scss 그리고 아래의 코드를 추가하여 저장한다.
// 글 양쪽 정렬 p { text-align: justify; word-break: break-all; } 다시 리빌드를 하면 텍스트 양쪽 정렬이 된 것을 확인할 수 있다.</description>
    </item>
    <item>
      <title>[논문] Pruning vs Quantization: Which is Better?</title>
      <link>https://russellgeum.github.io/posts/review/2023-07-23/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-07-23/</guid>
      <description>Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motivation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다.</description>
    </item>
    <item>
      <title>[생각] 나의 자아상</title>
      <link>https://russellgeum.github.io/posts/essay/2023-07-21/</link>
      <pubDate>Fri, 21 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-07-21/</guid>
      <description>의미박탈자와 의미부여자 얼마 전, 모임의 어느 전문가 분에게 짤막한 자아상 분석을 받았다.
몇 가지 질문에 대하여 나는 답을 내려야 했고, 앞으로 고민할 것이 생겼다.
나는 주변을 통제하려는 성향이 있다. -&amp;gt; 왜? 내가 자아상을 엄격하게 바라보기도 한다. 무슨 의미일까? 내가 자아상을 유연하게 바라보기도 한다. 무슨 의미일까? 몇 가지 개념을 들엇다. 그 중 기억에 남는 것이 곧 제목이다.
&amp;ldquo;의미박탈자라 함은 내 인생에 득이 되지않고 거리를 둬야하는&amp;rdquo;
&amp;ldquo;의미부여자라 함는 내 인생을 더 발전시켜 빛이 나도록 해주는&amp;rdquo;</description>
    </item>
    <item>
      <title>[논문] SqueezeLLM: Dense-and-Sparse Quantization</title>
      <link>https://russellgeum.github.io/posts/review/2023-07-19/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2023-07-19/</guid>
      <description>Paper Link Sehoon Kim et al (UC Berkeley)
Introduction This paper proposes a Psuedo-PTQ method considering the weight distribution of LLM and outliers.
Motivation Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. Deploying LLMs for inference has been a significant challenge due to their unprecedented resource requirements. AThis has forced existing deployment frameworks to use multi-GPU inference pipelines, or to use smaller and less performant models.</description>
    </item>
    <item>
      <title>[회고] 2023년 상반기 회고</title>
      <link>https://russellgeum.github.io/posts/essay/2023-07-14/</link>
      <pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/essay/2023-07-14/</guid>
      <description>1. 작년 졸업을 준비할 쯤, 큰 부상을 당해 팔 수술을 하였다. 팔목에 영구 후유가 생겼는데, 지금도 키보드를 오래 잡으면 조금 시큰하다. 어쨋든 작년에는 취업을 못할 것 같아서 반년 쉬고 준비를 다시 해야할 것 같았다. (회사 지원을 폭 넓게 많이 못했다. 꼭 하고 싶은 분야의 회사로만 지원을 할 수 밖에 없었다.) 그래도 운이 좋게 원하던 회사에 합격하였고, 어느덧 재직 딱 반년 차 주니어이다.
2. (큰 이변이 없는 한) 다시 학교로 돌아가지는 않을 것 같다.</description>
    </item>
    <item>
      <title>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</title>
      <link>https://russellgeum.github.io/posts/review/2022-07-31/</link>
      <pubDate>Sun, 31 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2022-07-31/</guid>
      <description>개요 호모그래피는 스테레오 비전의 근본이다. 영상이 대략 회전 모션이거나 장면이 평면 표면에 가까우면 호모그래피 행렬을 근사할 수 있다. 장면이 제약 조건을 만족하면 직접 호모그래피를 계산할 수 있다. 시맨텍 어웨어하고 러버스트한 호모그래피 추정 딥러닝 알고리즘을 개발 이전 연구의 한계 생략
제안 방법 두 이미지를 인코더
레즈넷34 백본을 받아서 3x3, 8DoF의 호모그래피 행렬을 추정
호모그래피 추정을 위해 Triplet Loss를 사용
호모그래피 추정이 완벽하다면 호모그래피를 통한 와핑이 잘 되어야 함 그래서 와핑한 피처 혹은 이미지가 타겟 피처 또는 이미지와 잘 얼라인 되어야 함 두 번째 로스 텀은 잘 모르겠음 호모그래피 a→b에서 b→a는 identity로 레귤라이저를 추가함 Content-aware prob map</description>
    </item>
    <item>
      <title>[논문] Rethinking the Augmentation Module in Contrastive Learning</title>
      <link>https://russellgeum.github.io/posts/review/2022-06-30/</link>
      <pubDate>Thu, 30 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2022-06-30/</guid>
      <description>Motivation CL은 DA에 강력하게 의존하는 방법이다. 인위적인 DA는 다음과 같은 단점이 있다. 데이터 증강의 휴리스틱한 조합은 특정적인 표현 불변성을 가져다 준다. 강력한 데이터 증강은 너무 많은 불변성을 가지고 있어서 오히려 fine-grained한 다운스트림 태스크에 적합하지 않다. 따라서 이 논문은 어디서? 무엇을? 이란 질문으로 DA를 하는 방법론을 소개한다. Related Work 생략
Contribution 다양한 augmentation module 조합을 사용한다.
샴 구조는 깊이에 따라 여러 스테이지로 활용한다. 각 스테이지의 피처를 CL에 활용한다.
Hierarchical augmentation invariance</description>
    </item>
    <item>
      <title>[논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception</title>
      <link>https://russellgeum.github.io/posts/review/2022-04-30/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2022-04-30/</guid>
      <description>Motivation CL이나 특정한 Pretext task는 비디오에서 중요하지 않은 배경에 집중하는 문제가 발생 비디오에는 모션이 있음 이 모션에 집중하기 위한 학습 방법을 제안해야함 Related Work Pretext task 방법
지오메트릭한 정보를 배우는 spatial learning clip order를 학습하는 temporal learning space-time 정보를 학습하는 spatiotemporal learning 그러나 이 방법의 단점은 비디오에 리더던시 정보가 많아서 불필요한 학습을 야기함
배경에 대하여 정적이거나 무관한 정보는 모델의 판단성을 저해할 수 있음
배경 때문에 모델의 비디오 이해도가 낮아질 수 있음</description>
    </item>
    <item>
      <title>[논문] Deep Video Prior for Consistency and Propagation</title>
      <link>https://russellgeum.github.io/posts/review/2022-01-31/</link>
      <pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2022-01-31/</guid>
      <description>Motivation 비디오 프레임간 시간 불일치성을 해결하기 위해 DVP를 implcit하게 DNN에 주는 방법을 제안 DVP가 무엇인가?
비디오를 사용한 멀티모달 태스크에서는 성능의 흔들림이 심함 → 이터레티브하게 중요도를 재할당하는 전략으로 해결 Related Work 이전 비디오 연구들은 구축된 대규모 비디오 데이터셋이 필요했음 옵티컬 플로우 같은 정보나, 단순 프레임 간 유사도를 비교하는 것만으로는 롱-텀 비디오에 적합하지 않음 이전 비디오 연구들은 멀티 모달 태스크에서 좋은 성능을 골고루 보이기 어려웠음 Contribution DVP가 무엇인가?
DVP는 비디오 처리에서 임플리싯하게 비디오 일관성을 주기 위해 사용되는 성질들을 일컬음</description>
    </item>
    <item>
      <title>[논문] Learning Optical Flow, Depth, and Scene FLow without Real-world Labels</title>
      <link>https://russellgeum.github.io/posts/review/2022-01-30/</link>
      <pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2022-01-30/</guid>
      <description>Motivation Depth, Sceneflow를 동시에 푸는 것은 ill-posed 문제이고, 수 많은 해가 존재한다. 먼저 옵티컬 플로우를 추정하고, 알려진 포즈와 함께 initial depth를 연산한다. 그리고 sceneflow, depth를 refinement하는 파이프라인을 제안한다.
(그러니까 원 스테이지로는 하기가 힘드니 투 스테이지로 해보겠다는 의미) Related Works 비디오 기반의 SSL를 통한 3D perception 학습들은 아래의 네 가지 태스크로 나뉜다.
Ego-motion estimation Monocular Depth estimation → Scale ambiguity, Static assumption 문제 발생 Opticalflow estimation Sceneflow estimation → Can not handle sceneflow from opticalflow esitmator (indirectly estimation), stereo manner at training time 모두 개별 태스크에서 우수한 성능을 보이지만, scale ambuiguity 문제가 있다.</description>
    </item>
    <item>
      <title>[논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio</title>
      <link>https://russellgeum.github.io/posts/review/2021-11-20/</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-11-20/</guid>
      <description>Motivation Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters?
Despite recent advances across different domains and tasks, current state-ofthe-art methods train a separate model with different model parameters for each task at hand.
Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains.</description>
    </item>
    <item>
      <title>[논문] Decoupled Contrastive Learning</title>
      <link>https://russellgeum.github.io/posts/review/2021-11-01/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-11-01/</guid>
      <description>Motivation CL의 로스에서 각 편미분에 대해 커플링되는 텀이 있음 → 이는 학습 효율성에 관여
Related Works CL은 학습에서 많은 양의 네거티브가 필요하다 → 큰 배치 사이즈를 요구로 함 → 이는 어쩔수없이 컴퓨터 리소스가 많이 필요 따라서 배치 사이즈에 민감하다. Contribution Infoloss를 각각 편미분 하였을때 공통된 텀이 포함되는 것을 보임 이 텀은 P/N 간 커플링 되는 것을 의미하고 학습 효율성에 영향을 줄 수 있음
예를 들어 N이 가깝고 P도 가까우면, N의 grad 또한 감소.</description>
    </item>
    <item>
      <title>[논문] Masked Autoencoders Are Scalable Vision Learners</title>
      <link>https://russellgeum.github.io/posts/review/2021-12-31/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-12-31/</guid>
      <description>Motivation 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이 복원할 수 있을까?
비대칭 형태의 인코더 - 디코더
인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이 latent vector를 가지고 원래의 이미지를 복원
인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성
Related Works 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만, 비전에서 오토인코딩으로의 진전 X
저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가?
자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다.</description>
    </item>
    <item>
      <title>[논문] SOFT: Softmax-free Transformer with Linear Complexity</title>
      <link>https://russellgeum.github.io/posts/review/2021-10-31/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-10-31/</guid>
      <description>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -&amp;gt; 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?</description>
    </item>
    <item>
      <title>[논문] Non Deep Networks</title>
      <link>https://russellgeum.github.io/posts/review/2021-10-20/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-10-20/</guid>
      <description>Motivation DNN의 깊이가 깊어지면 단점이 많음 → 레이턴시가 길어지기 때문에 빠른 반응을 필요로 하는 애플리케이션이 부적합 어떻게 하면 얕은 깊이의 DNN으로도 충분한 성능을 낼 수 있을까? → 해답은 패러렐한 뉴럴넷 구성으로 성능을 낼 수 있다. Related Works 생략
Contribution 구체적으로 ~10 레이어, ~12 레이어까지 적절함을 말한다.
VGG 스타일의 블록을 사용한다. (구체적으로 Rep-VGG을 빌리지만, 목적에 맞게 조금 수정)
제한된 네트워크 깊이로 receptive field가 좁다. 이를 해결하기 위해, Squeeze-Exicitation 레이어에 기반한 SSE 레이어를 추가하였다.</description>
    </item>
    <item>
      <title>[논문] Video Object Segmentation with Compressed Video</title>
      <link>https://russellgeum.github.io/posts/review/2021-08-10/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-08-10/</guid>
      <description>개요 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
이전 연구의 한계 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 제안 방법 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함</description>
    </item>
    <item>
      <title>[논문] Contextual Transformer Networks for Visual Recognition</title>
      <link>https://russellgeum.github.io/posts/review/2021-07-30/</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-07-30/</guid>
      <description>Motivation 비전 태스크에서 셀프 어텐션의 계산이, 즉 공간적인 위치에서 Q, K가 서로 independent하게 계산이 되어지는 것이 단점 → context가 필요
Related Works CNN의 receptive field를 넓히는 것 → context를 잘 보긴 하지만, long range dependecy를 보지 못함 ViT, long range dependency를 보기는 하지만, independent한 Q, K의 interaction을 계산 Contribution 기존의 conventional self-attention은 서로 다른 위치간의 interaction을 잘 계산. 그러나 모든 pairwise Q-K relation은 independent함 → 풍부한 context를 보지 못함, 따라서 Conetxt Transformer 구조를 제안.</description>
    </item>
    <item>
      <title>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</title>
      <link>https://russellgeum.github.io/posts/review/2021-06-30/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-06-30/</guid>
      <description>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인</description>
    </item>
    <item>
      <title>[논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations</title>
      <link>https://russellgeum.github.io/posts/review/2021-06-25/</link>
      <pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-06-25/</guid>
      <description>Motivation ViT, MLP 믹서가 어떤 경우에 레즈넷의 성능을 능가할 수 있을까? 의 고찰
ViT, MLP 믹서는 라지 스케일 트레이닝이나, 강한 데이터 arguments를 주어야 했음 모델이 인덕티브 바이어스를 포괄하기 힘들기 때문 그런데 이러한 기법 없이 레즈넷 보다 성능을 올리는 방법을 고민 Related Works 생략
Contribution ViT와 MLP 믹서의 그래디언트 필드는 매우 날카로운 로컬 미니마에 수렴한다는 것을 보여준다. (이는 레즈넷보다 몇 배 더 큼) 이러한 필드는 백프롭때 그래디언트가 누적되고, 초기 임베딩 레이어가 굉장히 큰 헤시안 행렬의 고유값을 가지면서 문제가 될 수 있음 네트워크들은 상대적으로 작은 훈련 에러를 가지고, 특히 MLP 믹서는 ViT보다 오버피팅 가능성이 있다.</description>
    </item>
    <item>
      <title>[논문] Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers</title>
      <link>https://russellgeum.github.io/posts/review/2021-06-20/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-06-20/</guid>
      <description>Motivation 비디오에서 시간 차원은 공간 차원과 같은 방법으로 처리되었음.
비디오에서 물리적 위치가 t 프레임에 사영된 것과 t+k 프레임에 사영된 지점은 서로 무관할 수 있기 때문임.
Temporal correspondence는 이러한 다이나믹 장면을 학습하기 용이하게 설계되어야 함
Related Works RAFT의 옵티컬 플로우 추정은 두 프레임의 temporal corrrespondence 문제이다. RAFT의 correlation volume은 본질적으로 attention map와 같다. Allan Jabri의 연구에서는 비디오 temporal correspondnce 문제를 contrastive random walk 문제로 정의하고 해결 위와는 다르게 trajectory attention을 통해 temporal correspondece 문제를 해결 Contribution Trajectory Attention</description>
    </item>
    <item>
      <title>[논문] Long-Shot Temporal Contrastive Learning of Video Transformers</title>
      <link>https://russellgeum.github.io/posts/review/2021-06-15/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-06-15/</guid>
      <description>Motivation 비디오 트랜스포머가 기존 CNN 기반 비디오 모델들에 비해 경쟁력있는 성능을 보임 그러나 많은 파라미터와 inductive bias 부족은 대규모 데이터셋으로 학습된 강력한 사전 모델을 요구함 Related Works 생략
Contribution TimSformer vs Swin Transformer
이 논문은 스윈 트랜스포머의 속성을 비디오 도메인으로 확장하였음
Long-Shot Temporal Contrastive Learning
템포랄 도메인의 CL을 고안
레이블링이 되지 않은 비디오 B를 입력으로 받는다. 비디오 B를 랜덤하게 숏클립과 롱클립으로 샘플링해서 나눈다. 프레임의 수는 같으나, 시간 간격이 달라서 롱클립이 더 긴 범위의 비디오 표현을 담고 있다.</description>
    </item>
    <item>
      <title>[논문]Self-Supervised Learning of Compressed Video Representation</title>
      <link>https://russellgeum.github.io/posts/review/2021-06-10/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-06-10/</guid>
      <description>Motivation 효율적으로 비디오 표현 학습을 하는 방법을 고민
Related Work 이전 연구들은 비디오 프레임을 프로세싱하기 전에 JPEG 같은 형태로 디코딩하여 저장하고 representation 학습을 하였음. 이것은 스토리지를 많이 요구하고, 대규모 트레이닝에 비효율적임. Decoded frame없이 학습할 수 있었지만, supervised 기반이었지, self-supervised는 관심이 덜했음
Contribution 압측된 비디오 포맷에서 직접 비디오 표현 학습을 한다. 압축된 비디오는 두 가지 고유한 특성이 있음, 일단 GOP란? MPEG 포맷을 위해 영상 프레임의 덩어리를 가리킴
GOP(Group Of Picture)
왜 압축된 비디오가 유리할까?</description>
    </item>
    <item>
      <title>[논문] When Does Contrastive Visual Representation Learning Work</title>
      <link>https://russellgeum.github.io/posts/review/2021-05-31/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-05-31/</guid>
      <description>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task </description>
    </item>
    <item>
      <title>[논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal</title>
      <link>https://russellgeum.github.io/posts/review/2021-05-30/</link>
      <pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-05-30/</guid>
      <description>Motivation Video Instance Segmentation 문제는 동시에 classify, segment, track을 하는 것이다. 이 태스크는 프레임 레벨 VIS보다 성능이 좋다. 그러나 리얼 타임이 아니다. VisTR이 이 문제를 해결하려 했으나, 훈련 시간이 길었다. 그리고 hand-crafted data association이 많이 필요해서 비효율적이다.
Related Works 프레임 레벨 VIS tracking by segmentation 방법 복잡한 data association 알고리즘이 필요 temporal context를 추출하는게 한계가 있음 object occlusion을 핸들링하지 못함 클립 레벨 VIS clip by clip으로 segmentation and tracking 프레임 레벨 VIS보다 long range temporal context를 추출 가능 그러나 실시간성이 부족해서 속도가 느림 Contribution EfficientVIS</description>
    </item>
    <item>
      <title>[논문] Video Object Segmentation using Space-Time Memory Networks</title>
      <link>https://russellgeum.github.io/posts/review/2021-05-12/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-05-12/</guid>
      <description>Motivation 준지도 학습에서 문제에 따라, 중간단계 예측과 함께 사용 가능한 정보는 풍부하다. 기존의 방법에서는 이러한 아이디어를 활용할 수 없었다. 논문은 메모리 네트워크를 통해, 가능한 모든 소수로부터 관련된 정보를 읽어 학습하고자 한다. 과거 프레임과 마스크가 외부 메모리를 형성하고, 현재 프레임이 쿼리로서 메모리 속 마스크 정보를 사용하여 세그멘테이션을 수행함 구체적으로 쿼리와 메모리는 피처 스페이스에서 매칭이 된다. (모든 space-time 픽셀 지점에서) Related Works 이전 프레임에서 형상을 추출하고 전파하는 방식 외관 변화에 더 잘 대처하나, 오클루션이나 에러 드리프트의 러버스트가 낮을 수 있다.</description>
    </item>
    <item>
      <title>[논문] Multi-view Optimization of Local Feature Geometry</title>
      <link>https://russellgeum.github.io/posts/review/2021-04-25/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-04-25/</guid>
      <description>Motivation 기존의 로컬 피처 디텍션은 싱글 이미지에서 이루어짐 → 에러가 누적되고 다운스트림 태스크에 악영향
Related Works 이전 논문들은 전통적인 방법이든 CNN 기반 방법이든, 싱글 뷰 이미지에서 로컬 피처 디텍션이 이루어졌다. 피처 매칭 단계에서 멀티 뷰를 고려하는 논문은 있지만, 저자가 아는 한, 더 정확한 키포인트 디텍팅을 위해 멀티뷰를 활용하는 사례는 없었다. Contribution 키포인트를 구성하는 그래프의 모든 엣지에 대해서 멀티뷰 refinement를 수행한다 이전 연구와 비슷하게 샴-네트워크와 코릴레이션 방법을 선택 파이널 플로우는 CNN, FCN을 통해 예측되어진다.</description>
    </item>
    <item>
      <title>[논문] Stand-Alone Self-Attention in Vision Models</title>
      <link>https://russellgeum.github.io/posts/review/2021-04-23/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-04-23/</guid>
      <description>Motivation 컴퓨터 비전에서 셀프 어텐션은 피처 스케일이 충분히 작아야 가능함 → 충분히 큰 피처맵에서도 셀프 어텐션 계산이 가능할까? 그리고 글로벌 어텐션은 계산량이 너무 많음 CNN이 없이 완전히 홀로 설 수 있는 셀프 어텐션 기반의 비전 모델을 제안 Related Work 이전에는 channel-wise, spatial-wise 등의 셀프 어텐션이 등장하였고, 적은 오버 헤드로 CNN 레이어 사이에 셀프 어텐션을 끼울 수 있었음 그러나 글로벌 어텐션 특성 상, 이미지 혹은 피처맵이 충분히 다운 샘플링 되어야 함 Contribution 모든 영역에서 어텐션을 계산하지 않음 → CNN의 로컬리티를 보증하면서도 어텐션을 계산할 수 있는 구조를 제안, 계산량을 줄일 수 있음 중심 픽셀을 쿼리로 두고, 그 주변 픽셀의 로컬 영역을 키와 밸류로 두어서 어텐션을 계산 Convolution STEM은 엣지 등의 정보를 파악하는 매우 중요한 요소</description>
    </item>
    <item>
      <title>[논문] Incorporating Convolution Design into Visual Transformers</title>
      <link>https://russellgeum.github.io/posts/review/2021-04-20/</link>
      <pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-04-20/</guid>
      <description>Motivation 트랜스포머는 대규모 데이터셋이 있을떄 CNN 모델에 필적하는 성능을 보임.
CNN의 로칼리티, 인덕티브 바이어스를 적극 활용하는 디자인의 트랜스포머 모델을 고안할 수 있을까?
Related Works ViT는 대규모 이미지 데이터셋을 이용해서 CNN에 필적하는 성능을 보임
→ 그러나 대규모 데이터는 컴퓨터 리소스의 요구가 크고, 훈련이 오래 걸림 DeiT는 잘 학습된 대규모 CNN 모델을 티처로 두고 KD를 통해, 비전 트랜스포머 모델을 학습시키려 고함
→ 이 역시 대규모 CNN 모델을 미리 준비해야한다는 단점 트랜스포머 태생이 인덕티브 바이어스를 반영하는 것이 어렵고, 불충분한 데이터로부터의 일반화 능력이 부족함</description>
    </item>
    <item>
      <title>[논문] Skip-Convolutions for Efficient Video Processing</title>
      <link>https://russellgeum.github.io/posts/review/2021-04-02/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-04-02/</guid>
      <description>Motivation 비디오는 정지된 이미지의 연속일수도 있고, 변화하는 이미지의 연속일수도 있다.
우리는 세상을 비디오로 인지 → 즉, 변화를 인지 → 변화를 느낀다는 건, 프레임간 차이 (residual)이 누적되면서 어느 임계를 넘어가서 알아채는 것. 이러한 동기로 몇 가지 연구들이 있다. (뉴로모픽, 이벤트 카메라, SNN 등등) 그러나 아직까지가 주류가 아님.
Related Works 기존의 비디오 처리는 픽셀 레벨의 dense prediction을 요구하는 경우가 많음 → 모든 프레임을 모델에 넣어서 연산
프레임 수가 증가할수록 연산량 오버헤드가 리니어하게 증가 → 심지어 새로운 변화가 없어도 계산을 해야만 함</description>
    </item>
    <item>
      <title>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
      <link>https://russellgeum.github.io/posts/review/2021-04-01/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-04-01/</guid>
      <description>Motivation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)</description>
    </item>
    <item>
      <title>[논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-30/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-30/</guid>
      <description>Motivation MoCo 구조를 비디오 도메인으로 확장
Related Works 생략
Contribution Propose temporallly adversarial learning to improve the feature representation of the encoder
ConvLSTM을 통해 프레임 마스크를 출력 → Discriminator(encoder)를 통해
쿼리 피처와 프레임 피처를 출력 → 프레임이 같으면 0, 마스킹된 것은 차이가 최대
마스킹 프레임의 피처를 잘 배울 수 있도록 이 차이가 최대가 되도록 학습
Propose temporal decay to reduce the effect from historical keys in the memory queyes during contrastive learning</description>
    </item>
    <item>
      <title>[논문] CvT, Introducing Convolution to Vision Transformer</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-25/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-25/</guid>
      <description>Motivation 트랜스포머를 적용한 비전 모델은 더 적은 데이터로 학습하고 비슷한 사이즈의 ResNet보다 성능이 낮음. 그 이유는 비전 태스크에서 CNN이 가지는 장점을 ViT는 활용할 수 없음. 이미지는 pixel간 local correlation이 있고 CNN은 이걸 잘 잡아내는데, ViT는 이 능력이 부족함. 이러한 CNN의 local correlation에는 shift, scale, distortion invariance가 있음
Related Works 생략
Contribution 가장 큰 핵심은 트랜스포머의 MLP를 컨볼루션으로 대체한 것
Convolutional Token Embedding Layer
이전의 CvT 출력을 입력으로 받아서, 새로운 토큰을 만드는 함수 f를 정의.</description>
    </item>
    <item>
      <title>[논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-15/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-15/</guid>
      <description>Motivation 이 논문은 일반적인 컴퓨터 비전에서의 트랜스포머 백본을 제공하기 위함이다. 자연어처리에서의 트랜스포머가 비전으로 옮겨올 때, 두 도메인에서의 차이가 있었다.
하나는 비주얼 객체의 다양한 바리에이션이고 단어와 비교해서 이미지의 높은 해상도가 문제이다. → 이미지나 이미지 패치에 직접 트랜스포머를 적용하면 계산량이 쿼드라틱하게 증가한다.
Related Works 작년 10월, 구글의 ViT는 비전 태스크에 컴퓨터 비전 분야는 CNN가 지배적임. AlexNet부터 더 크고, 다양하고, 정교한 기술들로 CNN backbone들이 발전함. 한편 자연어 분야는 트랜스포머가 지배적 → 트랜스포머는 데이터의 long range dependency를 잘 반영함 (언어의 특징).</description>
    </item>
    <item>
      <title>[논문] Representation Learning with Convtrastive Predictive Coding</title>
      <link>https://russellgeum.github.io/posts/devops/2021-03-07/</link>
      <pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/devops/2021-03-07/</guid>
      <description>Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다.</description>
    </item>
    <item>
      <title>[논문] ViViT, A Video Vision Transformer</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-05/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-05/</guid>
      <description>Motivation 비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
Contribution 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론 regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌 비디오 임베딩 ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing 다른 하나는 토큰 차원을 temporal로 확장해서 사용 세 가지 구조 모델 1</description>
    </item>
    <item>
      <title>[논문] Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLR v2)</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-02/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-02/</guid>
      <description>용어의 정리 task-agnostic: 태스크에 구애받지 않는 fine-tuning 할 때 사용하는 태스크는 최종 태스크 (specific task) Motivation 레이블이 없는 방대한 데이터를 잘 활용하면서, 몇 가지 레이블로만 학습 효율을 높이는 방법론 중 하나는 비지도 학습 기반의 사전 훈련과 fine-tuning이다. 즉, 레이블이 없는 방대한 데이터를 통한 비지도 학습으로 좋은 representation을 얻은 후, 이를 통해 적은 레이블의 데이터만으로 fine-tuning을 하는 것 이러한 방법론을 컴퓨터 비전에서는 어떻게 할 수 있을지에 대한 연구이다.
Related Work 이미 자연어 처리에서는 지배적인 방법이다.</description>
    </item>
    <item>
      <title>[논문] A Simple Framework for Contrastive Learning of Visual Representations</title>
      <link>https://russellgeum.github.io/posts/review/2021-03-01/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-03-01/</guid>
      <description>용어의 정의 Pretext task: represenation learning을 위해 수행되는 태스크 Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는 문제를 푸는 것 Motivation 모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?
Related Work Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음
Generative
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음 Discriminative supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등) Contribution representation learning에서 data augmentation에 대한 체계적인 고민이 없었음.</description>
    </item>
    <item>
      <title>[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer</title>
      <link>https://russellgeum.github.io/posts/review/2021-02-25/</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-02-25/</guid>
      <description>Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다.</description>
    </item>
    <item>
      <title>[논문] TCLR: Temporal Contrastive Learning for Video Representation</title>
      <link>https://russellgeum.github.io/posts/review/2021-02-22/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-02-22/</guid>
      <description>Motivation 비디오 레프리젠테이션 러닝에서 쓰일만한 두 가지 콘트라스티브 러닝 프레임워크를 제안. 첫 번째 로스는 같은 비디오에서 겹치지 않는 클립 간의 콘트라스티브 러닝. 두 번째 로스는 피처의 시간적 다양성을 위해서 입력 클립의 피처맵에서 타임 스탬프 간을 구분하는 콘트라스티브 러닝. 좋은 표현 학습은 다운스트림 태스크의 성능을 좋게 만들 수 있음
Related Work 생략
Contribution Local-Local Temporal Contrastive Learning
같은 비디오에서 같은 local timestamp의 (augmentation를 먹이더라도) 비디오 클립은 서로 attract 같은 비디오에서 다른 local timestamp의 비디오 클립은 서로 repel</description>
    </item>
    <item>
      <title>[논문] Spatiotemporal Contrastive Video Representation Learning</title>
      <link>https://russellgeum.github.io/posts/review/2021-02-15/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-02-15/</guid>
      <description>Motivation 비디오의 비지도 표현 학습을 위해, 시간-공간적 맥락에서 contrastive learning을 적용 풍부한 표현 학습을 위해 효과적인 spatial-tempral augmentation 방법을 연구 Related Work 생략
Contribution Contrasitve learning
임베딩 스페이스의 피처 벡터들을 쫙 나열한 다음에 유사한 피처들은 거리가 가깝게끔 학습 (유사도가 낮은 것은 거리가 먼 것이므로 패널티를 주지 않음) 이를 통해서 같은 비디오의 tempral distant가 있는 두 비디오 클립의 encoder는 attract하고, 다른 비디오는 repel하게끔 학습 (SimCLR 참고)
Temporal sampling strategy, consistenc spatial augmentation</description>
    </item>
    <item>
      <title>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</title>
      <link>https://russellgeum.github.io/posts/review/2021-09-31/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/review/2021-09-31/</guid>
      <description>Motivation 기존 뎁스 추정은 correspondence estimation으로 풀었다. 그러나 이 과정에는 문제가 있음
Conventional 방법은 텍스쳐가 약하거나, non-Lambertian 표면에서 문제가 생김 딥러닝 기반은 뎁스 consistency가 일정하지 않고, photometric consistency에서 3D 정보를 제대로 반영하지 못하는 문제 이 논문은 NeRF의 힘을 빌려, 멀티 뷰 스테레오 뎁스 추정을 하고자 함
correspondence estimation과 corr view depth reprojection 최적화 대신에, 이 논문은 다이렉트로 부피를 최적화함 → 그런데 NeRF에서는 shape-radiance ambiguity 문제가 있음. 이를 해결하기 위해 뎁스 프라이어 기반의 NeRF 훈련 가이던스를 제안함 Related Work Conventional 방법은 textuless, non-lambertian 표면에서 문제가 발생 Learning based 방법은 depth consistency 일정하지 않고, photometric loss가 3D 정보를 반영하지 못함 Contribution Depth consistency를 해결하기 위한 제안 방법</description>
    </item>
  </channel>
</rss>
