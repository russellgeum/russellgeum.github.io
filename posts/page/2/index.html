<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Posts - 5biwan's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[기술] GPU와 CUDA (5) - 스레드 레이아웃</h2></header><div class=entry-content><p>스레드 레이아웃 스레드 레이아웃 결정 앞서 CUDA 커널의 레이아웃은 그리드와 블록의 형태로 결정한다고 하였다.
구체적으로 다음의 과정을 따른다.
블록 형태 결정 (즉, 스레드를 어떻게 배치할껀지 결정) 데이터의 크기 및 블록 형태에 따라 그리드 형태 결정 블록 형태는 커널의 알고리즘 특성과 GPU 환경을 고려하여야 한다. 이때 레지스터, 공유 메모리 크기 등도 고려해야 할 요소이다.
큰 벡터의 합을 연산하는 CUDA 커널 (2) 벡터 차원이 1,024보다 크면 블록을 여러 개 지정해야 한다. 하나의 블록이었다면, 각 스레드가 벡터의 첫 번째 원소부터 담당하여 연산한다....</p></div><a class=entry-link aria-label="post link to [기술] GPU와 CUDA (5) - 스레드 레이아웃" href=https://russellgeum.github.io/posts/technical/2024-07-03/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[기술] GPU와 CUDA (4) - CUDA 연산 구조</h2></header><div class=entry-content><p>CUDA 스레드 계층 스레드 CUDA 스레드 계층에서 가장 작은 단위는 스레드이다. 따라서 CUDA 연산을 수행하거나, 코어를 사용하는 기본 단위이다. 커널 호출 시, CUDA 커널 코드는 모든 스레드에 공유된다. 각 스레드는 커널을 독립적으로 실행한다.
워프 CUDA 스레드 계층의 두 번째 계층은 워프이다. 워프는 32개 스레드를 하나로 묶은 단위이다. 중요한 점은 워프는 디바이스에서 하나의 제어 장치에 의해 제어된다. GPU의 SIMT 구조에서 멀티 스레드 단위가 바로 워프이다. 이 말은 1개의 명령어에 의해 32개 스레드가 동시에 움직이는 것을 의미한다....</p></div><a class=entry-link aria-label="post link to [기술] GPU와 CUDA (4) - CUDA 연산 구조" href=https://russellgeum.github.io/posts/technical/2024-06-29/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[기술] GPU와 CUDA (3) - CPU와 GPU의 벡터 합 연산</h2></header><div class=entry-content><p>벡터 합을 구하는 호스트 프로그램 #include &lt;stdio.h> #inlcude &lt;stdlib.h> #include &lt;string.h> #define NUM_DATA 1024 int main(void) { int* a, * b, * c; int memSize = sizeof(int) * NUM_DATA a = new int[NUM_DATA]; memset(a, 0, memSize); b = new int[NUM_DATA]; memset(b, 0, memSize); c = new int[NUM_DATA]; memset(c, 0, memSize); for (int i = 0; i &lt; NUM_DATA; i++) { a[i] = rand() % 10; b[i] = rand() % 10; } for (int i = 0; i &lt; NUM_DATA; i++) { c[i] = a[i] + b[i]; } delete[] a; delete[] b; delete[] c; } 벡터 합을 구하는 디바이스 프로그램 #include "cuda_runtime....</p></div><a class=entry-link aria-label="post link to [기술] GPU와 CUDA (3) - CPU와 GPU의 벡터 합 연산" href=https://russellgeum.github.io/posts/technical/2024-06-28/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[기술] GPU와 CUDA (2) - CPU와 GPU 통신</h2></header><div class=entry-content><p>호스트와 디바이스 호스트 호스트는 일반적으로 CPU를 의미한다. 따라서 호스트 코드는 CPU에서 실행되는 코드를 의미한다. 또한 호스트 메모리는 CPU가 사용하는 시스템 메모리이다. (DRAM)
디바이스 디바이스는 일반적으로 GPU를 의미한다. 따라서 디바이스 코드는 GPU에서 실행되는 코드를 의미한다. 또한 디바이스 코드는 GPU가 사용하는 GPU 메모리이다.
CUDA 프로그램 CUDA 프로그램은 호스트 코드와 디바이스 코드로 구성된다. 프로그램 실행 시 처음 호출되는 코드는 CPU에서 프로세스를 할당하기 때문에, 호스트 코드가 통상 같이 있다. CUDA 프로그램에서 호스트 코드는 gcc와 같은 컴파일러로, 디바이스 코드는 NVCC 컴파일러로 컴파일한다....</p></div><a class=entry-link aria-label="post link to [기술] GPU와 CUDA (2) - CPU와 GPU 통신" href=https://russellgeum.github.io/posts/technical/2024-06-26/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Speculative Decoding</h2></header><div class=entry-content><p>개요 이 글은 스퀴즈비츠의 김태수님이 발표한 내용으로 두 논문을 정리하였다.
LLM에 토큰을 하나씩 생성할 때마다 굉장히 많은 weight를 불러와야 한다. 그래서 DRAM bandwidth가 문제가 된다. Autoregressive 방식이 GPU를 완전하 활용하지 못하는 문제가 발생한다. 이를 해결하기 위한 방법 중 하나로 Speculative Decoding이 있다. Speculative Decoding은 1개의 프롬프트를 1 배치로 처리하는 것이 아니라, 예측한 여러 토큰들을 동시에 재입력하여 병렬 처리하는 기술이다. 따라서 모델은 여러 입력 문장을 배치 단위로 처리한다.
Speculative Decoding 이 논문은 Draft, Verification을 단순하게 구현하여 최적의 토큰을 찾는다....</p></div><a class=entry-link aria-label="post link to [논문] Speculative Decoding" href=https://russellgeum.github.io/posts/research/2024-05-23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[기술] GPU와 CUDA (1) - GPU의 연산 개념</h2></header><div class=entry-content><p>GPU에 관하여 GPU는 방대한 수학 연산을 가속하기 위해 설계된 전자 회로이다. GPU는 CPU에 비해 수천 개의 작은 코어(모델 및 사용 목적에 따라 다름)를 가지고 있기 때문에 GPU 아키텍처는 병렬 처리에 최적화되어 있다. GPU는 여러 작업을 동시에 처리할 수 있으며 그래픽 및 수학적 워크로드에서 더 빠르다.
GPU vs CPU GPU vs CPU 기본적인 GPU 구조 Flynn’s Taxanomy 플린의 분류법은 스탠포드 대학교의 마이클 J. 플린이 컴퓨터 아키텍처를 분류한 것이다. 플린의 분류법의 기본 개념은 간단하다....</p></div><a class=entry-link aria-label="post link to [기술] GPU와 CUDA (1) - GPU의 연산 개념" href=https://russellgeum.github.io/posts/technical/2024-04-06/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Layer Sharing for Parameter-Efficient Transformer</h2></header><div class=entry-content><p>개요 이 글은 Qualcomm의 심규홍 박사님이 발표해주신 자료를 토대로 작성한다.
대상: 트랜스포머를 이해하고 있는 개발자들을 위한 세미나
Motivation 트랜스포머 애플리케이션은 서버 베이스 모델에서는 활발하게 사용되고 있다. 이제 모바일 베이스로 들어갈려고 한다. 트랜스포머는 scaling-law를 따른다. 더 크고, 더 많이 쌓을수록 더 좋은 성능이 나온다. 따라서 돈을 들이면 성능이 보장된다. 그 예시가 LLM이다. 그러나 Efficiecy 관점에서 충분히 고민을 해봐야할 문제가 많다. RAM 사이즈, NPU 퍼포먼스, Cache 사이즈 등 고려해야 할 사항이 많다. On-device LLM에 대한 사이즈가 어느정도 적절할까?...</p></div><a class=entry-link aria-label="post link to [논문] Layer Sharing for Parameter-Efficient Transformer" href=https://russellgeum.github.io/posts/research/2024-04-04/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기</h2></header><div class=entry-content><p>Orion-14B 본 포스팅은 Orion-14B-Chat을 기준으로 한다.
llama.cpp Orion-14B 모델 Orion-14B-Chat in HuggingFace 추론 환경 CMake 설치 brew install cmake llama.cpp 환경 클론 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp llama.cpp 환경 빌드 mkdir build cd build cmake .. cmake --build . --config Release Orion-14B 모델 다운로드 허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation.utils import GenerationConfig tokenizer = AutoTokenizer.from_pretrained("OrionStarAI/Orion-14B", use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM....</p></div><a class=entry-link aria-label="post link to [개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기" href=https://russellgeum.github.io/posts/technical/2024-02-13/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Multimodal Models</h2></header><div class=entry-content><p>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Multimodal Models" href=https://russellgeum.github.io/posts/research/2024-02-04/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Efficient Large Language Models</h2></header><div class=entry-content><p>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Efficient Large Language Models" href=https://russellgeum.github.io/posts/research/2024-01-07/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/posts/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>