<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Posts - 5biwan's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Speculative Decoding</h2></header><div class=entry-content><p>개요 이 글은 스퀴즈비츠의 김태수님이 발표한 내용으로 두 논문을 정리하였다.
LLM에 토큰을 하나씩 생성할 때마다 굉장히 많은 weight를 불러와야 한다. 그래서 DRAM bandwidth가 문제가 된다. Autoregressive 방식이 GPU를 완전하 활용하지 못하는 문제가 발생한다. 이를 해결하기 위한 방법 중 하나로 Speculative Decoding이 있다. Speculative Decoding은 1개의 프롬프트를 1 배치로 처리하는 것이 아니라, 예측한 여러 토큰들을 동시에 재입력하여 병렬 처리하는 기술이다. 따라서 모델은 여러 입력 문장을 배치 단위로 처리한다.
Speculative Decoding 이 논문은 Draft, Verification을 단순하게 구현하여 최적의 토큰을 찾는다....</p></div><a class=entry-link aria-label="post link to [논문] Speculative Decoding" href=https://russellgeum.github.io/posts/tech/2024-05-23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[CS] GPU와 CUDA (1) - GPU의 연산 개념</h2></header><div class=entry-content><p>GPU에 관하여 GPU는 방대한 수학 연산을 가속하기 위해 설계된 전자 회로이다. GPU는 CPU에 비해 수천 개의 작은 코어(모델 및 사용 목적에 따라 다름)를 가지고 있기 때문에 GPU 아키텍처는 병렬 처리에 최적화되어 있다. GPU는 여러 작업을 동시에 처리할 수 있으며 그래픽 및 수학적 워크로드에서 더 빠르다.
GPU vs CPU GPU vs CPU 기본적인 GPU 구조 Flynn’s Taxanomy 플린의 분류법은 스탠포드 대학교의 마이클 J. 플린이 컴퓨터 아키텍처를 분류한 것이다. 플린의 분류법의 기본 개념은 간단하다....</p></div><a class=entry-link aria-label="post link to [CS] GPU와 CUDA (1) - GPU의 연산 개념" href=https://russellgeum.github.io/posts/tech/2024-04-06/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Layer Sharing for Parameter-Efficient Transformer</h2></header><div class=entry-content><p>개요 이 글은 Qualcomm의 심규홍 박사님이 발표해주신 자료를 토대로 작성한다.
대상: 트랜스포머를 이해하고 있는 개발자들을 위한 세미나
Motivation 트랜스포머 애플리케이션은 서버 베이스 모델에서는 활발하게 사용되고 있다. 이제 모바일 베이스로 들어갈려고 한다. 트랜스포머는 scaling-law를 따른다. 더 크고, 더 많이 쌓을수록 더 좋은 성능이 나온다. 따라서 돈을 들이면 성능이 보장된다. 그 예시가 LLM이다. 그러나 Efficiecy 관점에서 충분히 고민을 해봐야할 문제가 많다. RAM 사이즈, NPU 퍼포먼스, Cache 사이즈 등 고려해야 할 사항이 많다. On-device LLM에 대한 사이즈가 어느정도 적절할까?...</p></div><a class=entry-link aria-label="post link to [논문] Layer Sharing for Parameter-Efficient Transformer" href=https://russellgeum.github.io/posts/tech/2024-04-04/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기</h2></header><div class=entry-content><p>Orion-14B 본 포스팅은 Orion-14B-Chat을 기준으로 한다.
llama.cpp Orion-14B 모델 Orion-14B-Chat in HuggingFace 추론 환경 CMake 설치 brew install cmake llama.cpp 환경 클론 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp llama.cpp 환경 빌드 mkdir build cd build cmake .. cmake --build . --config Release Orion-14B 모델 다운로드 허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation.utils import GenerationConfig tokenizer = AutoTokenizer.from_pretrained("OrionStarAI/Orion-14B", use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM....</p></div><a class=entry-link aria-label="post link to [개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기" href=https://russellgeum.github.io/posts/tech/2024-02-13/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Multimodal Models</h2></header><div class=entry-content><p>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Multimodal Models" href=https://russellgeum.github.io/posts/tech/2024-02-04/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Efficient Large Language Models</h2></header><div class=entry-content><p>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Efficient Large Language Models" href=https://russellgeum.github.io/posts/tech/2024-01-07/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[생각] 2023년 회고</h2></header><div class=entry-content><p>1. 졸업과 취업 졸업 (1분기) 석사 디펜스를 끝내고, 완전히 학교를 떠났다. 포항과 서울을 2주마다 오가곤 했으니, 타지에서 지내는 외로움이 컸다. 그렇지만 개개인 퍼포먼스가 훌륭한 연구실 친구들은 나에게 큰 자산이다. 연구적으로 디스커션도 많이하고, 같은 업계에 있으니 서로 동향을 알기 좋은 사람들이다.
일전에도 말한바 있지만, 학위로 얻은 것은 다음과 같이 정리한다.
내가 무엇을 공부를 하든 스스로 커리큘럼을 설계하고, 학습할 수 있는 능력 체계적, 논리적으로 고민하고 그 결과를 계층적 구조로 작문하는 능력 나는 연구가 잘 풀린 사람은 아니다....</p></div><a class=entry-link aria-label="post link to [생각] 2023년 회고" href=https://russellgeum.github.io/posts/essay/2023-12-11/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[생각] 재능있는 척 하지 않기</h2></header><div class=entry-content><p>재능있는 척 하지 않기 재능 있는 척 하지 않기
이 글은 브랜치의 “향로"님의 글을 보고 나에게 대입하여 재정리하였다.
요약하자면 작가는 개발을 잘 하지 못했던 시절에,
따로 공부했다는 사실을 주변에 알리고 싶지 않아했다.
왜냐하면 못한 성과에 대해서 재능이 없는 것이 아니라,
노력이 부족했다는 면피성 명분을 만들수 있기 때문이었다.
나의 이야기 올 여름~가을 나는 회사에서 (개인적으로 스스로가 너무 절망적이었던)
퍼포먼스가 너무 좋지 못한 태스크를 수행중이었다.
나는 “내가 잘 모른다. 어렵다. 그러니 나를 도와주었으면 좋겠다....</p></div><a class=entry-link aria-label="post link to [생각] 재능있는 척 하지 않기" href=https://russellgeum.github.io/posts/essay/2023-11-12/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] ICCV 2023 관심 논문 리스트업</h2></header><div class=entry-content><p>ICCV 2023 ICCV 2023 Link
Papers ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.
(일부 특이한 연구도 포함)
Neural Radiance Fields NeRF-MS: Neural Radiance Fields with Multi-Sequence
Peihao Li et al.
Re-ReND: Real-time Rendering of NeRFs across Devices
Sara Rojas et al.
CLNeRF: Continual Learning Meets NeRF
Zhipeng Cai, Matthias Muller
Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
Hansheng Chen et al.
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields...</p></div><a class=entry-link aria-label="post link to [논문] ICCV 2023 관심 논문 리스트업" href=https://russellgeum.github.io/posts/tech/2023-10-03/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Language Models Compression</h2></header><div class=entry-content><p>대규모 언어 모델의 경량화 동향 Abstract LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬 LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함 Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌 압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함 Introduction & Method 대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다....</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Language Models Compression" href=https://russellgeum.github.io/posts/tech/2023-08-29/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/posts/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>