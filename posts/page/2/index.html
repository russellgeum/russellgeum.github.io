<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Oppenheimer's BLOG</title><meta name=keywords content><meta name=description content="Posts - Oppenheimer's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.320a776f3feea31f033329ad2f4d703286c3a97768974a7bfa19c4f6bd49fc79.css integrity="sha256-Mgp3bz/uox8DMymtL01wMobDqXdol0p7+hnE9r1J/Hk=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[개발] Hugo 테마에서 마크다운 텍스트 양쪽 정렬</h2></header><div class=entry-content><p>Hugo 텍스트 양쪽 정렬 기본적으로 마크다운 문법은 텍스트 양쪽 정렬을 지원하지 않는다.
다만 .scss 파일에 몇 줄 코드 추가로 강제 양쪽 정렬을 할 수 있다.
먼저 아래의 경로로 들어가자.
&lt;blog folder>/assets/themes/_main.scss &lt;blog folder>/assets/themes/_markdown.scss 그리고 아래의 코드를 추가하여 저장한다.
// 글 양쪽 정렬 p { text-align: justify; word-break: break-all; } 다시 리빌드를 하면 텍스트 양쪽 정렬이 된 것을 확인할 수 있다.</p></div><a class=entry-link aria-label="post link to [개발] Hugo 테마에서 마크다운 텍스트 양쪽 정렬" href=https://russellgeum.github.io/posts/2023-07-24/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Pruning vs Quantization: Which is Better?</h2></header><div class=entry-content><p>Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다....</p></div><a class=entry-link aria-label="post link to [논문] Pruning vs Quantization: Which is Better?" href=https://russellgeum.github.io/posts/2023-07-23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[생각] 나의 자아상</h2></header><div class=entry-content><p>의미박탈자와 의미부여자 얼마 전, 모임의 어느 전문가 분에게 짤막한 자아상 분석을 받았다.
몇 가지 질문에 대하여 나는 답을 내려야 했고, 앞으로 고민할 것이 생겼다.
나는 주변을 통제하려는 성향이 있다. -> 왜? 내가 자아상을 엄격하게 바라보기도 한다. 무슨 의미일까? 내가 자아상을 유연하게 바라보기도 한다. 무슨 의미일까? 몇 가지 개념을 들엇다. 그 중 기억에 남는 것이 곧 제목이다.
“의미박탈자라 함은 내 인생에 득이 되지않고 거리를 둬야하는”
“의미부여자라 함는 내 인생을 더 발전시켜 빛이 나도록 해주는”...</p></div><a class=entry-link aria-label="post link to [생각] 나의 자아상" href=https://russellgeum.github.io/posts/2023-07-21/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] SqueezeLLM: Dense-and-Sparse Quantization</h2></header><div class=entry-content><p>Paper Link Sehoon Kim et al (UC Berkeley)
Introduction This paper proposes a Psuedo-PTQ method considering the weight distribution of LLM and outliers.
Motiviation Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. Deploying LLMs for inference has been a significant challenge due to their unprecedented resource requirements. AThis has forced existing deployment frameworks to use multi-GPU inference pipelines, or to use smaller and less performant models....</p></div><a class=entry-link aria-label="post link to [논문] SqueezeLLM: Dense-and-Sparse Quantization" href=https://russellgeum.github.io/posts/2023-07-19/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[회고] 2023년 상반기 회고</h2></header><div class=entry-content><p>1. 작년 졸업을 준비할 쯤, 큰 부상을 당해 팔 수술을 하였다. 팔목에 영구 후유가 생겼는데, 지금도 키보드를 오래 잡으면 조금 시큰하다. 어쨋든 작년에는 취업을 못할 것 같아서 반년 쉬고 준비를 다시 해야할 것 같았다. (회사 지원을 폭 넓게 많이 못했다. 꼭 하고 싶은 분야의 회사로만 지원을 할 수 밖에 없었다.) 그래도 운이 좋게 원하던 회사에 합격하였고, 어느덧 재직 딱 반년 차 주니어이다.
2. (큰 이변이 없는 한) 다시 학교로 돌아가지는 않을 것 같다....</p></div><a class=entry-link aria-label="post link to [회고] 2023년 상반기 회고" href=https://russellgeum.github.io/posts/2023-07-14/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</h2></header><div class=entry-content><p>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인...</p></div><a class=entry-link aria-label="post link to [논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes" href=https://russellgeum.github.io/posts/2021-06-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Does Contrastive Visual Representation Learning Work</h2></header><div class=entry-content><p>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task</p></div><a class=entry-link aria-label="post link to [논문] When Does Contrastive Visual Representation Learning Work" href=https://russellgeum.github.io/posts/2021-05-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</h2></header><div class=entry-content><p>Motiviation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)...</p></div><a class=entry-link aria-label="post link to [논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning" href=https://russellgeum.github.io/posts/2021-04-01/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] ViViT, A Video Vision Transformer</h2></header><div class=entry-content><p>Motiviation 비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
Contribution 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론 regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌 비디오 임베딩 ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing 다른 하나는 토큰 차원을 temporal로 확장해서 사용 세 가지 구조 모델 1...</p></div><a class=entry-link aria-label="post link to [논문] ViViT, A Video Vision Transformer" href=https://russellgeum.github.io/posts/2021-03-05/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] A Simple Framework for Contrastive Learning of Visual Representations</h2></header><div class=entry-content><p>용어의 정의 Pretext task: represenation learning을 위해 수행되는 태스크 Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는 문제를 푸는 것 Motivation 모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?
Related Work Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음
Generative
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음 Discriminative supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등) Contribution representation learning에서 data augmentation에 대한 체계적인 고민이 없었음...</p></div><a class=entry-link aria-label="post link to [논문] A Simple Framework for Contrastive Learning of Visual Representations" href=https://russellgeum.github.io/posts/2021-03-01/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/>«&nbsp;Prev&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>