<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Oppenheimer's BLOG</title><meta name=keywords content><meta name=description content="Posts - Oppenheimer's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] SOFT: Softmax-free Transformer with Linear Complexity</h2></header><div class=entry-content><p>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -> 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?...</p></div><a class=entry-link aria-label="post link to [논문] SOFT: Softmax-free Transformer with Linear Complexity" href=https://russellgeum.github.io/posts/2021-10-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation with Compressed Video</h2></header><div class=entry-content><p>개요 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
이전 연구의 한계 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 제안 방법 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함...</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation with Compressed Video" href=https://russellgeum.github.io/posts/2021-08-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</h2></header><div class=entry-content><p>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인...</p></div><a class=entry-link aria-label="post link to [논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes" href=https://russellgeum.github.io/posts/2021-06-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations</h2></header><div class=entry-content><p>Motivation ViT, MLP 믹서가 어떤 경우에 레즈넷의 성능을 능가할 수 있을까? 의 고찰
ViT, MLP 믹서는 라지 스케일 트레이닝이나, 강한 데이터 arguments를 주어야 했음 모델이 인덕티브 바이어스를 포괄하기 힘들기 때문 그런데 이러한 기법 없이 레즈넷 보다 성능을 올리는 방법을 고민 Related Works 생략
Contribution ViT와 MLP 믹서의 그래디언트 필드는 매우 날카로운 로컬 미니마에 수렴한다는 것을 보여준다. (이는 레즈넷보다 몇 배 더 큼) 이러한 필드는 백프롭때 그래디언트가 누적되고, 초기 임베딩 레이어가 굉장히 큰 헤시안 행렬의 고유값을 가지면서 문제가 될 수 있음 네트워크들은 상대적으로 작은 훈련 에러를 가지고, 특히 MLP 믹서는 ViT보다 오버피팅 가능성이 있다....</p></div><a class=entry-link aria-label="post link to [논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations" href=https://russellgeum.github.io/posts/2021-06-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Does Contrastive Visual Representation Learning Work</h2></header><div class=entry-content><p>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task</p></div><a class=entry-link aria-label="post link to [논문] When Does Contrastive Visual Representation Learning Work" href=https://russellgeum.github.io/posts/2021-05-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal</h2></header><div class=entry-content><p>Motivation Video Instance Segmentation 문제는 동시에 classify, segment, track을 하는 것이다. 이 태스크는 프레임 레벨 VIS보다 성능이 좋다. 그러나 리얼 타임이 아니다. VisTR이 이 문제를 해결하려 했으나, 훈련 시간이 길었다. 그리고 hand-crafted data association이 많이 필요해서 비효율적이다.
Related Works 프레임 레벨 VIS tracking by segmentation 방법 복잡한 data association 알고리즘이 필요 temporal context를 추출하는게 한계가 있음 object occlusion을 핸들링하지 못함 클립 레벨 VIS clip by clip으로 segmentation and tracking 프레임 레벨 VIS보다 long range temporal context를 추출 가능 그러나 실시간성이 부족해서 속도가 느림 Contribution EfficientVIS...</p></div><a class=entry-link aria-label="post link to [논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal" href=https://russellgeum.github.io/posts/2021-05-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Skip-Convolutions for Efficient Video Processing</h2></header><div class=entry-content><p>Motivation 비디오는 정지된 이미지의 연속일수도 있고, 변화하는 이미지의 연속일수도 있다.
우리는 세상을 비디오로 인지 → 즉, 변화를 인지 → 변화를 느낀다는 건, 프레임간 차이 (residual)이 누적되면서 어느 임계를 넘어가서 알아채는 것. 이러한 동기로 몇 가지 연구들이 있다. (뉴로모픽, 이벤트 카메라, SNN 등등) 그러나 아직까지가 주류가 아님.
Related Works 기존의 비디오 처리는 픽셀 레벨의 dense prediction을 요구하는 경우가 많음 → 모든 프레임을 모델에 넣어서 연산
프레임 수가 증가할수록 연산량 오버헤드가 리니어하게 증가 → 심지어 새로운 변화가 없어도 계산을 해야만 함...</p></div><a class=entry-link aria-label="post link to [논문] Skip-Convolutions for Efficient Video Processing" href=https://russellgeum.github.io/posts/2021-04-02/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</h2></header><div class=entry-content><p>Motivation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)...</p></div><a class=entry-link aria-label="post link to [논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning" href=https://russellgeum.github.io/posts/2021-04-01/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples</h2></header><div class=entry-content><p>Motivation MoCo 구조를 비디오 도메인으로 확장
Related Works 생략
Contribution Propose temporallly adversarial learning to improve the feature representation of the encoder
ConvLSTM을 통해 프레임 마스크를 출력
→ Discriminator(encoder)를 통해 쿼리 피처와 프레임 피처를 출력
→ 프레임이 같으면 0, 마스킹된 것은 차이가 최대
마스킹 프레임의 피처를 잘 배울 수 있도록 이 차이가 최대가 되도록 학습
Propose temporal decay to reduce the effect from historical keys in the memory queyes during contrastive learning...</p></div><a class=entry-link aria-label="post link to [논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples" href=https://russellgeum.github.io/posts/2021-03-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] ViViT, A Video Vision Transformer</h2></header><div class=entry-content><p>Motivation 비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
Contribution 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론 regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌 비디오 임베딩 ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing 다른 하나는 토큰 차원을 temporal로 확장해서 사용 세 가지 구조 모델 1...</p></div><a class=entry-link aria-label="post link to [논문] ViViT, A Video Vision Transformer" href=https://russellgeum.github.io/posts/2021-03-05/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/page/2/>«&nbsp;Prev&nbsp;</a>
<a class=next href=https://russellgeum.github.io/posts/page/4/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>