<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Posts - 5biwan's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</h2></header><div class=entry-content><p>Motivation 호모그래피는 스테레오 비전의 근본이다. 영상이 대략 회전 모션이거나 장면이 평면 표면에 가까우면 호모그래피 행렬을 근사할 수 있다. 장면이 제약 조건을 만족하면 직접 호모그래피를 계산할 수 있다. 시맨텍 어웨어하고 러버스트한 호모그래피 추정 딥러닝 알고리즘을 개발 Related Works 생략
Contribution 두 이미지를 인코더
레즈넷34 백본을 받아서 3x3, 8DoF의 호모그래피 행렬을 추정
호모그래피 추정을 위해 Triplet Loss를 사용
호모그래피 추정이 완벽하다면 호모그래피를 통한 와핑이 잘 되어야 함 그래서 와핑한 피처 혹은 이미지가 타겟 피처 또는 이미지와 잘 얼라인 되어야 함 두 번째 로스 텀은 잘 모르겠음 호모그래피 a→b에서 b→a는 identity로 레귤라이저를 추가함 Content-aware prob map...</p></div><a class=entry-link aria-label="post link to [논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions" href=https://russellgeum.github.io/posts/research/2022-07-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Rethinking the Augmentation Module in Contrastive Learning</h2></header><div class=entry-content><p>Motivation CL은 DA에 강력하게 의존하는 방법이다. 인위적인 DA는 다음과 같은 단점이 있다. 데이터 증강의 휴리스틱한 조합은 특정적인 표현 불변성을 가져다 준다. 강력한 데이터 증강은 너무 많은 불변성을 가지고 있어서 오히려 fine-grained한 다운스트림 태스크에 적합하지 않다. 따라서 이 논문은 어디서? 무엇을? 이란 질문으로 DA를 하는 방법론을 소개한다. Related Work 생략
Contribution 다양한 augmentation module 조합을 사용한다.
샴 구조는 깊이에 따라 여러 스테이지로 활용한다. 각 스테이지의 피처를 CL에 활용한다.
Hierarchical augmentation invariance...</p></div><a class=entry-link aria-label="post link to [논문] Rethinking the Augmentation Module in Contrastive Learning" href=https://russellgeum.github.io/posts/research/2022-06-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception</h2></header><div class=entry-content><p>Motivation CL이나 특정한 Pretext task는 비디오에서 중요하지 않은 배경에 집중하는 문제가 발생 비디오에는 모션이 있음 이 모션에 집중하기 위한 학습 방법을 제안해야함 Related Work Pretext task 방법
지오메트릭한 정보를 배우는 spatial learning clip order를 학습하는 temporal learning space-time 정보를 학습하는 spatiotemporal learning 그러나 이 방법의 단점은 비디오에 리더던시 정보가 많아서 불필요한 학습을 야기함
배경에 대하여 정적이거나 무관한 정보는 모델의 판단성을 저해할 수 있음
배경 때문에 모델의 비디오 이해도가 낮아질 수 있음...</p></div><a class=entry-link aria-label="post link to [논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception" href=https://russellgeum.github.io/posts/research/2022-04-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Deep Video Prior for Consistency and Propagation</h2></header><div class=entry-content><p>Motivation 비디오 프레임간 시간 불일치성을 해결하기 위해 DVP를 implcit하게 DNN에 주는 방법을 제안 DVP가 무엇인가?
비디오를 사용한 멀티모달 태스크에서는 성능의 흔들림이 심함 → 이터레티브하게 중요도를 재할당하는 전략으로 해결 Related Work 이전 비디오 연구들은 구축된 대규모 비디오 데이터셋이 필요했음 옵티컬 플로우 같은 정보나, 단순 프레임 간 유사도를 비교하는 것만으로는 롱-텀 비디오에 적합하지 않음 이전 비디오 연구들은 멀티 모달 태스크에서 좋은 성능을 골고루 보이기 어려웠음 Contribution DVP가 무엇인가?
DVP는 비디오 처리에서 임플리싯하게 비디오 일관성을 주기 위해 사용되는 성질들을 일컬음...</p></div><a class=entry-link aria-label="post link to [논문] Deep Video Prior for Consistency and Propagation" href=https://russellgeum.github.io/posts/research/2022-01-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Learning Optical Flow, Depth, and Scene FLow without Real-world Labels</h2></header><div class=entry-content><p>Motivation Depth, Sceneflow를 동시에 푸는 것은 ill-posed 문제이고, 수 많은 해가 존재한다. 먼저 옵티컬 플로우를 추정하고, 알려진 포즈와 함께 initial depth를 연산한다. 그리고 sceneflow, depth를 refinement하는 파이프라인을 제안한다.
(그러니까 원 스테이지로는 하기가 힘드니 투 스테이지로 해보겠다는 의미) Related Works 비디오 기반의 SSL를 통한 3D perception 학습들은 아래의 네 가지 태스크로 나뉜다.
Ego-motion estimation Monocular Depth estimation → Scale ambiguity, Static assumption 문제 발생 Opticalflow estimation Sceneflow estimation → Can not handle sceneflow from opticalflow esitmator (indirectly estimation), stereo manner at training time 모두 개별 태스크에서 우수한 성능을 보이지만, scale ambuiguity 문제가 있다....</p></div><a class=entry-link aria-label="post link to [논문] Learning Optical Flow, Depth, and Scene FLow without Real-world Labels" href=https://russellgeum.github.io/posts/research/2022-01-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Masked Autoencoders Are Scalable Vision Learners</h2></header><div class=entry-content><p>Motivation 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이 복원할 수 있을까?
비대칭 형태의 인코더 - 디코더
인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이 latent vector를 가지고 원래의 이미지를 복원
인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성
Related Works 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만, 비전에서 오토인코딩으로의 진전 X
저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가?
자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다....</p></div><a class=entry-link aria-label="post link to [논문] Masked Autoencoders Are Scalable Vision Learners" href=https://russellgeum.github.io/posts/research/2021-12-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Neural Scene Flow Prior</h2></header><div class=entry-content><p>Motivation Scene Flow prior를 신경망으로 강력하게 regularizaion할 수 있는 식을 주장
Related Works 지도 학습 기반의 SF 추정은 많은 GT가 필요 → 그러나 prior가 강한 regularizer라서 필요 없음
SSL 기반 SF 추정은 도메인 스페시픽하고, 일반화 성능이 떨어짐 그리고 충분히 많은 양의 데이터가 필요함 → regularization이 약함
Contribution MLP로 구성된 네트워크에 S1, S2 간의 Forward, Backward optimazation 식을 제안
이 식의 목적은 S1, S2 사이의 distance를 최소화하는 MLP 파라미터를 찾는 것 (regularization 텀은 Laplacian regularizer)...</p></div><a class=entry-link aria-label="post link to [논문] Neural Scene Flow Prior" href=https://russellgeum.github.io/posts/research/2021-11-21/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio</h2></header><div class=entry-content><p>Motivation Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters?
Despite recent advances across different domains and tasks, current state-ofthe-art methods train a separate model with different model parameters for each task at hand.
Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains....</p></div><a class=entry-link aria-label="post link to [논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio" href=https://russellgeum.github.io/posts/research/2021-11-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] MonoPLFlowNet: Permutohedral Lattice FlowNet for Real-Scale 3D Scene Flow Estimation with Monocular Images</h2></header><div class=entry-content><p>Motiviation 3D scene flow는 라이다나 스테레오 환경에서만 real scale을 알 수 있음, 모노큘라 환경에서는 알지 못함 real scale을 알기 위해서는 GT 뎁스나 GT point cloud를 알아야 했음, 이 논문은 2장의 모노큘라 시퀀스로부터 real scale scene flow를 알아내기 위함임
Related Works 논문 갈래 정리: PointNet → PointNet++ // FlowNet → PWC-Net → FlowNet3D → PointPWCNet
포인트 클라우드 기반은 라이다가 필요 → 라이다는 너무 비쌈 스테레오를 이용한 방법 → 카메라칸 캘리가 필요 → 굳이?...</p></div><a class=entry-link aria-label="post link to [논문] MonoPLFlowNet: Permutohedral Lattice FlowNet for Real-Scale 3D Scene Flow Estimation with Monocular Images" href=https://russellgeum.github.io/posts/research/2021-11-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Decoupled Contrastive Learning</h2></header><div class=entry-content><p>Motivation CL의 로스에서 각 편미분에 대해 커플링되는 텀이 있음 → 이는 학습 효율성에 관여
Related Works CL은 학습에서 많은 양의 네거티브가 필요하다 → 큰 배치 사이즈를 요구로 함 → 이는 어쩔수없이 컴퓨터 리소스가 많이 필요 따라서 배치 사이즈에 민감하다. Contribution Infoloss를 각각 편미분 하였을때 공통된 텀이 포함되는 것을 보임 이 텀은 P/N 간 커플링 되는 것을 의미하고 학습 효율성에 영향을 줄 수 있음
예를 들어 N이 가깝고 P도 가까우면, N의 grad 또한 감소....</p></div><a class=entry-link aria-label="post link to [논문] Decoupled Contrastive Learning" href=https://russellgeum.github.io/posts/research/2021-11-01/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/page/3/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/posts/page/5/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>