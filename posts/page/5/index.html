<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Posts - 5biwan's BLOG"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/posts/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/posts/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Posts"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/posts/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Posts"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span class=active>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Posts
<a href=/posts/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Learning Optical Flow, Depth, and Scene FLow without Real-world Labels</h2></header><div class=entry-content><p>Motivation Depth, Sceneflow를 동시에 푸는 것은 ill-posed 문제이고, 수 많은 해가 존재한다. 먼저 옵티컬 플로우를 추정하고, 알려진 포즈와 함께 initial depth를 연산한다. 그리고 sceneflow, depth를 refinement하는 파이프라인을 제안한다.
(그러니까 원 스테이지로는 하기가 힘드니 투 스테이지로 해보겠다는 의미) Related Works 비디오 기반의 SSL를 통한 3D perception 학습들은 아래의 네 가지 태스크로 나뉜다.
Ego-motion estimation Monocular Depth estimation → Scale ambiguity, Static assumption 문제 발생 Opticalflow estimation Sceneflow estimation → Can not handle sceneflow from opticalflow esitmator (indirectly estimation), stereo manner at training time 모두 개별 태스크에서 우수한 성능을 보이지만, scale ambuiguity 문제가 있다....</p></div><a class=entry-link aria-label="post link to [논문] Learning Optical Flow, Depth, and Scene FLow without Real-world Labels" href=https://russellgeum.github.io/posts/research/2022-01-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Masked Autoencoders Are Scalable Vision Learners</h2></header><div class=entry-content><p>Motivation 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이 복원할 수 있을까?
비대칭 형태의 인코더 - 디코더
인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이 latent vector를 가지고 원래의 이미지를 복원
인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성
Related Works 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만, 비전에서 오토인코딩으로의 진전 X
저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가?
자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다....</p></div><a class=entry-link aria-label="post link to [논문] Masked Autoencoders Are Scalable Vision Learners" href=https://russellgeum.github.io/posts/research/2021-12-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Neural Scene Flow Prior</h2></header><div class=entry-content><p>Motivation Scene Flow prior를 신경망으로 강력하게 regularizaion할 수 있는 식을 주장
Related Works 지도 학습 기반의 SF 추정은 많은 GT가 필요 → 그러나 prior가 강한 regularizer라서 필요 없음
SSL 기반 SF 추정은 도메인 스페시픽하고, 일반화 성능이 떨어짐 그리고 충분히 많은 양의 데이터가 필요함 → regularization이 약함
Contribution MLP로 구성된 네트워크에 S1, S2 간의 Forward, Backward optimazation 식을 제안
이 식의 목적은 S1, S2 사이의 distance를 최소화하는 MLP 파라미터를 찾는 것 (regularization 텀은 Laplacian regularizer)...</p></div><a class=entry-link aria-label="post link to [논문] Neural Scene Flow Prior" href=https://russellgeum.github.io/posts/research/2021-11-21/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio</h2></header><div class=entry-content><p>Motivation Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters?
Despite recent advances across different domains and tasks, current state-ofthe-art methods train a separate model with different model parameters for each task at hand.
Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains....</p></div><a class=entry-link aria-label="post link to [논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio" href=https://russellgeum.github.io/posts/research/2021-11-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] MonoPLFlowNet: Permutohedral Lattice FlowNet for Real-Scale 3D Scene Flow Estimation with Monocular Images</h2></header><div class=entry-content><p>Motiviation 3D scene flow는 라이다나 스테레오 환경에서만 real scale을 알 수 있음, 모노큘라 환경에서는 알지 못함 real scale을 알기 위해서는 GT 뎁스나 GT point cloud를 알아야 했음, 이 논문은 2장의 모노큘라 시퀀스로부터 real scale scene flow를 알아내기 위함임
Related Works 논문 갈래 정리: PointNet → PointNet++ // FlowNet → PWC-Net → FlowNet3D → PointPWCNet
포인트 클라우드 기반은 라이다가 필요 → 라이다는 너무 비쌈 스테레오를 이용한 방법 → 카메라칸 캘리가 필요 → 굳이?...</p></div><a class=entry-link aria-label="post link to [논문] MonoPLFlowNet: Permutohedral Lattice FlowNet for Real-Scale 3D Scene Flow Estimation with Monocular Images" href=https://russellgeum.github.io/posts/research/2021-11-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Decoupled Contrastive Learning</h2></header><div class=entry-content><p>Motivation CL의 로스에서 각 편미분에 대해 커플링되는 텀이 있음 → 이는 학습 효율성에 관여
Related Works CL은 학습에서 많은 양의 네거티브가 필요하다 → 큰 배치 사이즈를 요구로 함 → 이는 어쩔수없이 컴퓨터 리소스가 많이 필요 따라서 배치 사이즈에 민감하다. Contribution Infoloss를 각각 편미분 하였을때 공통된 텀이 포함되는 것을 보임 이 텀은 P/N 간 커플링 되는 것을 의미하고 학습 효율성에 영향을 줄 수 있음
예를 들어 N이 가깝고 P도 가까우면, N의 grad 또한 감소....</p></div><a class=entry-link aria-label="post link to [논문] Decoupled Contrastive Learning" href=https://russellgeum.github.io/posts/research/2021-11-01/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] SOFT: Softmax-free Transformer with Linear Complexity</h2></header><div class=entry-content><p>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -> 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?...</p></div><a class=entry-link aria-label="post link to [논문] SOFT: Softmax-free Transformer with Linear Complexity" href=https://russellgeum.github.io/posts/research/2021-10-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Non Deep Networks</h2></header><div class=entry-content><p>Motivation DNN의 깊이가 깊어지면 단점이 많음 → 레이턴시가 길어지기 때문에 빠른 반응을 필요로 하는 애플리케이션이 부적합 어떻게 하면 얕은 깊이의 DNN으로도 충분한 성능을 낼 수 있을까? → 해답은 패러렐한 뉴럴넷 구성으로 성능을 낼 수 있다. Related Works 생략
Contribution 구체적으로 ~10 레이어, ~12 레이어까지 적절함을 말한다.
VGG 스타일의 블록을 사용한다. (구체적으로 Rep-VGG을 빌리지만, 목적에 맞게 조금 수정)
제한된 네트워크 깊이로 receptive field가 좁다. 이를 해결하기 위해, Squeeze-Exicitation 레이어에 기반한 SSE 레이어를 추가하였다....</p></div><a class=entry-link aria-label="post link to [논문] Non Deep Networks" href=https://russellgeum.github.io/posts/research/2021-10-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation with Compressed Video</h2></header><div class=entry-content><p>Motivation 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
Related Works 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 Contribution 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함...</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation with Compressed Video" href=https://russellgeum.github.io/posts/research/2021-08-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Contextual Transformer Networks for Visual Recognition</h2></header><div class=entry-content><p>Motivation 비전 태스크에서 셀프 어텐션의 계산이, 즉 공간적인 위치에서 Q, K가 서로 independent하게 계산이 되어지는 것이 단점 → context가 필요
Related Works CNN의 receptive field를 넓히는 것 → context를 잘 보긴 하지만, long range dependecy를 보지 못함 ViT, long range dependency를 보기는 하지만, independent한 Q, K의 interaction을 계산 Contribution 기존의 conventional self-attention은 서로 다른 위치간의 interaction을 잘 계산. 그러나 모든 pairwise Q-K relation은 independent함 → 풍부한 context를 보지 못함, 따라서 Conetxt Transformer 구조를 제안....</p></div><a class=entry-link aria-label="post link to [논문] Contextual Transformer Networks for Visual Recognition" href=https://russellgeum.github.io/posts/research/2021-07-30/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/posts/page/4/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/posts/page/6/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>