<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.
스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.
데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다."><meta name=author content="5biwan"><link rel=canonical href=https://russellgeum.github.io/posts/technical/2024-07-25/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://russellgeum.github.io/apple-touch-icon.png><link rel=mask-icon href=https://russellgeum.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/technical/2024-07-25/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><meta name=theme-color content="#ffffff"><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈"><meta property="og:description" content="CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.
스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.
데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/technical/2024-07-25/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-25T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈"><meta name=twitter:description content="CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.
스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.
데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈","item":"https://russellgeum.github.io/posts/technical/2024-07-25/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈","name":"[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈","description":"CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.\n스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.\n데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다.","keywords":[],"articleBody":"CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.\n스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.\n데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다.\n// 호스트 코드 for (int r = 0; r \u003c m; r++) for (int c = 0; c \u003c n; c++) for (int i = 0; i \u003c k; i++) C[r][c] += A[r][i] * B[i][c] 메모리 접근 측면에서 보면 행렬 A, B에서 read가 반복된다. 그리고 행렬 C의 (row, col) 원소에는 write가 반복된다. 이러한 메모리 접근을 기억하고, 2가지 레이아웃 방법을 비교한다.\nA, B 기준 스레드 레이아웃 행렬 A의 각 행과 행렬 B의 각 열을 하나의 스레드가 처리하면, 스레드가 하는 일이 완전히 독립적이며 이상적일 것 같으나, 스레드의 수가 행 개수 또는 열 개수로 제한된다. CUDA 코어보다도 수십 배의 스레드를 사용해야 하는 것 특성 상, 이러한 스레드 레이아웃은 GPU 유틸라지션에서 적절하지 않다.\n두 행렬 입력을 동시에 고려해볼 수 있다. 모든 for 무느이 각 반복을 하나의 스레드가 처리한다. 행, 열 그리고 차원의 수 만큼 스레드가 할당되어 대규모 병렬 처리가 가능하다. 하지만 C의 입장에서 C(row, col)에 여러 개의 스레드가 동시에 접근하여 값을 갱신해야 한다.\n읽기 연산은 동일한 데이터를 여러 스레드가 읽어도 문제되지 않는다. 하지만 동일한 메모리 영역에 여러 스레드가 쓰기 연산을 하면 잘못된 값이 생성될 수 있다. 이를 해결하기 위해 동기화 기법을 사용할 수 있지만, 스레드들이 직렬화되어 병렬처리 성능을 크게 떨어트린다.\nC 기준 스레드 레이아웃 동기화 문제가 발생하는 연산은 write이다. 행렬 C의 한 원소처럼 동일한 메모리에 여러 스레드가 동시에 write를 하지 않도록 하는 방법 중 하나는 write 영역을 분할하여 스레드에게 분배하는 것이다. 행렬 곱셈 연산은 C의 각 원소를 스레드에 분배함으로써 실현할 수 있다.\nC(row, col)을 담당하는 스레드는 행렬 A의 row 행과 행렬 B의 col 열의 원소들을 순차적으로 읽으며, C의 값을 계산한다. C(row, col + n)을 담당하는 스레드도 행렬 A의 row 행에 동시 접근 가능하지만, read 연산은 동기화 문제가 발생하지 않는다. 따라서 스레드 수가 행렬 C의 원소 수와 동일하다. 대규모 행렬의 경우, GPU에서 충분한 병렬 처리 연산을 수행할 수 있다.\n행렬은 2차원 데이터로, 레이아웃 또한 2차원으로 잡는 것이 일반적이다. 따라서 각 차원의 길이를 행렬 C의 행과 열의 길이와 같게 하는 것이 스레드 인덱싱에서 직관적이다.\n스레드 인덱싱 스레드 레이아웃을 결정하면, 커널에서 각 스레드가 접근할 데이터 위치를 결정하는 스레드 인덱싱 작업을 해야 한다. 결과 행렬 C를 기준으로 스레드 레이아웃을 설정함에 따라, 각 스레드가 접근하는 데이터는 아래와 같다.\n행렬 C 중 자신이 담당하는 원소 C의 원소를 계산하기 위한 행렬 A의 한 행과 B의 한 열 C의 크기가 블록 최대 크기(1,024)보다 작은 경우 이 설명은 블록 내 최대 스레드 수인 1,024개 인 경우, 블록 1개를 사용하는 행렬 연산이다. C가 2차원 스레드 레이아웃을 사용하기 때문에, 각 스레드의 C(row, col) 인덱스는 스레드 ID를 사용하여 지정할 수 있다.\nrow = threadIdx.x col = threadIdx.y 메모리상의 C(row, col) 위치는 row, col 값과 C의 행 길이 n을 이용하여 계산한다. 또한 이처럼 단일 블록의 행렬 연산에서는 행 길이는 스레드 블록의 y-차원 길이와 같다.\nindex = row * n + col index = row * blockDim.y + col C(row, col) 값 연산은 A의 row 번째 행 원소와 B col 번째 원소들을 차례대로 방문해야 한다. offset = 0, 1, … k일때 A(row, offset)과 B(offset, col)에 접근한다. 이를 각 행렬의 목표 원소라 할 때, 행렬에서 목표 원소에 접근하기 위한 인덱싱은 다음과 같다.\nA(row, offset) = row * k + offset B(offset, col) = col + (offset * n) 행렬 A는 행 길이가 k이며, rok * k는 row 행의 첫 번째 원소를 가리킨다. 접근하고자 하는 원소는 row 행의 offset 원소이므로, row * k + offset이다. 반면에 행렬 B는 열이 고정되며 행을 따라 원소를 읽는다. 행렬 B의 행 길이는 n이며, offset 원소 행의 첫 번째 원소는 n * offset이다. 해당 행의 col 번째 원소를 읽으면 된다. 즉 n * offet + col로서 메모리 상 위치를 계산할 수 있다.\n곱셈 커널 아래 코드는 스레드 수가 1024개 이하인 경우에서의 행렬 곱셈 커널이다. 호스트 코드에서 첫 번째, 두 번째 반복문이 병렬화되어 스레드에 분배됨에 따라, 사실상 offset 레벨의 반복문만 남아있다. 스레드 레이아웃에 의해 C의 인덱스를 지정하여, 디바이스 메모리 영역의 실제 메모리에 접근한다.\n// 단일 블록에서의 CUDA 기반 행렬 곱셉 커널 __global__ void matMul_krenl(int* A, int* B, int* C, int m, int n, int k) { int row = threadIdx.x; int col = threadIdx.y; int index = row * n + col; C[index] = 0 for (int offset = 0; offset \u003c k; offset++) { C[index] += A[row*k + offset] * B[col + offset*n]; } } C의 크기가 블록 최대 크기(1,024)보다 큰 경우 이 경우에는 여러 개의 블록을 사용해야 한다. 다만 스레드 레이아웃이 행렬 C와 같은 크기를 가지는 점은 동일하다. 따라서 스레드의 x-차원과 y-차원을 이용해서 행렬 C의 각 원소를 지정할 수 있다. 단일 블록에서는 블록 내 스레드 번호만 사용하면 되었지만, 여러 개의 블록은 스레드의 글로벌 인덱스를 사용해야 한다. 즉, (2차원 그리드, 1차원 블록)이 아닌, (2차원 그리드, 2차원 블록)이다.\n이전에서 사용한 글로벌 인덱스를 구하는 방법을 활용할 수 있다. 목표 원소가 속한 블록의 첫 번째 원소까지 이동한 후, 블록 내부에서 자신의 인덱스까지 이동하면 된다.\nrow = (blockDim.x * blockIdx.y) + threadIdx.x; col = (blockDim.y * blockIdx.y) + threadIdx.y; 곱셈 커널 입력 행렬 A, B의 인덱싱은 C(row, col)을 기준으로 계산한다. 커널 역시 row, col을 계산하는 부분만 수정하면 된다.\n// 1024개 스레드보다 큰 여러 블록에서의 행렬 곱셉 커널 __global__ void matMul_kernel (int* A, int* B, int* C, int m, int n, int k) { int row = (blockDim.x * blockIdx.x) + threadIdx.x; int col = (blockDim.y * blockIdx.y) + threadIdx.y; int index = row * n + col; C[index] = 0; for (int offset = 0; offset \u003c k; offset++) { C[index] += A[row * k + offset] * B[col + offset * n]; } } 구현 및 성능 평가 전체 프로그램 코드\n#define BLOCK_SIZE 16 int main(int argc, char* argv[]) { int m, n, k; m = atoi(argv[1]); n = atoi(argv[2]); k = atoi(argv[3]); int sizeA = m * k; int sizeB = k * n; int sizeC = m * n; int* dA, * dB, * dC; // 1. dA, dB, dC를 위한 디바이스 메모리 할당 cudaMalloc(\u0026dA, sizeA * sizeof(int)); cudaMalloc(\u0026dB, sizeB * sizeof(int)); cudaMalloc(\u0026dC, sizeC * sizeof(int)); cudaMemset(\u0026dA, sizeA * sizeof(int)); cudaMemset(\u0026dB, sizeB * sizeof(int)); cudaMemset(\u0026dC, sizeC * sizeof(int)); // 2. 행렬을 GPU로 데이터 복사 cudaMemcpy(dA, A, sizeA * sizeof(int), cudaMemcpyHostToDevice); cudaMemcpy(dB, B, sizeB * sizeof(int), cudaMemcpyHostToDevice); cudaMemcpy(dC, C, sizeC * sizeof(int), cudaMemcpyHostToDevice); // 3. 스레드 레이아웃 설정 dim3 gridDim(ceil((float) m / BLOCK_SIZE), ceil((float) n / BLOCK_SIZE)); dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE); // 4. 커널 호출 MatMul_kernel \u003c\u003c\u003e\u003e (dA, dB, dC, m, n, k); // 5. 디바이스 메모리에서 호스트 메모리로 데이터 복사 cudaMemcpy(Cgpu, dC, sizeC * sizeof(int), cudaMemcpyDeviceToHost); // 6. 디바이스 메모리 해제 cudaFree(dA); cudaFree(dB); cudaFree(dC); return 0; } 커널을 실제로 호출하기 위해서는 대상 행렬의 크기에 따라 스레드 레이아웃을 결정해야 한다. 또한 커널 수행 시, 사용할 스레드 블록의 크기를 결정해야 한다. 스레드 블록의 크기를 결정했다면, 그리드 크기를 계산해야 한다. 위 구현에서 사용하는 스레드 레이아웃은 연산 결과인 행렬 C를 기준으로 한다. 따라서 C의 원소 수만큼 스레드를 사용해야 한다.\n// 행이 m이고 열이 n인 행렬 C의 필요한 블록 수 x-차원: m / BLOCK_SIZE y-차원: n / BLOCK_SIZE 여기서 문제는 BLOCK_SIZE로 나누어 떨어지지 않는 경우이다. 그런데 그리드는 반드시 정수여야 한다. 만약 내림 연산을 한다면, 오른쪽 원소들을 담당할 스레드가 없어서 해당 원소들을 연산할 수 없다. 따라서 실수 값을 올림 처리하는 것이 유리하다. 이때 ceil() 함수를 통해 누락되는 부분 없이 모든 원소를 연산할 수 있다.\nceil() 연산으로 올림 처리하여, 모든 행렬의 원소를 연산할 수 있지만 행렬을 벗어나는 영역까지 연산을 위해 메모리에 접근하려는 문제가 생긴다. 따라서 메모리 할당을 받지 않는 영역을 접근하는 것은 메모리 접근 위반으로 프로그램에 오류를 발생시킬 수 있다.\n__global__ void matMul_kernel (int* A, int* B, int* C, int m, int n, int k) { int row = (blockDim.x * blockIdx.x) + threadIdx.x; int col = (blockDim.y * blockIdx.y) + threadIdx.y; int index = row * n + col; // 메모리 접근 위반을 방지하는 예외처리 코드 if (row \u003e= m || col \u003e= n) { return; } C[index] = 0 for (int offset = 0; offset \u003c k; offset++) { C[index] += A[row * k + offset] * B[col + offset * n]; } } 스레드가 담당할 원소 (row, col)이 행렬 C의 범위를 벗어나는지 체크한다. 만약 범위를 벗어나면, 연산할 원소가 아니기 때문에 스레드를 종료하면 된다. (이를 return 처리 한다.) 마지막으로 GPU로 연산을 위해서는, 디바이스 메모리 공간을 확보해야 한다. 메모리를 직접 할당하고 초기화하기 위해, 연산이 종료된 데이터의 메모리는 항상 메모리 해제를 하는 습관을 가진다.\n부동소수점 정밀도 CUDA 연산 중, 쉬운 예제는 데이터가 정수인 경우이다. 하지만 딥러닝, 수치해석 같이 다양한 분야에서 부동소수점 연산을 한다. 따라서 float 데이터형을 가지는 경우를 고려해야 한다. float 연산에서는 CPU와 GPU의 동일한 로직 연산이 다른 경우가 발생할 수 있다.\n이는 부동소수점 표현 방식에서 가지는 정밀도 차이로 발생하는 문제이다. 부동소수점은 다음과 같이 이루어진다.\n부호(sign) 지수부(exponenet) 가수부(fraction) 부호는 += 여부를 결정한다. 지수부는 숫자 표현의 범위를 결정한다. 그리고 가수부는 숫자 표현의 정밀도를 결정한다. 예를 들어서 딥러닝에서 쓰이는 부동소수점 표현 중 같은 16비트 데이터 형이여도, bfloat16, float16은 서로 표현이 다르다.\nbfloat16: 지수부가 8, 가수부가 7이다. float16: 지수부가 5, 가수부가 10이다. bf16은 fp16에 비해 수의 범위가 넓지만, 표현 정밀도가 떨어져 0에 가까운 값은 모두 0으로 표현된다. bf16이 fp16과 다른 특징 때문에 딥러닝 구조와 연산에 따라서 bf16이 더 유리할 수 있다. 예를 들어서 LLM같이 매우 디코더 레이어가 많은 연산이라면,\n","wordCount":"1518","inLanguage":"en","datePublished":"2024-07-25T00:00:00Z","dateModified":"2024-07-25T00:00:00Z","author":{"@type":"Person","name":"5biwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/technical/2024-07-25/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5biwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5biwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#스레드-레이아웃-설정>스레드 레이아웃 설정</a><ul><li><a href=#c--ab>C = AB</a></li><li><a href=#a-b-기준-스레드-레이아웃>A, B 기준 스레드 레이아웃</a></li><li><a href=#c-기준-스레드-레이아웃>C 기준 스레드 레이아웃</a></li></ul></li><li><a href=#스레드-인덱싱>스레드 인덱싱</a><ul><li><a href=#c의-크기가-블록-최대-크기1024보다-작은-경우>C의 크기가 블록 최대 크기(1,024)보다 작은 경우</a></li><li><a href=#곱셈-커널>곱셈 커널</a></li></ul></li><li><a href=#c의-크기가-블록-최대-크기1024보다-큰-경우>C의 크기가 블록 최대 크기(1,024)보다 큰 경우</a><ul><li><a href=#곱셈-커널-1>곱셈 커널</a></li><li><a href=#구현-및-성능-평가>구현 및 성능 평가</a></li><li><a href=#부동소수점-정밀도>부동소수점 정밀도</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=cuda-기반-행렬-곱셈>CUDA 기반 행렬 곱셈<a hidden class=anchor aria-hidden=true href=#cuda-기반-행렬-곱셈>#</a></h1><p>행렬 연산은 CUDA 연산에 가장 어울리는 문제이다.
따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.</p><h2 id=스레드-레이아웃-설정>스레드 레이아웃 설정<a hidden class=anchor aria-hidden=true href=#스레드-레이아웃-설정>#</a></h2><p>대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다.
어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.</p><ul><li>데이터를 읽는 행렬 A, B 기준</li><li>결과가 저장되는 행렬 C 기준</li><li></li></ul><h3 id=c--ab>C = AB<a hidden class=anchor aria-hidden=true href=#c--ab>#</a></h3><p>이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다.</p><pre tabindex=0><code>// 호스트 코드
for (int r = 0; r &lt; m; r++) 
    for (int c = 0; c &lt; n; c++)
        for (int i = 0; i &lt; k; i++)
            C[r][c] += A[r][i] * B[i][c]
</code></pre><p>메모리 접근 측면에서 보면 행렬 A, B에서 read가 반복된다. 그리고 행렬 C의 (row, col) 원소에는 write가 반복된다.
이러한 메모리 접근을 기억하고, 2가지 레이아웃 방법을 비교한다.</p><h3 id=a-b-기준-스레드-레이아웃>A, B 기준 스레드 레이아웃<a hidden class=anchor aria-hidden=true href=#a-b-기준-스레드-레이아웃>#</a></h3><p>행렬 A의 각 행과 행렬 B의 각 열을 하나의 스레드가 처리하면, 스레드가 하는 일이 완전히 독립적이며 이상적일 것 같으나, 스레드의 수가 행 개수 또는 열 개수로 제한된다. CUDA 코어보다도 수십 배의 스레드를 사용해야 하는 것 특성 상, 이러한 스레드 레이아웃은 GPU 유틸라지션에서 적절하지 않다.</p><p>두 행렬 입력을 동시에 고려해볼 수 있다. 모든 for 무느이 각 반복을 하나의 스레드가 처리한다. 행, 열 그리고 차원의 수 만큼 스레드가 할당되어 대규모 병렬 처리가 가능하다. 하지만 C의 입장에서 C(row, col)에 여러 개의 스레드가 동시에 접근하여 값을 갱신해야 한다.</p><p>읽기 연산은 동일한 데이터를 여러 스레드가 읽어도 문제되지 않는다. 하지만 동일한 메모리 영역에 여러 스레드가 쓰기 연산을 하면 잘못된 값이 생성될 수 있다. 이를 해결하기 위해 동기화 기법을 사용할 수 있지만, 스레드들이 직렬화되어 병렬처리 성능을 크게 떨어트린다.</p><h3 id=c-기준-스레드-레이아웃>C 기준 스레드 레이아웃<a hidden class=anchor aria-hidden=true href=#c-기준-스레드-레이아웃>#</a></h3><p>동기화 문제가 발생하는 연산은 write이다. 행렬 C의 한 원소처럼 동일한 메모리에 여러 스레드가 동시에 write를 하지 않도록 하는 방법 중 하나는 write 영역을 분할하여 스레드에게 분배하는 것이다. 행렬 곱셈 연산은 C의 각 원소를 스레드에 분배함으로써 실현할 수 있다.</p><p>C(row, col)을 담당하는 스레드는 행렬 A의 row 행과 행렬 B의 col 열의 원소들을 순차적으로 읽으며, C의 값을 계산한다. C(row, col + n)을 담당하는 스레드도 행렬 A의 row 행에 동시 접근 가능하지만, read 연산은 동기화 문제가 발생하지 않는다. 따라서 스레드 수가 행렬 C의 원소 수와 동일하다. 대규모 행렬의 경우, GPU에서 충분한 병렬 처리 연산을 수행할 수 있다.</p><p>행렬은 2차원 데이터로, 레이아웃 또한 2차원으로 잡는 것이 일반적이다. 따라서 각 차원의 길이를 행렬 C의 행과 열의 길이와 같게 하는 것이 스레드 인덱싱에서 직관적이다.</p><h2 id=스레드-인덱싱>스레드 인덱싱<a hidden class=anchor aria-hidden=true href=#스레드-인덱싱>#</a></h2><p>스레드 레이아웃을 결정하면, 커널에서 각 스레드가 접근할 데이터 위치를 결정하는 스레드 인덱싱 작업을 해야 한다. 결과 행렬 C를 기준으로 스레드 레이아웃을 설정함에 따라, 각 스레드가 접근하는 데이터는 아래와 같다.</p><ul><li>행렬 C 중 자신이 담당하는 원소</li><li>C의 원소를 계산하기 위한 행렬 A의 한 행과 B의 한 열</li></ul><h3 id=c의-크기가-블록-최대-크기1024보다-작은-경우>C의 크기가 블록 최대 크기(1,024)보다 작은 경우<a hidden class=anchor aria-hidden=true href=#c의-크기가-블록-최대-크기1024보다-작은-경우>#</a></h3><p>이 설명은 블록 내 최대 스레드 수인 1,024개 인 경우, 블록 1개를 사용하는 행렬 연산이다. C가 2차원 스레드 레이아웃을 사용하기 때문에, 각 스레드의 C(row, col) 인덱스는 스레드 ID를 사용하여 지정할 수 있다.</p><pre tabindex=0><code>row = threadIdx.x
col = threadIdx.y
</code></pre><p>메모리상의 C(row, col) 위치는 row, col 값과 C의 행 길이 n을 이용하여 계산한다. 또한 이처럼 단일 블록의 행렬 연산에서는 행 길이는 스레드 블록의 y-차원 길이와 같다.</p><pre tabindex=0><code>index = row * n + col
index = row * blockDim.y + col
</code></pre><p>C(row, col) 값 연산은 A의 row 번째 행 원소와 B col 번째 원소들을 차례대로 방문해야 한다. offset = 0, 1, &mldr; k일때 A(row, offset)과 B(offset, col)에 접근한다. 이를 각 행렬의 목표 원소라 할 때, 행렬에서 목표 원소에 접근하기 위한 인덱싱은 다음과 같다.</p><pre tabindex=0><code>A(row, offset) = row * k + offset
B(offset, col) = col + (offset * n)
</code></pre><p>행렬 A는 행 길이가 k이며, rok * k는 row 행의 첫 번째 원소를 가리킨다. 접근하고자 하는 원소는 row 행의 offset 원소이므로, row * k + offset이다. 반면에 행렬 B는 열이 고정되며 행을 따라 원소를 읽는다. 행렬 B의 행 길이는 n이며, offset 원소 행의 첫 번째 원소는 n * offset이다. 해당 행의 col 번째 원소를 읽으면 된다. 즉 n * offet + col로서 메모리 상 위치를 계산할 수 있다.</p><h3 id=곱셈-커널>곱셈 커널<a hidden class=anchor aria-hidden=true href=#곱셈-커널>#</a></h3><p>아래 코드는 스레드 수가 1024개 이하인 경우에서의 행렬 곱셈 커널이다.
호스트 코드에서 첫 번째, 두 번째 반복문이 병렬화되어 스레드에 분배됨에 따라, 사실상 offset 레벨의 반복문만 남아있다. 스레드 레이아웃에 의해 C의 인덱스를 지정하여, 디바이스 메모리 영역의 실제 메모리에 접근한다.</p><pre tabindex=0><code>// 단일 블록에서의 CUDA 기반 행렬 곱셉 커널
__global__ void matMul_krenl(int* A, int* B, int* C, int m, int n, int k) {
    int row = threadIdx.x;
    int col = threadIdx.y;
    int index = row * n + col;

    C[index] = 0
    for (int offset = 0; offset &lt; k; offset++) {
        C[index] += A[row*k + offset] * B[col + offset*n];
    }
}
</code></pre><h2 id=c의-크기가-블록-최대-크기1024보다-큰-경우>C의 크기가 블록 최대 크기(1,024)보다 큰 경우<a hidden class=anchor aria-hidden=true href=#c의-크기가-블록-최대-크기1024보다-큰-경우>#</a></h2><p>이 경우에는 여러 개의 블록을 사용해야 한다. 다만 스레드 레이아웃이 행렬 C와 같은 크기를 가지는 점은 동일하다. 따라서 스레드의 x-차원과 y-차원을 이용해서 행렬 C의 각 원소를 지정할 수 있다. 단일 블록에서는 블록 내 스레드 번호만 사용하면 되었지만, 여러 개의 블록은 스레드의 글로벌 인덱스를 사용해야 한다. 즉, (2차원 그리드, 1차원 블록)이 아닌, (2차원 그리드, 2차원 블록)이다.</p><p>이전에서 사용한 글로벌 인덱스를 구하는 방법을 활용할 수 있다. 목표 원소가 속한 블록의 첫 번째 원소까지 이동한 후, 블록 내부에서 자신의 인덱스까지 이동하면 된다.</p><pre tabindex=0><code>row = (blockDim.x * blockIdx.y) + threadIdx.x;
col = (blockDim.y * blockIdx.y) + threadIdx.y;
</code></pre><h3 id=곱셈-커널-1>곱셈 커널<a hidden class=anchor aria-hidden=true href=#곱셈-커널-1>#</a></h3><p>입력 행렬 A, B의 인덱싱은 C(row, col)을 기준으로 계산한다. 커널 역시 row, col을 계산하는 부분만 수정하면 된다.</p><pre tabindex=0><code>// 1024개 스레드보다 큰 여러 블록에서의 행렬 곱셉 커널
__global__ void matMul_kernel (int* A, int* B, int* C, int m, int n, int k) {
    int row = (blockDim.x * blockIdx.x) + threadIdx.x;
    int col = (blockDim.y * blockIdx.y) + threadIdx.y;
    int index = row * n + col;

    C[index] = 0;
    for (int offset = 0; offset &lt; k; offset++) {
        C[index] += A[row * k + offset] * B[col + offset * n];
    }
}
</code></pre><h3 id=구현-및-성능-평가>구현 및 성능 평가<a hidden class=anchor aria-hidden=true href=#구현-및-성능-평가>#</a></h3><p>전체 프로그램 코드</p><pre tabindex=0><code>#define BLOCK_SIZE 16

int main(int argc, char* argv[]) {
    int m, n, k;
    m = atoi(argv[1]); n = atoi(argv[2]); k = atoi(argv[3]);

    int sizeA = m * k;
    int sizeB = k * n;
    int sizeC = m * n;

    int* dA, * dB, * dC;

    // 1. dA, dB, dC를 위한 디바이스 메모리 할당
    cudaMalloc(&amp;dA, sizeA * sizeof(int));
    cudaMalloc(&amp;dB, sizeB * sizeof(int));
    cudaMalloc(&amp;dC, sizeC * sizeof(int));

    cudaMemset(&amp;dA, sizeA * sizeof(int));
    cudaMemset(&amp;dB, sizeB * sizeof(int));
    cudaMemset(&amp;dC, sizeC * sizeof(int));

    // 2. 행렬을 GPU로 데이터 복사
    cudaMemcpy(dA, A, sizeA * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(dB, B, sizeB * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(dC, C, sizeC * sizeof(int), cudaMemcpyHostToDevice);

    // 3. 스레드 레이아웃 설정
    dim3 gridDim(ceil((float) m  / BLOCK_SIZE), ceil((float) n / BLOCK_SIZE));
    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);
    
    // 4. 커널 호출
    MatMul_kernel &lt;&lt;&lt;gridDim. blockDim&gt;&gt;&gt; (dA, dB, dC, m, n, k);
    
    // 5. 디바이스 메모리에서 호스트 메모리로 데이터 복사
    cudaMemcpy(Cgpu, dC, sizeC * sizeof(int), cudaMemcpyDeviceToHost);

    // 6. 디바이스 메모리 해제
    cudaFree(dA);
    cudaFree(dB);
    cudaFree(dC);

    return 0;
}
</code></pre><p>커널을 실제로 호출하기 위해서는 대상 행렬의 크기에 따라 스레드 레이아웃을 결정해야 한다.
또한 커널 수행 시, 사용할 스레드 블록의 크기를 결정해야 한다.
스레드 블록의 크기를 결정했다면, 그리드 크기를 계산해야 한다. 위 구현에서 사용하는 스레드 레이아웃은 연산 결과인 행렬 C를 기준으로 한다. 따라서 C의 원소 수만큼 스레드를 사용해야 한다.</p><pre tabindex=0><code>// 행이 m이고 열이 n인 행렬 C의 필요한 블록 수
x-차원: m / BLOCK_SIZE
y-차원: n / BLOCK_SIZE
</code></pre><p>여기서 문제는 BLOCK_SIZE로 나누어 떨어지지 않는 경우이다. 그런데 그리드는 반드시 정수여야 한다.
만약 내림 연산을 한다면, 오른쪽 원소들을 담당할 스레드가 없어서 해당 원소들을 연산할 수 없다.
따라서 실수 값을 올림 처리하는 것이 유리하다. 이때 <strong>ceil()</strong> 함수를 통해 누락되는 부분 없이 모든 원소를 연산할 수 있다.</p><p><strong>ceil()</strong> 연산으로 올림 처리하여, 모든 행렬의 원소를 연산할 수 있지만 행렬을 벗어나는 영역까지 연산을 위해 메모리에 접근하려는 문제가 생긴다. 따라서 메모리 할당을 받지 않는 영역을 접근하는 것은 메모리 접근 위반으로 프로그램에 오류를 발생시킬 수 있다.</p><pre tabindex=0><code>__global__ void matMul_kernel (int* A, int* B, int* C, int m, int n, int k) {
    int row = (blockDim.x * blockIdx.x) + threadIdx.x;
    int col = (blockDim.y * blockIdx.y) + threadIdx.y;
    int index = row * n + col;

    // 메모리 접근 위반을 방지하는 예외처리 코드
    if (row &gt;= m || col &gt;= n) {
        return;
    }

    C[index] = 0
    for (int offset = 0; offset &lt; k; offset++) {
        C[index] += A[row * k + offset] * B[col + offset * n];
    }
}
</code></pre><p>스레드가 담당할 원소 (row, col)이 행렬 C의 범위를 벗어나는지 체크한다. 만약 범위를 벗어나면,
연산할 원소가 아니기 때문에 스레드를 종료하면 된다. (이를 return 처리 한다.) 마지막으로 GPU로 연산을 위해서는, 디바이스 메모리 공간을 확보해야 한다. 메모리를 직접 할당하고 초기화하기 위해, 연산이 종료된 데이터의 메모리는 항상 메모리 해제를 하는 습관을 가진다.</p><h3 id=부동소수점-정밀도>부동소수점 정밀도<a hidden class=anchor aria-hidden=true href=#부동소수점-정밀도>#</a></h3><p>CUDA 연산 중, 쉬운 예제는 데이터가 정수인 경우이다. 하지만 딥러닝, 수치해석 같이 다양한 분야에서 부동소수점 연산을 한다. 따라서 float 데이터형을 가지는 경우를 고려해야 한다. float 연산에서는 CPU와 GPU의 동일한 로직 연산이 다른 경우가 발생할 수 있다.</p><p>이는 부동소수점 표현 방식에서 가지는 정밀도 차이로 발생하는 문제이다. 부동소수점은 다음과 같이 이루어진다.</p><ul><li>부호(sign)</li><li>지수부(exponenet)</li><li>가수부(fraction)</li></ul><p>부호는 <strong>+=</strong> 여부를 결정한다. 지수부는 숫자 표현의 범위를 결정한다. 그리고 가수부는 숫자 표현의 정밀도를 결정한다. 예를 들어서 딥러닝에서 쓰이는 부동소수점 표현 중 같은 16비트 데이터 형이여도, bfloat16, float16은 서로 표현이 다르다.</p><pre tabindex=0><code>bfloat16: 지수부가 8, 가수부가 7이다.
float16:  지수부가 5, 가수부가 10이다.

bf16은 fp16에 비해 수의 범위가 넓지만, 표현 정밀도가 떨어져 0에 가까운 값은 모두 0으로 표현된다.
</code></pre><p>bf16이 fp16과 다른 특징 때문에 딥러닝 구조와 연산에 따라서 bf16이 더 유리할 수 있다.
예를 들어서 LLM같이 매우 디코더 레이어가 많은 연산이라면,</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on x" href="https://x.com/intent/tweet/?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f&amp;title=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88&amp;summary=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f&title=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on whatsapp" href="https://api.whatsapp.com/send?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on telegram" href="https://telegram.me/share/url?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%287%29%20-%20CUDA%20%ea%b8%b0%eb%b0%98%20%ed%96%89%eb%a0%ac%20%ea%b3%b1%ec%85%88&u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftechnical%2f2024-07-25%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>