<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Pruning vs Quantization: Which is Better? | Oppenheimer's BLOG</title><meta name=keywords content><meta name=description content="Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다."><meta name=author content="Oppenheimer"><link rel=canonical href=https://russellgeum.github.io/posts/2023-07-23/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.1dae343854ebf45ad763d2856aed78754da99c0d0bb698888f0ac7faaaf5a30f.css integrity="sha256-Ha40OFTr9FrXY9KFau14dU2pnA0LtpiIjwrH+qr1ow8=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Pruning vs Quantization: Which is Better?"><meta property="og:description" content="Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/2023-07-23/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-23T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-23T00:00:00+00:00"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Pruning vs Quantization: Which is Better?"><meta name=twitter:description content="Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Pruning vs Quantization: Which is Better?","item":"https://russellgeum.github.io/posts/2023-07-23/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Pruning vs Quantization: Which is Better?","name":"[논문] Pruning vs Quantization: Which is Better?","description":"Paper Link Andrey Kuzmin et al (Qualcomm AI Research)\nIntroduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.\nMotiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다.","keywords":[],"articleBody":"Paper Link Andrey Kuzmin et al (Qualcomm AI Research)\nIntroduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.\nMotiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다. 이를 기준으로 하여, 50% 프루닝은 INT8 양자화와 비교한다. 그리고 75% 프루닝은 INT4 양자화와 비교한다. 또한 프루닝 기법은 magnitude pruning을 사용하고, 양자화는 symmetric uniform quantization을 사용한다. 전자를 선택한 이유는 magnitude pruning + fine tuning을 능가하는 일반적인 프루닝 방법론은 없다. 또한 후자의 이유는 가장 광범위하게 사용되는 양자화 기법이기 때문이다. 또한 두 기법의 차이를 계량하기 위해 SNR을 사용한다. 이 메트릭은 MSE의 로그 스케일과 같다.\nContribution Standard Normal Distribiution 신경망의 많은 가중치들은 러프하게 가우시안 분포를 따른다. 이 모습은 우리가 두 비교를 훨씬 용이하게 한다.\n(Gaussian-like한 분포와 두 비교가 용이한 것과 어떤 상관일까?)\n양자화 에러는 양자화된 노드들 사이에서 진동한다. 오차가 작으면 SNR 수식에 의해 값이 커진다. 프루닝 에러는 많은 가중치들을 0으로 만들기에 오차가 크다. 오차가 크면, SNR 수식에 의해 값이 작아진다. 정리하자면 양자화 에러는 오차가 작으면 SNR 값이 커진다.\nDistribution with Heavy Tails Outlier에 대한 고민도 필요하다. 양자화는 outlier에 민감하지만, 프루닝은 0 근처의 값을 제거하고 outlier를 보존하기 때문에 영향을 크게 받지 않는다.\nOutlier를 고려한 분포를 모델링하기 위하여 Student-t 분포를 사용한다. 이 분포는 끝 값에서 많은 비중을 두기 때문에 Outlier-aware한 weight 모델링에 매우 적합하다. Kurtosis를 통해, 분포의 꼬리가 얼마나 두꺼운지를 계량한다. 의외로 많은 outlier를 포함해도, 적당한 압축률에서 높은 SNR 수치를 보여준다. 이는 양자화의 오차가 꽤 작다는 것을 의미한다. 그러나 뻔하지만 프루닝은 매우 높은 클리핑 기준과 압축률에서 더 높은 성능을 보여주었다. 이 내용에서 말하는 의미는 다음과 같다. 생각보다 양자화는 outlier에 robust하다. 그리고 높은 클리핑, 높은 압축률에서 프루닝은 성능이 더 좋다.\nPost-Training Quantization and Pruning 지금까지 발전한 방법들은 제각기 좋은 결과를 내는 경험적인 알고리즘에 의존한다. 그래서 양자화와 프루닝의 동등한 비교를 아렵게 만든다. 알고리즘 최적성을 위해, 의존성을 줄이는 방법이 필요하다. 저자는 양자화 에러의 타이트한 lower bound를 제안하고, 프루닝을 위해 적절한 차원에서의 lower bound를 정의하여 정확하게 문제를 푸는 방법을 제안한다. 이 두 과정을 통하여 알고리즘 의존성 없이 같은 비교를 가능하는 조건을 만들 수 있다.\n(Q) Lower bound로 양자화과 프루닝을 같은 선상에서 비교 가능하다는 근거는 무엇일까? Experiments Resnet, ViT를 포함하여 9가지 모델에서 lower bound를 고려한 양자화, 프루닝 실험을 하였다.\n적당한 압축에서는 양자화가 프루닝보다 좋은 성능을 보인다. 극한의 압축에서는 둘의 방법 모두 서로 비교 가능한 수준으로 비슷해진다. Conclusion Uniform quantization에 대해서만 실험, 다른 포맷의 양자화 방법은 고려 X 프루닝 또한 magnitude based pruning에 대해서만 검증, 다른 포맷의 프루닝 방법은 고려 X 다양한 하드웨어 deployment에 대한 고려 X ","wordCount":"441","inLanguage":"en","datePublished":"2023-07-23T00:00:00Z","dateModified":"2023-07-23T00:00:00Z","author":{"@type":"Person","name":"Oppenheimer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/2023-07-23/"},"publisher":{"@type":"Organization","name":"Oppenheimer's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Pruning vs Quantization: Which is Better?</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#paper-linkhttpsarxivorgabs230702973><a href=https://arxiv.org/abs/2307.02973>Paper Link</a></a></li><li><a href=#introduction>Introduction</a></li><li><a href=#motiviation>Motiviation</a></li><li><a href=#contribution>Contribution</a><ul><li><a href=#standard-normal-distribiution>Standard Normal Distribiution</a></li><li><a href=#distribution-with-heavy-tails>Distribution with Heavy Tails</a></li><li><a href=#post-training-quantization-and-pruning>Post-Training Quantization and Pruning</a></li></ul></li><li><a href=#experiments>Experiments</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=paper-linkhttpsarxivorgabs230702973><a href=https://arxiv.org/abs/2307.02973>Paper Link</a><a hidden class=anchor aria-hidden=true href=#paper-linkhttpsarxivorgabs230702973>#</a></h2><p>Andrey Kuzmin et al (Qualcomm AI Research)</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.</p><h2 id=motiviation>Motiviation<a hidden class=anchor aria-hidden=true href=#motiviation>#</a></h2><ol><li>양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다.</li><li>그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다.</li><li>본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다.</li></ol><p>이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다. 이를 기준으로 하여, 50% 프루닝은 INT8 양자화와 비교한다. 그리고 75% 프루닝은 INT4 양자화와 비교한다. 또한 프루닝 기법은 magnitude pruning을 사용하고, 양자화는 symmetric uniform quantization을 사용한다. 전자를 선택한 이유는 magnitude pruning + fine tuning을 능가하는 일반적인 프루닝 방법론은 없다. 또한 후자의 이유는 가장 광범위하게 사용되는 양자화 기법이기 때문이다. 또한 두 기법의 차이를 계량하기 위해 SNR을 사용한다. 이 메트릭은 MSE의 로그 스케일과 같다.</p><h2 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h2><h3 id=standard-normal-distribiution>Standard Normal Distribiution<a hidden class=anchor aria-hidden=true href=#standard-normal-distribiution>#</a></h3><p>신경망의 많은 가중치들은 러프하게 가우시안 분포를 따른다. 이 모습은 우리가 두 비교를 훨씬 용이하게 한다.<br>(Gaussian-like한 분포와 두 비교가 용이한 것과 어떤 상관일까?)</p><ol><li>양자화 에러는 양자화된 노드들 사이에서 진동한다. 오차가 작으면 SNR 수식에 의해 값이 커진다.</li><li>프루닝 에러는 많은 가중치들을 0으로 만들기에 오차가 크다. 오차가 크면, SNR 수식에 의해 값이 작아진다.</li></ol><p>정리하자면 양자화 에러는 오차가 작으면 SNR 값이 커진다.</p><h3 id=distribution-with-heavy-tails>Distribution with Heavy Tails<a hidden class=anchor aria-hidden=true href=#distribution-with-heavy-tails>#</a></h3><p>Outlier에 대한 고민도 필요하다. 양자화는 outlier에 민감하지만, 프루닝은 0 근처의 값을 제거하고 outlier를 보존하기 때문에 영향을 크게 받지 않는다.</p><ol><li>Outlier를 고려한 분포를 모델링하기 위하여 Student-t 분포를 사용한다. 이 분포는 끝 값에서 많은 비중을 두기 때문에 Outlier-aware한 weight 모델링에 매우 적합하다.</li><li>Kurtosis를 통해, 분포의 꼬리가 얼마나 두꺼운지를 계량한다.</li></ol><p>의외로 많은 outlier를 포함해도, 적당한 압축률에서 높은 SNR 수치를 보여준다. 이는 양자화의 오차가 꽤 작다는 것을 의미한다. 그러나 뻔하지만 프루닝은 매우 높은 클리핑 기준과 압축률에서 더 높은 성능을 보여주었다. 이 내용에서 말하는 의미는 다음과 같다. 생각보다 양자화는 outlier에 robust하다. 그리고 높은 클리핑, 높은 압축률에서 프루닝은 성능이 더 좋다.</p><h3 id=post-training-quantization-and-pruning>Post-Training Quantization and Pruning<a hidden class=anchor aria-hidden=true href=#post-training-quantization-and-pruning>#</a></h3><p>지금까지 발전한 방법들은 제각기 좋은 결과를 내는 경험적인 알고리즘에 의존한다. 그래서 양자화와 프루닝의 동등한 비교를 아렵게 만든다. 알고리즘 최적성을 위해, 의존성을 줄이는 방법이 필요하다. 저자는 양자화 에러의 타이트한 lower bound를 제안하고, 프루닝을 위해 적절한 차원에서의 lower bound를 정의하여 정확하게 문제를 푸는 방법을 제안한다. 이 두 과정을 통하여 알고리즘 의존성 없이 같은 비교를 가능하는 조건을 만들 수 있다.</p><ul><li>(Q) Lower bound로 양자화과 프루닝을 같은 선상에서 비교 가능하다는 근거는 무엇일까?</li></ul><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>Resnet, ViT를 포함하여 9가지 모델에서 lower bound를 고려한 양자화, 프루닝 실험을 하였다.</p><ol><li>적당한 압축에서는 양자화가 프루닝보다 좋은 성능을 보인다.</li><li>극한의 압축에서는 둘의 방법 모두 서로 비교 가능한 수준으로 비슷해진다.</li></ol><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ol><li>Uniform quantization에 대해서만 실험, 다른 포맷의 양자화 방법은 고려 X</li><li>프루닝 또한 magnitude based pruning에 대해서만 검증, 다른 포맷의 프루닝 방법은 고려 X</li><li>다양한 하드웨어 deployment에 대한 고려 X</li></ol></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Pruning vs Quantization: Which is Better? on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Pruning%20vs%20Quantization%3a%20Which%20is%20Better%3f&u=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-07-23%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>