<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Representation Learning with Convtrastive Predictive Coding | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다."><meta name=author content="Oppenheimer"><link rel=canonical href=https://russellgeum.github.io/posts/review/2021-03-07/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/review/2021-03-07/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Representation Learning with Convtrastive Predictive Coding"><meta property="og:description" content="Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/review/2021-03-07/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-03-07T00:00:00+00:00"><meta property="article:modified_time" content="2021-03-07T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Representation Learning with Convtrastive Predictive Coding"><meta name=twitter:description content="Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Representation Learning with Convtrastive Predictive Coding","item":"https://russellgeum.github.io/posts/review/2021-03-07/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Representation Learning with Convtrastive Predictive Coding","name":"[논문] Representation Learning with Convtrastive Predictive Coding","description":"Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다.","keywords":[],"articleBody":"Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다. Related Works 셍략\nContribution Original signal X와 C에 대해 Mutual Information\n$$ I(x;c)=\\sum_{x,c}p(x, c)log{p(x,c) \\over p(x)p(c)} = \\sum_{x,c}p(x, c)log{p(x|c)p(c) \\over p(x)p(c)} = \\sum_{x,c}p(x, c)log{p(x|c) \\over p(x)} $$\n상호 정보는 두 확률 변수 간의 정보량 평균을 나타낸다. 엔트로피에서 정보량에 들어가는 확률은 c→x에 대한 조건부 확률이고 p(x, c)는 joint probability이다. 베이지안 규칙에 따라 이 식을 위와 같이 쓸 수 있다. 마지막 식의 정보량에 해당하는 p(x|c)/p(x)가 density ratio인데, 상호 정보는 이 density ratio에 비례하는 관계를 가진다.\nContrastive Predictive Coding\nSignal의 각 time segmenet마다 해당하는 신호들을 encoder에 넣어서 latent vector z를 출력한다. 그리고 t 이하의 모든 time segment의 latent vector들을 모아서 autoregressive 모델에 입력한다. 이것이 context latent representation이다. 이전 섹션에서 p(x|c) 형태 모델은 x_t+k를 직접적으로 예측하지 못한다고 주장하였다. 대신에 mutual information의 density ratio를 모델링한다.\n$$ f_k(x_{t+k},c_t) \\propto {p(x_{t+k}|c_t) \\over p(x_{t+k})} \\rightarrow f_k(x_{t+k},c_t)=exp(z^T_{t+k}W_kc_t) $$\n왼쪽 식은 어떤 함수 f가 density ratio에 비례한다고 가정한다. 그 식은 z와 z의 dim에 맞게 linear transformation하는 행렬 W와 context represenation c의 내적으로 이루전 bilinear form이다. 내적은 similarity를 측정하는 measure이다. 이러한 Wc는 이전 컨텍스트로부터 예측한 predictve z이고, z^T는 현재 time signal을 encoding한 latent vector이다. 만약 예측과 인코딩한 정보가 완벽하게 일치하면 density ratio는 크고 당연히 뻔한 정보이므로 상호 간 정보량은 크다. 반면에 서로 전혀 관련이 없는 정보라면 내적이 orthogonal하고 desinty ratio가 1이고 예측을 못해 놀라움이 크기에 정보량이 log 0, 즉 정보량잉 매우 크다. InforNCE Loss and Mutual Information Estimation\n$$ L_N=-\\mathbb{E} {f_k(x_{t+k},c_t) \\over \\sum f_k(x_{t+k},c_t)} $$\n따라서 위의 모델링 함수 f를 mutual information에 넣으면 다음과 같이 되고 NCE 형태로 만들어서 이 Loss를 학습하도록 한다.\nExperiemnets 생략\nConclusion 그러면 얼마나 negative sample pair가 필요할까? sample N에서 positive sample은 1이고 negative sample은 N-1이다. 이때 같은 positive pair면 exp 값이 매우 커져서 정보량이 뻔하고, negative pair면 exp값이 작아서 놀람의 척도인 정보량이 매우 큰데, 얼마나 negative sample이 있어야 정보량을 잘 측정할수 있을까? 사실 N-1의 수는 상호 정보량의 하한에 중요한 결정을 한다.\n","wordCount":"378","inLanguage":"en","datePublished":"2021-03-07T00:00:00Z","dateModified":"2021-03-07T00:00:00Z","author":{"@type":"Person","name":"Oppenheimer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/review/2021-03-07/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="Obiwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>Obiwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Representation Learning with Convtrastive Predictive Coding</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#related-works>Related Works</a></li><li><a href=#contribution>Contribution</a></li><li><a href=#experiemnets>Experiemnets</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><ul><li>Contrastive Learning은 latent space으로부터 downstream task에
유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러
태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면
더 좋다.</li><li>이 논문의 중요한 직관은 signal의 서로 다른 부분
사이에서 공유되는 정보를 인코딩하여 representation learning을 하는
것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다.
그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을
생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지
이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이
아니다.</li></ul><h2 id=related-works>Related Works<a hidden class=anchor aria-hidden=true href=#related-works>#</a></h2><p>셍략</p><h2 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h2><ul><li><p>Original signal X와 C에 대해 Mutual Information</p><p>$$
I(x;c)=\sum_{x,c}p(x, c)log{p(x,c) \over p(x)p(c)} = \sum_{x,c}p(x,
c)log{p(x|c)p(c) \over p(x)p(c)} = \sum_{x,c}p(x, c)log{p(x|c) \over
p(x)}
$$</p><p>상호 정보는 두 확률 변수 간의 정보량 평균을 나타낸다. 엔트로피에서
정보량에 들어가는 확률은 c→x에 대한 조건부 확률이고 p(x, c)는 joint
probability이다. 베이지안 규칙에 따라 이 식을 위와 같이 쓸 수 있다.
마지막 식의 정보량에 해당하는 p(x|c)/p(x)가 density ratio인데, 상호
정보는 이 density ratio에 비례하는 관계를 가진다.</p></li><li><p>Contrastive Predictive Coding</p><p>Signal의 각 time segmenet마다 해당하는 신호들을 encoder에 넣어서
latent vector z를 출력한다. 그리고 t 이하의 모든 time segment의 latent
vector들을 모아서 autoregressive 모델에 입력한다. 이것이 context latent
representation이다. 이전 섹션에서 p(x|c) 형태 모델은 x_t+k를 직접적으로
예측하지 못한다고 주장하였다. 대신에 mutual information의 density
ratio를 모델링한다.</p><p>$$
f_k(x_{t+k},c_t) \propto {p(x_{t+k}|c_t) \over
p(x_{t+k})} \rightarrow f_k(x_{t+k},c_t)=exp(z^T_{t+k}W_kc_t)
$$</p><ul><li>왼쪽 식은 어떤 함수 f가 density ratio에 비례한다고 가정한다.</li><li>그 식은 z와 z의 dim에 맞게 linear transformation하는 행렬 W와
context represenation c의 내적으로 이루전 bilinear form이다.</li><li>내적은 similarity를 측정하는 measure이다. 이러한 Wc는 이전
컨텍스트로부터 예측한 predictve z이고, z^T는 현재 time signal을
encoding한 latent vector이다.</li><li>만약 예측과 인코딩한 정보가 완벽하게 일치하면 density ratio는 크고
당연히 뻔한 정보이므로 상호 간 정보량은 크다. 반면에 서로 전혀 관련이
없는 정보라면 내적이 orthogonal하고 desinty ratio가 1이고 예측을 못해
놀라움이 크기에 정보량이 log 0, 즉 정보량잉 매우 크다.</li></ul></li><li><p>InforNCE Loss and Mutual Information Estimation</p><p>$$
L_N=-\mathbb{E} {f_k(x_{t+k},c_t) \over \sum f_k(x_{t+k},c_t)}
$$</p><p>따라서 위의 모델링 함수 f를 mutual information에 넣으면 다음과 같이
되고 NCE 형태로 만들어서 이 Loss를 학습하도록 한다.</p></li></ul><h2 id=experiemnets>Experiemnets<a hidden class=anchor aria-hidden=true href=#experiemnets>#</a></h2><p>생략</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>그러면 얼마나 negative sample pair가 필요할까? sample N에서
positive sample은 1이고 negative sample은 N-1이다. 이때 같은 positive
pair면 exp 값이 매우 커져서 정보량이 뻔하고, negative pair면 exp값이
작아서 놀람의 척도인 정보량이 매우 큰데, 얼마나 negative sample이 있어야
정보량을 잘 측정할수 있을까? 사실 N-1의 수는 상호 정보량의 하한에 중요한
결정을 한다.</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Representation Learning with Convtrastive Predictive Coding on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Representation%20Learning%20with%20Convtrastive%20Predictive%20Coding&u=https%3a%2f%2frussellgeum.github.io%2fposts%2freview%2f2021-03-07%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>