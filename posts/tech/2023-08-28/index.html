<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Survey: Large Language Models | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models"><meta name=author content="Oppenheimer"><link rel=canonical href=https://russellgeum.github.io/posts/tech/2023-08-28/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/tech/2023-08-28/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Survey: Large Language Models"><meta property="og:description" content="LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models"><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/tech/2023-08-28/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-28T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Survey: Large Language Models"><meta name=twitter:description content="LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Survey: Large Language Models","item":"https://russellgeum.github.io/posts/tech/2023-08-28/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Survey: Large Language Models","name":"[논문] Survey: Large Language Models","description":"LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.\nLarge Language Models Survey on Large Language Models","keywords":[],"articleBody":"LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.\nLarge Language Models Survey on Large Language Models\nChatGPT 공개까지의 Large Language Models의 발전 과정을 담은 서베이 논문이다. 이 논문은 3월에 공개되었는데, 그 이후의 업데이트는 반영되어 있지 않다. 따라서 3월 ~ 8월의 중요한 몇 가지 최신 LLM 연구들을 같이 보면 더 완성도 있는 동향을 파악할 수 있다. [1, 2, 3]\n[1] Llama 2: Open Foundation and Fine-Tuned Chat Models\n[2] Code Llama: Open Foundation Models for Code\n[3] PaLM 2 Technical Report\nLLM with Multimodal Survey on Multimodal Large Language Models\nFoundation 모델의 중요한 데이터 도메인은 결국 자연어이고, LLM 개념이 성립할 수 있다. 동시에 Visual-Language, Audio-Language 같이 2개 이상의 데이터 입력을 받는 모델 연구도 계속 활발히 진행중이다. 당연히 이 둘은 만날 수 밖에 없고, LLM 기반으로 Multimodal 딥러닝 연구 또한 이루어지는 것은 자연스럽다. 이에 관하여 올해 6월까지의 서베이이다.\nLLM with Autonomous Agents Survey on Large Language Model based Autonomous Agents\nAI 에이전트는 사람과 달리 열린 환경에서 마음껏 의사 결정을 할 수 있지 않다. 제약 조건에서 추론과 의사 결정을 하고 사람을 보조해야 한다. 이 AI 에이전트를 tractable하도록 만드는 베이스라인이 LLM이란 관점으로 최근 8월까지의 동향 논문이다. 최근에는 LLM을 컨트롤러로 활용하여 로봇 제어나 게임 NPC 등 다양하게 적용하는 연구들이 많이 나오고 있다.\nLLM with Instruction Tuning Instruction Tuning for Large Language Models: A Survey\n이 서베이는 LLM의 기능 및 성능을 향상시키는데 중요한 기술인 Instruction Tuning의 최근 8월까지의 동향을 이야기한다. Intruction Tuning은 기본 LLM이 모델이 야기하는 부정적 답변이나 이상한 답변을 사람의 상식 범위로 alignment하는 기술이다. 쉽게 말해 LLM용 Fine-tuning을 위한 데이터셋과 학습 방법론 전체를 아우르는 카테고리이다. 어떻게 이 분야가 변해왔는지 전반적인 동향을 담고 있다.\nLLM with Compression Survey on Model Compression for Large Language Models\nLLM 모델을 Full Precision으로 배포하는 것은 매우 시간/비용이 많이 드는 문제이다. 따라서 추론 최적화/경량화 관점에서 LLM을 lightweight하게 만드는 것은 중요하다. LLM 모델에서만 고유하게 발견되는 몇 특성을 활용한 기술도 있고 그래서 여러 측면에서 LLM 프렌들리한 경량화 기술이 발전하는 중이다. Pruning, Knowledge Distillation, Quantization, Low-Rank Factorization 카테고리로 나누어서 8월까지의 동향을 설명한다. (다만 나의 개인적인 생각으로 LoRA에는 그리 동의하지 않는다.) [1]\n[1] LoRA: Low-Rank Adaptation of Large Language Models\nLLM with Evaluation Survey on Evaluation of Large Language Models\n“LLM은 어떻게 평가되어야 할까? 특정 도메인에서는 어떤 metric이 가장 적절한지 그 기준을 알 수 있을까?” 와 같이 LLM의 객관적 metric에 대한 논의들이 담겨있다. 일단 모든 LLM을 객관적으로 평가하는 metric은 존재하지 않는다. 심지어 상당한 수준의 모델이라면 metric에 따라 순위가 바뀌는 경우도 많다. 그래서 LLM의 성능을 측정하는 여러가지 방법론에 대해서 알고자 한다면 이 7월까지의 동향을 읽어본다.\nLLM with Applications Challenges and Applications of Large Language Models\nLLM 모델을 연구로서 말고, 상품으로서는 어떨까? 그 고민과 앞으로 해결해야할 문제에 대한 7월까지의 동향이다. 다양한 산업 사례에서 LLM이 적용된 결과를 볼 수 있고, 그 한계점 또한 서술한다.\n","wordCount":"494","inLanguage":"en","datePublished":"2023-08-28T00:00:00Z","dateModified":"2023-08-28T00:00:00Z","author":{"@type":"Person","name":"Oppenheimer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/tech/2023-08-28/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Survey: Large Language Models</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#llm-동향>LLM 동향</a><ul><li><a href=#large-language-models>Large Language Models</a></li><li><a href=#llm-with-multimodal>LLM with Multimodal</a></li><li><a href=#llm-with-autonomous-agents>LLM with Autonomous Agents</a></li><li><a href=#llm-with-instruction-tuning>LLM with Instruction Tuning</a></li><li><a href=#llm-with-compression>LLM with Compression</a></li><li><a href=#llm-with-evaluation>LLM with Evaluation</a></li><li><a href=#llm-with-applications>LLM with Applications</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=llm-동향>LLM 동향<a hidden class=anchor aria-hidden=true href=#llm-동향>#</a></h2><p>LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.</p><h3 id=large-language-models>Large Language Models<a hidden class=anchor aria-hidden=true href=#large-language-models>#</a></h3><p><a href=https://arxiv.org/abs/2303.18223>Survey on Large Language Models</a><br>ChatGPT 공개까지의 Large Language Models의 발전 과정을 담은 서베이 논문이다. 이 논문은 3월에 공개되었는데, 그 이후의 업데이트는 반영되어 있지 않다. 따라서 3월 ~ 8월의 중요한 몇 가지 최신 LLM 연구들을 같이 보면 더 완성도 있는 동향을 파악할 수 있다. [1, 2, 3]<br>[1] <a href=https://arxiv.org/abs/2307.09288>Llama 2: Open Foundation and Fine-Tuned Chat Models</a><br>[2] <a href=https://arxiv.org/abs/2308.12950v2>Code Llama: Open Foundation Models for Code</a><br>[3] <a href=https://arxiv.org/abs/2305.10403>PaLM 2 Technical Report</a></p><h3 id=llm-with-multimodal>LLM with Multimodal<a hidden class=anchor aria-hidden=true href=#llm-with-multimodal>#</a></h3><p><a href=https://arxiv.org/abs/2306.13549>Survey on Multimodal Large Language Models</a><br>Foundation 모델의 중요한 데이터 도메인은 결국 자연어이고, LLM 개념이 성립할 수 있다. 동시에 Visual-Language, Audio-Language 같이 2개 이상의 데이터 입력을 받는 모델 연구도 계속 활발히 진행중이다. 당연히 이 둘은 만날 수 밖에 없고, LLM 기반으로 Multimodal 딥러닝 연구 또한 이루어지는 것은 자연스럽다. 이에 관하여 올해 6월까지의 서베이이다.</p><h3 id=llm-with-autonomous-agents>LLM with Autonomous Agents<a hidden class=anchor aria-hidden=true href=#llm-with-autonomous-agents>#</a></h3><p><a href=https://arxiv.org/abs/2308.11432>Survey on Large Language Model based Autonomous Agents</a><br>AI 에이전트는 사람과 달리 열린 환경에서 마음껏 의사 결정을 할 수 있지 않다. 제약 조건에서 추론과 의사 결정을 하고 사람을 보조해야 한다. 이 AI 에이전트를 tractable하도록 만드는 베이스라인이 LLM이란 관점으로 최근 8월까지의 동향 논문이다. 최근에는 LLM을 컨트롤러로 활용하여 로봇 제어나 게임 NPC 등 다양하게 적용하는 연구들이 많이 나오고 있다.</p><h3 id=llm-with-instruction-tuning>LLM with Instruction Tuning<a hidden class=anchor aria-hidden=true href=#llm-with-instruction-tuning>#</a></h3><p><a href=https://arxiv.org/abs/2308.10792v1>Instruction Tuning for Large Language Models: A Survey</a><br>이 서베이는 LLM의 기능 및 성능을 향상시키는데 중요한 기술인 Instruction Tuning의 최근 8월까지의 동향을 이야기한다. Intruction Tuning은 기본 LLM이 모델이 야기하는 부정적 답변이나 이상한 답변을 사람의 상식 범위로 alignment하는 기술이다. 쉽게 말해 LLM용 Fine-tuning을 위한 데이터셋과 학습 방법론 전체를 아우르는 카테고리이다. 어떻게 이 분야가 변해왔는지 전반적인 동향을 담고 있다.</p><h3 id=llm-with-compression>LLM with Compression<a hidden class=anchor aria-hidden=true href=#llm-with-compression>#</a></h3><p><a href=https://arxiv.org/abs/2308.07633>Survey on Model Compression for Large Language Models</a><br>LLM 모델을 Full Precision으로 배포하는 것은 매우 시간/비용이 많이 드는 문제이다. 따라서 추론 최적화/경량화 관점에서 LLM을 lightweight하게 만드는 것은 중요하다. LLM 모델에서만 고유하게 발견되는 몇 특성을 활용한 기술도 있고 그래서 여러 측면에서 LLM 프렌들리한 경량화 기술이 발전하는 중이다. Pruning, Knowledge Distillation, Quantization, Low-Rank Factorization 카테고리로 나누어서 8월까지의 동향을 설명한다. (다만 나의 개인적인 생각으로 LoRA에는 그리 동의하지 않는다.) [1]<br>[1] <a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></p><h3 id=llm-with-evaluation>LLM with Evaluation<a hidden class=anchor aria-hidden=true href=#llm-with-evaluation>#</a></h3><p><a href=https://arxiv.org/abs/2307.03109>Survey on Evaluation of Large Language Models</a><br>&ldquo;LLM은 어떻게 평가되어야 할까? 특정 도메인에서는 어떤 metric이 가장 적절한지 그 기준을 알 수 있을까?&rdquo; 와 같이 LLM의 객관적 metric에 대한 논의들이 담겨있다. 일단 모든 LLM을 객관적으로 평가하는 metric은 존재하지 않는다. 심지어 상당한 수준의 모델이라면 metric에 따라 순위가 바뀌는 경우도 많다. 그래서 LLM의 성능을 측정하는 여러가지 방법론에 대해서 알고자 한다면 이 7월까지의 동향을 읽어본다.</p><h3 id=llm-with-applications>LLM with Applications<a hidden class=anchor aria-hidden=true href=#llm-with-applications>#</a></h3><p><a href=https://arxiv.org/abs/2307.10169>Challenges and Applications of Large Language Models</a><br>LLM 모델을 연구로서 말고, 상품으로서는 어떨까? 그 고민과 앞으로 해결해야할 문제에 대한 7월까지의 동향이다. 다양한 산업 사례에서 LLM이 적용된 결과를 볼 수 있고, 그 한계점 또한 서술한다.</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models&u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2023-08-28%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>