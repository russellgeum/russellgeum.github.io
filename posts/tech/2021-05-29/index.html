<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Self-Supervised Multi-Frame Monocular Scene Flow | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구
Related Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음"><meta name=author content="Obiwan"><link rel=canonical href=https://russellgeum.github.io/posts/tech/2021-05-29/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/tech/2021-05-29/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Self-Supervised Multi-Frame Monocular Scene Flow"><meta property="og:description" content="Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구
Related Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음"><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/tech/2021-05-29/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-29T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-29T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Self-Supervised Multi-Frame Monocular Scene Flow"><meta name=twitter:description content="Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구
Related Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Self-Supervised Multi-Frame Monocular Scene Flow","item":"https://russellgeum.github.io/posts/tech/2021-05-29/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Self-Supervised Multi-Frame Monocular Scene Flow","name":"[논문] Self-Supervised Multi-Frame Monocular Scene Flow","description":"Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구\nRelated Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음","keywords":[],"articleBody":"Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구\nRelated Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음\nContribution 저자는 이전 연구에서 separate decoder보다 single decoder가 성능이더 낫다고 주장. (scene flow와 disparity를 하나의 네트워크 path에서 추정하는) 그러나 다시 재관찰해본 결과, decoder 문제가 아니라 이를 하나로 엮는 context network에서 문제. context 네트워크를 없애서 성능이 올라감을 발견 → splited decoder가 문제없다고 판단. 점진적으로 last layer로부터 디코더 스타팅부터 시작해서, 마지막으로부터 몇 번째 레이어로부터 splitting하는 것이 유리할지 조사\nConvLSTM을 통한 prev frame의 hidden state를 propgation → 리얼 월드 시나리오에 알맞는 세팅\n이때 propagation을 그냥 하지 않고 forward warping을 통해서 하는데, occlusion 핸들링에 더 적절함 그리고 backward warping보다 안정적임 Temporal consistency를 이용하는 방법은 옵티컬 플로우의 정확도를 올리고, 특히 가려지거나 영역 밖으로 벗어난 부분 에 대해서 핸들링이 쉬워짐 Occlusion-aware mask\nSSL 로스에 brightness diff를 더 robust하게 measure할 수 있는 cue를 제공 Reference 영상의 윈도우 (Census Window)와 Target 영상의 각각의 윈도우 내에 존재하는 중심 픽셀과 주변 픽셀의 값의 대소 비교로 1 or 0의 패턴을 가지는 바이너리 마스크를 생성 Reference 영상 윈도우의 마스크와 Target 영상 윈도우의 마스크를 비교해서 Hamming Distance를 계산\n→ Hamming distance는 문자열 중 어떤 문자를 바꾸어야 서로 같아지는지 계산하는 것 그러나 Census Transform 자체는 recon 이미지의 outlier에 대해서 취약함 → 이것은 occlusion 때문으로 추정. 따라서 outliter 부분은 occlusion mask로 element-wise product를 하여 제거해줌 (0이면 occlusion, 1이면 visible). 두 census patch의 계산 함수는 Geman-McClure function을 사용 → 각 픽셀 값이 같으면 0, 다르면 1을 할당하여 교환해야할 비용을 계산 occlusion mask는 이 계산 결과에 1을 빼서 얻음 (이것은 나의 생각)\nGradient detaching strategy\n디스패리티 디코더를 분리해서 설계하는데 학습 과정을 더 안정적으로 만듬 Experiments Scene flow를 추정하는 태스크 이므로 관례대로 해온 KITTI Benchmark Scene flow로 결과를 보임\nMulti-frame을 사용하여 추론 속도를 더욱 빠르게 개선한 실험 테이블이 있음\n멀티프레임을 사용했을때 성능 개선 (O) Occ mask를 사용했을때 성능 개선 (O) 이 둘을 모두 사용했을때 성능이 가장 좋음 (O) Conclusion Hur의 연구는 아직까지 카메라 모션을 명시적으로 넣지는 않음\n→ 이것은 저자도 나중에 할 것이라고 밝힘. 이 연구는 현재 내가 하고 있는 것, 빨리 할 것 Cost Volume + Conv LSTM 모듈을 Recurrent하게 작동하는 Attention으로 할 수 있지 않을까?\n→ 코스트 볼륨이 attention과 같이 correlation을 보는 것과 동일, 차이점이라면 코스트 볼륨은 기준 픽셀의 주변 영역만 보지만 어텐션은 전부다 관측함. 코스트 볼륨을 Conv LSTM에 넣어주는 형태인데, 코릴레이션을 본 결과를 다시 LSTM에 넣어주는 것과 이전에 관측한 어텐션을 다시 현재 어텐션 모듈에 Cross Attention으로 넣어주는 것과 행위가 유사하지 않을까? → Perceiver에서 아이디어를 얻음 ","wordCount":"441","inLanguage":"en","datePublished":"2021-05-29T00:00:00Z","dateModified":"2021-05-29T00:00:00Z","author":{"@type":"Person","name":"Obiwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/tech/2021-05-29/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Self-Supervised Multi-Frame Monocular Scene Flow</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#related-works>Related Works</a></li><li><a href=#contribution>Contribution</a></li><li><a href=#experiments>Experiments</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음 (Hur의 이전 연구). 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구</p><h2 id=related-works>Related Works<a hidden class=anchor aria-hidden=true href=#related-works>#</a></h2><p>이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음. 물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음</p><h2 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h2><p>저자는 이전 연구에서 separate decoder보다 single decoder가 성능이더 낫다고 주장. (scene flow와 disparity를 하나의 네트워크 path에서 추정하는) 그러나 다시 재관찰해본 결과, decoder 문제가 아니라 이를 하나로 엮는 context network에서 문제. context 네트워크를 없애서 성능이 올라감을 발견 → splited decoder가 문제없다고 판단. 점진적으로 last layer로부터 디코더 스타팅부터 시작해서, 마지막으로부터 몇 번째 레이어로부터 splitting하는 것이 유리할지 조사</p><ul><li>ConvLSTM을 통한 prev frame의 hidden state를 propgation → 리얼 월드 시나리오에 알맞는 세팅<br>이때 propagation을 그냥 하지 않고 forward warping을 통해서 하는데, occlusion 핸들링에 더 적절함 그리고 backward
warping보다 안정적임
Temporal consistency를 이용하는 방법은 옵티컬 플로우의 정확도를 올리고, 특히 가려지거나 영역 밖으로 벗어난 부분
에 대해서 핸들링이 쉬워짐</li><li>Occlusion-aware mask<br>SSL 로스에 brightness diff를 더 robust하게 measure할 수 있는 cue를 제공<ol><li>Reference 영상의 윈도우 (Census Window)와 Target 영상의 각각의 윈도우 내에 존재하는 중심 픽셀과 주변 픽셀의 값의 대소 비교로 1 or 0의 패턴을 가지는 바이너리 마스크를 생성</li><li>Reference 영상 윈도우의 마스크와 Target 영상 윈도우의 마스크를 비교해서 Hamming Distance를 계산<br>→ Hamming distance는 문자열 중 어떤 문자를 바꾸어야 서로 같아지는지 계산하는 것</li></ol></li></ul><p>그러나 Census Transform 자체는 recon 이미지의 outlier에 대해서 취약함 → 이것은 occlusion 때문으로 추정. 따라서 outliter 부분은 occlusion mask로 element-wise product를 하여 제거해줌 (0이면 occlusion, 1이면 visible). 두 census patch의 계산 함수는 Geman-McClure function을 사용 → 각 픽셀 값이 같으면 0, 다르면 1을 할당하여 교환해야할 비용을 계산 occlusion mask는 이 계산 결과에 1을 빼서 얻음 (이것은 나의 생각)</p><ul><li>Gradient detaching strategy<br>디스패리티 디코더를 분리해서 설계하는데 학습 과정을 더 안정적으로 만듬</li></ul><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>Scene flow를 추정하는 태스크 이므로 관례대로 해온 KITTI Benchmark Scene flow로 결과를 보임<br>Multi-frame을 사용하여 추론 속도를 더욱 빠르게 개선한 실험 테이블이 있음</p><ul><li>멀티프레임을 사용했을때 성능 개선 (O)</li><li>Occ mask를 사용했을때 성능 개선 (O)</li><li>이 둘을 모두 사용했을때 성능이 가장 좋음 (O)</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ul><li>Hur의 연구는 아직까지 카메라 모션을 명시적으로 넣지는 않음<br>→ 이것은 저자도 나중에 할 것이라고 밝힘. 이 연구는 현재 내가 하고 있는 것, 빨리 할 것</li><li>Cost Volume + Conv LSTM 모듈을 Recurrent하게 작동하는 Attention으로 할 수 있지 않을까?<br>→ 코스트 볼륨이 attention과 같이 correlation을 보는 것과 동일, 차이점이라면 코스트 볼륨은 기준 픽셀의 주변 영역만 보지만 어텐션은 전부다 관측함. 코스트 볼륨을 Conv LSTM에 넣어주는 형태인데, 코릴레이션을 본 결과를 다시 LSTM에 넣어주는 것과 이전에 관측한 어텐션을 다시 현재 어텐션 모듈에 Cross Attention으로 넣어주는 것과 행위가 유사하지 않을까? → Perceiver에서 아이디어를 얻음</li></ul></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Self-Supervised Multi-Frame Monocular Scene Flow on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Self-Supervised%20Multi-Frame%20Monocular%20Scene%20Flow&u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2021-05-29%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>