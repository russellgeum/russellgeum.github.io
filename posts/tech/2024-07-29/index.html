<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Survey: Efficient Large Language Models | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al &ldquo;Efficient Multimodal Large Language Models&rdquo; 서베이에 기반한다.
2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다."><meta name=author content="5biwan"><link rel=canonical href=https://russellgeum.github.io/posts/tech/2024-07-29/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/tech/2024-07-29/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Survey: Efficient Large Language Models"><meta property="og:description" content="Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al &ldquo;Efficient Multimodal Large Language Models&rdquo; 서베이에 기반한다.
2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/tech/2024-07-29/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-29T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Survey: Efficient Large Language Models"><meta name=twitter:description content="Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al &ldquo;Efficient Multimodal Large Language Models&rdquo; 서베이에 기반한다.
2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Survey: Efficient Large Language Models","item":"https://russellgeum.github.io/posts/tech/2024-07-29/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Survey: Efficient Large Language Models","name":"Survey: Efficient Large Language Models","description":"Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al \u0026ldquo;Efficient Multimodal Large Language Models\u0026rdquo; 서베이에 기반한다.\n2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다.","keywords":[],"articleBody":"Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al “Efficient Multimodal Large Language Models” 서베이에 기반한다.\n2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다. 이러한 변화는 LLM의 소형화와 우수한 비전 인코더의 발전과 함께 진행되고 있다.\n대규모 사전 학습은 최근 AI의 가장 중요한 테마 중 하나다. 이러한 과정은 LLM이나 멀티모달 모델에게 범용적인 특성을 부여하여, 새로운 태스크를 잘 수행할 수 있게 한다. 이를 통해 여러 애플리케이션에 응용할 수 있다. 이미 OpenAI의 GPT-4V와 Google의 Gemini 등이 대표적인 상용 모델로 자리잡고 있다. 이 모델들은 모두 멀티모달 기반 LLM이다. 따라서 MLMMs 또한 많은 주목을 받고 있다.\n당연히 MLMMs도 스케일링 법칙을 따른다. 범용 모델의 성능은 데이터, 컴퓨팅 파워, 모델 크기의 세 가지 변수에 의해 결정된다. 그러나 이 법칙은 높은 자원 부담을 초래하여 MLMM의 개발 및 배포를 어렵게 만든다. 예를 들어, MiniGPT-v2의 훈련은 A100 GPU 기준으로 총 800 GPU 시간이 필요하다. 이러한 리소스를 여유롭게 감당할 수 있는 기관은 흔하지 않다.\nMLMMs를 효율적으로 만드는 관점은 아래와 같이 정리할 수 있다.\n아키텍처: MLLM의 프레임워크가 요구하는 컴퓨터 리소스를 줄이는 것에 집중한다. 효율적인 비전 모델: 시각적 인식을 하기 위한 인코더 모델을 효율적으로 만드는 것에 집중한다. 효율적인 언어 모델: 언어 모델은 MLLM에서 가장 큰 비중을 차지한다. 따라서 이를 효율적으로 만드는 것에 집중한다. 학습과 데이터 그리고 벤치마크: MLLM의 효율적인 개발과 검증을 위해 올바른 데이터셋, 벤치마크 기준이 필요하다. Architecture 효율적인 MLMMs는 3가지 구성이 중요하다.\nVisual Encoder Large Language Models Projector 대부분의 발전은 이 3가지 내에서 디테일한 고민이 이루어진다. 예를 들면 고해상도 이미지 처리, 비전 토큰 압축, 모델 경량화 등이다. Vision Encoder Vision-Language Projector Small Language Model Token Compression Efficient Structures Efficient Vision Efficient LLMs Limitation 어떻게 더 많은 이미지 시퀀스를 처리할 것인가?\n아직까지 MLLM은 \u003c이미지, 텍스트\u003e 처리에 중점을 둔다. 실제 세상은 이미지로만 해석되지 않는다. Long-context를 가지는 비디오 도메인과의 결합이 필요하다. 어떻게 더 풍부한 모달리티를 처리할 것인가?\n앞서 말한 것처럼 \u003c이미지, 텍스트\u003e 처리를 벗어나 \u003c이미지, 텍스트, …\u003e 와 같이 더 다양한 모달리티 처리가 필요하다. 더 많은 데이터와 더 우수한 경량 언어 모델\n가장 중요한 것은 데이터이다. 소형 모델의 제한된 표현력을 극한으로 끌어올리는 방법은 품질 좋은 더 많은 데이터를 써야한다. 그리고 어디까지 소형 언어 모델이 발전할 수 있을지와 연관된다. 이는 제한된 환경이 요구하는 까다로운 요구사항을 충족할 수 있다. On-Device에서의 에이전트 애플리케이션\nEfficient MLMM의 가능성은 로컬 장치에서의 에이전트 애플리케이션이다. 도메인 특화된 기능으로 현실 세계와 상호작용할 수 있는 에이전트 구현은 지속적으로 도전해야할 영역이다. ","wordCount":"416","inLanguage":"en","datePublished":"2024-07-29T00:00:00Z","dateModified":"2024-07-29T00:00:00Z","author":{"@type":"Person","name":"5biwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/tech/2024-07-29/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Survey: Efficient Large Language Models</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#architecture>Architecture</a><ul><li><a href=#vision-encoder>Vision Encoder</a></li><li><a href=#vision-language-projector>Vision-Language Projector</a></li><li><a href=#small-language-model>Small Language Model</a></li><li><a href=#token-compression>Token Compression</a></li><li><a href=#efficient-structures>Efficient Structures</a></li></ul></li><li><a href=#efficient-vision>Efficient Vision</a></li><li><a href=#efficient-llms>Efficient LLMs</a></li><li><a href=#limitation>Limitation</a></li></ul></nav></div></details></div><div class=post-content><h1 id=efficient-large-language-models>Efficient Large Language Models<a hidden class=anchor aria-hidden=true href=#efficient-large-language-models>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>본 글은 Yizhang Jin et al &ldquo;Efficient Multimodal Large Language Models&rdquo; 서베이에 기반한다.</p><p>2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다. 이러한 변화는 LLM의 소형화와 우수한 비전 인코더의 발전과 함께 진행되고 있다.</p><p>대규모 사전 학습은 최근 AI의 가장 중요한 테마 중 하나다. 이러한 과정은 LLM이나 멀티모달 모델에게 범용적인 특성을 부여하여, 새로운 태스크를 잘 수행할 수 있게 한다. 이를 통해 여러 애플리케이션에 응용할 수 있다. 이미 OpenAI의 GPT-4V와 Google의 Gemini 등이 대표적인 상용 모델로 자리잡고 있다. 이 모델들은 모두 멀티모달 기반 LLM이다. 따라서 MLMMs 또한 많은 주목을 받고 있다.</p><p>당연히 MLMMs도 스케일링 법칙을 따른다. 범용 모델의 성능은 데이터, 컴퓨팅 파워, 모델 크기의 세 가지 변수에 의해 결정된다. 그러나 이 법칙은 높은 자원 부담을 초래하여 MLMM의 개발 및 배포를 어렵게 만든다. 예를 들어, MiniGPT-v2의 훈련은 A100 GPU 기준으로 총 800 GPU 시간이 필요하다. 이러한 리소스를 여유롭게 감당할 수 있는 기관은 흔하지 않다.</p><p>MLMMs를 효율적으로 만드는 관점은 아래와 같이 정리할 수 있다.</p><ul><li>아키텍처: MLLM의 프레임워크가 요구하는 컴퓨터 리소스를 줄이는 것에 집중한다.</li><li>효율적인 비전 모델: 시각적 인식을 하기 위한 인코더 모델을 효율적으로 만드는 것에 집중한다.</li><li>효율적인 언어 모델: 언어 모델은 MLLM에서 가장 큰 비중을 차지한다. 따라서 이를 효율적으로 만드는 것에 집중한다.</li><li>학습과 데이터 그리고 벤치마크: MLLM의 효율적인 개발과 검증을 위해 올바른 데이터셋, 벤치마크 기준이 필요하다.</li></ul><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>효율적인 MLMMs는 3가지 구성이 중요하다.</p><ol><li>Visual Encoder</li><li>Large Language Models</li><li>Projector
대부분의 발전은 이 3가지 내에서 디테일한 고민이 이루어진다. 예를 들면 고해상도 이미지 처리, 비전 토큰 압축, 모델 경량화 등이다.</li></ol><h3 id=vision-encoder>Vision Encoder<a hidden class=anchor aria-hidden=true href=#vision-encoder>#</a></h3><h3 id=vision-language-projector>Vision-Language Projector<a hidden class=anchor aria-hidden=true href=#vision-language-projector>#</a></h3><h3 id=small-language-model>Small Language Model<a hidden class=anchor aria-hidden=true href=#small-language-model>#</a></h3><h3 id=token-compression>Token Compression<a hidden class=anchor aria-hidden=true href=#token-compression>#</a></h3><h3 id=efficient-structures>Efficient Structures<a hidden class=anchor aria-hidden=true href=#efficient-structures>#</a></h3><h2 id=efficient-vision>Efficient Vision<a hidden class=anchor aria-hidden=true href=#efficient-vision>#</a></h2><h2 id=efficient-llms>Efficient LLMs<a hidden class=anchor aria-hidden=true href=#efficient-llms>#</a></h2><h2 id=limitation>Limitation<a hidden class=anchor aria-hidden=true href=#limitation>#</a></h2><ul><li>어떻게 더 많은 이미지 시퀀스를 처리할 것인가?<br>아직까지 MLLM은 &lt;이미지, 텍스트> 처리에 중점을 둔다. 실제 세상은 이미지로만 해석되지 않는다. Long-context를 가지는 비디오 도메인과의 결합이 필요하다.</li><li>어떻게 더 풍부한 모달리티를 처리할 것인가?<br>앞서 말한 것처럼 &lt;이미지, 텍스트> 처리를 벗어나 &lt;이미지, 텍스트, &mldr;> 와 같이 더 다양한 모달리티 처리가 필요하다.</li><li>더 많은 데이터와 더 우수한 경량 언어 모델<br>가장 중요한 것은 데이터이다. 소형 모델의 제한된 표현력을 극한으로 끌어올리는 방법은 품질 좋은 더 많은 데이터를 써야한다. 그리고 어디까지 소형 언어 모델이 발전할 수 있을지와 연관된다. 이는 제한된 환경이 요구하는 까다로운 요구사항을 충족할 수 있다.</li><li>On-Device에서의 에이전트 애플리케이션<br>Efficient MLMM의 가능성은 로컬 장치에서의 에이전트 애플리케이션이다. 도메인 특화된 기능으로 현실 세계와 상호작용할 수 있는 에이전트 구현은 지속적으로 도전해야할 영역이다.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on x" href="https://x.com/intent/tweet/?text=Survey%3a%20Efficient%20Large%20Language%20Models&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f&amp;title=Survey%3a%20Efficient%20Large%20Language%20Models&amp;summary=Survey%3a%20Efficient%20Large%20Language%20Models&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f&title=Survey%3a%20Efficient%20Large%20Language%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on whatsapp" href="https://api.whatsapp.com/send?text=Survey%3a%20Efficient%20Large%20Language%20Models%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on telegram" href="https://telegram.me/share/url?text=Survey%3a%20Efficient%20Large%20Language%20Models&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Survey: Efficient Large Language Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Survey%3a%20Efficient%20Large%20Language%20Models&u=https%3a%2f%2frussellgeum.github.io%2fposts%2ftech%2f2024-07-29%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>