<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Sparse Binarization for Fast Keyword Spotting | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.
엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.
이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.
SparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.
2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다."><meta name=author content="5biwan"><link rel=canonical href=https://russellgeum.github.io/posts/research/2024-12-18/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/research/2024-12-18/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Sparse Binarization for Fast Keyword Spotting"><meta property="og:description" content="1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.
엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.
이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.
SparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.
2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/research/2024-12-18/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-18T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Sparse Binarization for Fast Keyword Spotting"><meta name=twitter:description content="1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.
엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.
이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.
SparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.
2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Sparse Binarization for Fast Keyword Spotting","item":"https://russellgeum.github.io/posts/research/2024-12-18/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Sparse Binarization for Fast Keyword Spotting","name":"[논문] Sparse Binarization for Fast Keyword Spotting","description":"1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.\n엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.\n이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.\nSparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.\n2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다.","keywords":[],"articleBody":"1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.\n엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.\n이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.\nSparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.\n2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다. 기존 연구는 소형 CNN, RNN, 또는 하이브리드 네트워크를 사용하여 엣지 디바이스에서 최적화된 모델을 설계해왔다. 주요 기법으로는 양자화(Quantization), 프루닝(Pruning), 그리고 **1D 깊이분리 합성곱(Depthwise Separable Convolution)**이 활용되었다. 3. Proposed Method Method Overview Sparse Binarization: 입력 데이터에서 유효하지 않은 특징을 제거하고, 예측에 유용한 정보를 유지하기 위해 이진화된 표현을 학습한다. 모델 구조: SparkNet은 입력 데이터를 이진화하여 선형 분류기로 전달하며, 효율적인 계산을 위해 **1D 시간-채널 분리 합성곱(Time-Channel Separable Convolution)**을 사용한다. SparkNet Architecture 입력 데이터: 멜 주파수 스펙트럼(MFCC)을 기반으로 한 (F \\times T) 크기의 입력 데이터를 사용한다. 구조: 4개의 블록으로 구성된 1D 깊이분리 합성곱 레이어. 배치 정규화와 ReLU 활성화를 포함. 마지막 출력 레이어는 1x1 합성곱으로 구성되며 Tanh 활성화를 사용한다. 출력: 12개의 키워드 범주로 매핑되며, 여기에는 10개의 타겟 단어, “Unknown”, 그리고 “Silence\"가 포함된다. Sparse Binarized Representation Learning 학습 과정: 입력 데이터를 이진화하기 위해 가우시안 기반의 이완된 Bernoulli 분포를 활용한다. 학습 중, 스파스 표현을 강화하기 위해 정규화 손실((L_{sparse}))을 추가. 효과: 입력 데이터의 시공간적 특징을 간결하게 유지하여, 계산량은 줄이면서 높은 정확도를 보장한다. Classification Learning 학습 목표: 이진화된 표현을 평균 풀링한 후, 단일 선형 레이어로 타겟 키워드를 예측. 손실 함수: (L = L_{sparse} + \\lambda L_{ce}), 여기서 (L_{ce})는 크로스 엔트로피 손실. 4. Experiments Experimental Setup 데이터셋: Google Speech Commands 버전 1(V1) 및 2(V2). 각각 30개와 35개의 키워드 범주를 포함하며, 1초 길이의 샘플로 구성. MFCC를 사용하여 32개의 주파수 빈으로 전처리. 평가 지표: Top-1 정확도와 Multiply-Accumulate Operations(MACs). 소음 환경에서의 강건성: 다양한 신호대잡음비(SNR)에서 모델의 성능을 측정. Results 속도와 정확도: SparkNet은 SOTA 모델(BC-ResNet)보다 4배 빠르며, 동일하거나 더 높은 정확도를 달성. SparkNet[C=32]: SC2 데이터셋에서 97.0%의 정확도를 기록하며 BC-ResNet을 초과. 소음 강건성: 다양한 SNR에서 SparkNet이 BC-ResNet 대비 일관되게 높은 정확도를 보임. Ablation Study 모델 구성 요소 검증: 이진화 과정(Lsparse)이 모델 성능에 가장 큰 기여를 함을 확인. 보조 분류기를 추가했을 때, 성능 향상이 없었음을 실험적으로 입증. 5. Conclusion \u0026 Limitation Conclusion SparkNet은 효율성과 정확성을 동시에 달성한 KWS 모델로, 엣지 디바이스에 최적화되었다. 소음 환경에서도 강건성을 가지며, 기존 모델보다 적은 계산량으로 높은 성능을 보인다. Limitation 이 모델은 감독 학습(Supervised Learning)에 기반하며, 자가 지도 학습(Self-Supervised Learning)으로 확장이 필요함. 더욱 소형화된 디바이스를 대상으로 한 추가 최적화 가능성이 존재. Related Works BC-ResNet: Broadcasted Residual Learning 기반의 KWS 모델. MatchboxNet: 1D 시간-채널 분리 합성곱 구조를 사용한 KWS 모델. TinySpeech: 엣지 디바이스에서 경량화를 위해 설계된 Attention 기반 모델. Key References Svirsky et al., “SG-VAD: Stochastic Gates Based Speech Activity Detection” (ICASSP 2023) Kim et al., “Broadcasted Residual Learning for Efficient Keyword Spotting” (Interspeech 2021) Majumdar et al., “MatchboxNet: 1D Time-Channel Separable CNN for Speech Commands Recognition” (2020) ","wordCount":"474","inLanguage":"en","datePublished":"2024-12-18T00:00:00Z","dateModified":"2024-12-18T00:00:00Z","author":{"@type":"Person","name":"5biwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/research/2024-12-18/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Sparse Binarization for Fast Keyword Spotting</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-motivation>1. Motivation</a></li><li><a href=#2-related-work>2. Related Work</a><ul><li><a href=#keyword-spotting-kws>Keyword Spotting (KWS)</a></li></ul></li><li><a href=#3-proposed-method>3. Proposed Method</a><ul><li><a href=#method-overview>Method Overview</a></li><li><a href=#sparknet-architecture>SparkNet Architecture</a></li><li><a href=#sparse-binarized-representation-learning>Sparse Binarized Representation Learning</a></li><li><a href=#classification-learning>Classification Learning</a></li></ul></li><li><a href=#4-experiments>4. Experiments</a><ul><li><a href=#experimental-setup>Experimental Setup</a></li><li><a href=#results>Results</a></li><li><a href=#ablation-study>Ablation Study</a></li></ul></li><li><a href=#5-conclusion--limitation>5. Conclusion & Limitation</a><ul><li><a href=#conclusion>Conclusion</a></li><li><a href=#limitation>Limitation</a></li></ul></li><li><a href=#related-works>Related Works</a></li><li><a href=#key-references>Key References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=1-motivation>1. Motivation<a hidden class=anchor aria-hidden=true href=#1-motivation>#</a></h2><p>음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.<br>엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.<br>이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 <strong>Sparse Binarization</strong>을 기반으로 한 모델 <strong>SparkNet</strong>을 제안한다.<br>SparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.</p><h2 id=2-related-work>2. Related Work<a hidden class=anchor aria-hidden=true href=#2-related-work>#</a></h2><h3 id=keyword-spotting-kws>Keyword Spotting (KWS)<a hidden class=anchor aria-hidden=true href=#keyword-spotting-kws>#</a></h3><ul><li>KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다.</li><li>기존 연구는 소형 CNN, RNN, 또는 하이브리드 네트워크를 사용하여 엣지 디바이스에서 최적화된 모델을 설계해왔다.</li><li>주요 기법으로는 <strong>양자화(Quantization)</strong>, <strong>프루닝(Pruning)</strong>, 그리고 **1D 깊이분리 합성곱(Depthwise Separable Convolution)**이 활용되었다.</li></ul><h2 id=3-proposed-method>3. Proposed Method<a hidden class=anchor aria-hidden=true href=#3-proposed-method>#</a></h2><h3 id=method-overview>Method Overview<a hidden class=anchor aria-hidden=true href=#method-overview>#</a></h3><ul><li><strong>Sparse Binarization</strong>: 입력 데이터에서 유효하지 않은 특징을 제거하고, 예측에 유용한 정보를 유지하기 위해 이진화된 표현을 학습한다.</li><li><strong>모델 구조</strong>: SparkNet은 입력 데이터를 이진화하여 선형 분류기로 전달하며, 효율적인 계산을 위해 **1D 시간-채널 분리 합성곱(Time-Channel Separable Convolution)**을 사용한다.</li></ul><h3 id=sparknet-architecture>SparkNet Architecture<a hidden class=anchor aria-hidden=true href=#sparknet-architecture>#</a></h3><ul><li><strong>입력 데이터</strong>: 멜 주파수 스펙트럼(MFCC)을 기반으로 한 (F \times T) 크기의 입력 데이터를 사용한다.</li><li><strong>구조</strong>:<ul><li>4개의 블록으로 구성된 1D 깊이분리 합성곱 레이어.</li><li>배치 정규화와 ReLU 활성화를 포함.</li><li>마지막 출력 레이어는 1x1 합성곱으로 구성되며 Tanh 활성화를 사용한다.</li></ul></li><li><strong>출력</strong>: 12개의 키워드 범주로 매핑되며, 여기에는 10개의 타겟 단어, &ldquo;Unknown&rdquo;, 그리고 &ldquo;Silence"가 포함된다.</li></ul><h3 id=sparse-binarized-representation-learning>Sparse Binarized Representation Learning<a hidden class=anchor aria-hidden=true href=#sparse-binarized-representation-learning>#</a></h3><ul><li><strong>학습 과정</strong>:<ul><li>입력 데이터를 이진화하기 위해 가우시안 기반의 이완된 Bernoulli 분포를 활용한다.</li><li>학습 중, 스파스 표현을 강화하기 위해 정규화 손실((L_{sparse}))을 추가.</li></ul></li><li><strong>효과</strong>:<ul><li>입력 데이터의 시공간적 특징을 간결하게 유지하여, 계산량은 줄이면서 높은 정확도를 보장한다.</li></ul></li></ul><h3 id=classification-learning>Classification Learning<a hidden class=anchor aria-hidden=true href=#classification-learning>#</a></h3><ul><li><strong>학습 목표</strong>:<ul><li>이진화된 표현을 평균 풀링한 후, 단일 선형 레이어로 타겟 키워드를 예측.</li><li>손실 함수: (L = L_{sparse} + \lambda L_{ce}), 여기서 (L_{ce})는 크로스 엔트로피 손실.</li></ul></li></ul><h2 id=4-experiments>4. Experiments<a hidden class=anchor aria-hidden=true href=#4-experiments>#</a></h2><h3 id=experimental-setup>Experimental Setup<a hidden class=anchor aria-hidden=true href=#experimental-setup>#</a></h3><ul><li><strong>데이터셋</strong>: Google Speech Commands 버전 1(V1) 및 2(V2).<ul><li>각각 30개와 35개의 키워드 범주를 포함하며, 1초 길이의 샘플로 구성.</li><li>MFCC를 사용하여 32개의 주파수 빈으로 전처리.</li></ul></li><li><strong>평가 지표</strong>:<ul><li><strong>Top-1 정확도</strong>와 <strong>Multiply-Accumulate Operations(MACs)</strong>.</li><li><strong>소음 환경에서의 강건성</strong>: 다양한 신호대잡음비(SNR)에서 모델의 성능을 측정.</li></ul></li></ul><h3 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h3><ul><li><strong>속도와 정확도</strong>:<ul><li>SparkNet은 SOTA 모델(BC-ResNet)보다 4배 빠르며, 동일하거나 더 높은 정확도를 달성.</li><li>SparkNet[C=32]: SC2 데이터셋에서 97.0%의 정확도를 기록하며 BC-ResNet을 초과.</li></ul></li><li><strong>소음 강건성</strong>:<ul><li>다양한 SNR에서 SparkNet이 BC-ResNet 대비 일관되게 높은 정확도를 보임.</li></ul></li></ul><h3 id=ablation-study>Ablation Study<a hidden class=anchor aria-hidden=true href=#ablation-study>#</a></h3><ul><li><strong>모델 구성 요소 검증</strong>:<ul><li>이진화 과정(Lsparse)이 모델 성능에 가장 큰 기여를 함을 확인.</li><li>보조 분류기를 추가했을 때, 성능 향상이 없었음을 실험적으로 입증.</li></ul></li></ul><h2 id=5-conclusion--limitation>5. Conclusion & Limitation<a hidden class=anchor aria-hidden=true href=#5-conclusion--limitation>#</a></h2><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><ul><li>SparkNet은 효율성과 정확성을 동시에 달성한 KWS 모델로, 엣지 디바이스에 최적화되었다.</li><li>소음 환경에서도 강건성을 가지며, 기존 모델보다 적은 계산량으로 높은 성능을 보인다.</li></ul><h3 id=limitation>Limitation<a hidden class=anchor aria-hidden=true href=#limitation>#</a></h3><ol><li>이 모델은 감독 학습(Supervised Learning)에 기반하며, 자가 지도 학습(Self-Supervised Learning)으로 확장이 필요함.</li><li>더욱 소형화된 디바이스를 대상으로 한 추가 최적화 가능성이 존재.</li></ol><h2 id=related-works>Related Works<a hidden class=anchor aria-hidden=true href=#related-works>#</a></h2><ol><li><strong>BC-ResNet</strong>: Broadcasted Residual Learning 기반의 KWS 모델.</li><li><strong>MatchboxNet</strong>: 1D 시간-채널 분리 합성곱 구조를 사용한 KWS 모델.</li><li><strong>TinySpeech</strong>: 엣지 디바이스에서 경량화를 위해 설계된 Attention 기반 모델.</li></ol><h2 id=key-references>Key References<a hidden class=anchor aria-hidden=true href=#key-references>#</a></h2><ol><li>Svirsky et al., &ldquo;SG-VAD: Stochastic Gates Based Speech Activity Detection&rdquo; (ICASSP 2023)</li><li>Kim et al., &ldquo;Broadcasted Residual Learning for Efficient Keyword Spotting&rdquo; (Interspeech 2021)</li><li>Majumdar et al., &ldquo;MatchboxNet: 1D Time-Channel Separable CNN for Speech Commands Recognition&rdquo; (2020)</li></ol></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Sparse Binarization for Fast Keyword Spotting on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Sparse%20Binarization%20for%20Fast%20Keyword%20Spotting&u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-12-18%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>