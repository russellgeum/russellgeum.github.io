<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[기술] GPU와 CUDA (8) - 공유 메모리 | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.
L1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다."><meta name=author content="5biwan"><link rel=canonical href=https://russellgeum.github.io/posts/research/2024-08-25/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/research/2024-08-25/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[기술] GPU와 CUDA (8) - 공유 메모리"><meta property="og:description" content="공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.
L1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/research/2024-08-25/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-25T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[기술] GPU와 CUDA (8) - 공유 메모리"><meta name=twitter:description content="공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.
L1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[기술] GPU와 CUDA (8) - 공유 메모리","item":"https://russellgeum.github.io/posts/research/2024-08-25/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[기술] GPU와 CUDA (8) - 공유 메모리","name":"[기술] GPU와 CUDA (8) - 공유 메모리","description":"공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.\nL1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다.","keywords":[],"articleBody":"공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.\nL1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다. 스레드 간 데이터를 공유하기 위해 중요한 메커니즘으로 활용된다. 동기화의 필요성: 여러 스레드가 공유 메모리에서 데이터를 읽고 쓰는 과정에서 데이터의 일관성을 유지하기 위해 동기화가 필요하다. 이를 위해 __syncthreads() 함수가 사용되며, 이 함수는 모든 스레드가 특정 지점에 도달할 때까지 대기하게 한다. L1 캐시 역할 및 특징: L1 캐시는 GPU가 자동으로 관리하는 고속 메모리로, 최근 접근한 데이터를 저장해 이후 접근 시 메모리 접근 시간을 줄인다. 캐시 히트와 미스: 캐시 히트: 필요한 데이터가 캐시에 있을 경우, 신속하게 데이터를 가져올 수 있어 성능이 향상된다. 캐시 미스: 필요한 데이터가 캐시에 없을 경우, 메모리에서 데이터를 가져와야 하므로 레이턴시가 증가하고 성능이 저하된다. 최적화 고려사항: 공간적 및 시간적 지역성을 고려한 데이터 접근 패턴을 설계하면, 캐시 히트율을 높여 성능을 최적화할 수 있다. SIMT 구조와 메모리 접근 패턴 SIMT 구조: GPU는 SIMT (Single Instruction, Multiple Threads) 구조로, 워프(warp) 단위로 스레드들이 동시에 실행된다. 메모리 접근 최적화: 효율적인 접근: 워프 내의 스레드들이 동일한 데이터 블록에 접근할 경우, 메모리 트랜잭션 수가 최소화되어 성능이 향상된다. 비효율적 접근: 스레드들이 여러 데이터 블록에 분산되어 접근하면, 메모리 트랜잭션 수가 증가해 성능이 저하될 수 있다. 사용자 관리 캐시 (User-managed Cache) 역할 및 특징: 개발자가 공유 메모리나 레지스터를 활용해 데이터를 수동으로 관리하는 것을 의미한다. 이는 자동으로 관리되는 L1 캐시와 달리, 알고리즘의 특성에 맞게 데이터를 직접 제어하는 방식이다. 활용 예시: 예를 들어, 행렬 곱셈에서 공유 메모리를 활용하여 데이터를 재사용함으로써 메모리 대역폭을 효율적으로 사용할 수 있다. 이를 통해 커널의 성능을 크게 최적화할 수 있다. 공유 메모리 사용 예제 1: 작은 행렬 하나의 블록만 사용는 행렬 곱셉 커널은 아래와 같다.\n각 스레드의 인덱스는 행렬 C에서 동일 인덱스의 원소를 연산한다.\nint index는 행렬 C의 원소들을 1차원 형태 인덱스로 표현한 변수이다.\n__global__ void matMul_kernel(float* A, float* B, float* C) { int row = threadIdx.x; int col = threadIdx.y; int index = row * blockDim.y + col; float result = 0; for (int k = 0; k \u003c K_SIZE; k++) { result += _A[row * K_SIZE + k] * _B[col + k * COL_SIZE]; } _C[index] = result; } 공유 메모리를 사용자 정의 캐시로 활용하는 핵심은 블록 내 스레드들이 자주 사용하는 데이터들을 공유 메모리에 저장하는 것이다. 이를 통해 전역 메모리의 접근 수를 줄인다. 위 코드에서 행렬 C의 (row, col) 원소를 연산하기 위해서는, 행렬 A의 row와 행렬 B의 col을 한 번씩 읽기가 있다. 그리고 그 결과값을 저장하는 1번의 쓰기가 발생한다.\n그런데 행렬 A의 row는 C(row, 0)에서 C(row, col)까지 연산하는 동안 계속 접근이 필요하다. 따라서 행렬 A의 row는 col번 만큼 접근이 필요하다. 마찬가지로 B의 col은 row번 만큼 접근이 필요하다. 따라서 두 가지 결론이 나온다.\n행렬 A, B의 데이터는 반복해서 접근한다. 행렬 C는 데이터를 1번만 저장하면 된다. 자주 사용하는 데이터를 공유 메모리에 저장하는 것이 중요하다고 하였다. 따라서 행렬 A, B를 공유 메모리에 저장하는 것이 성능에 유리하다. (현재 문제는 공유 메모리 크기보다 작은 행렬이므로 공유 메모리 크기 자체는 고려하지 않는다.)\n공유 메모리를 사용하는 행렬 곱셉 커널은 아래와 같다.\n__global__ void matMul_kernel(float* A, float* B, float* C) { int row = threadIdx.x; int col = threadIdx.y; int index = row * blockDim.y + col; __shared__ float sA[ROW_SIZE][K_SIZE]; __shared__ float sB[K_SIZE][COL_SIZE]; if (threadIdx.x == 0 \u0026\u0026 threadIdx.y == 0) { for (int r = 0; k \u003c ROW_SIZE; k++) { for (int k = 0; k \u003c K_SIZE; k++) { sA[r][k] = _A[r * K_SIZE + k]; } } for (int c = 0; k \u003c ROW_SIZE; k++) { for (int k = 0; k \u003c K_SIZE; k++) { sB[k][c] = _B[c + k * K_SIZE]; } } } __syncthreads(); ... float result = 0; for (int k = 0; k \u003c K_SIZE; k++) { result += _A[row * K_SIZE + k] * _B[col + k * COL_SIZE]; } _C[index] = result; } 코드에서는 블록 내에서 threadIdx.x == 0 \u0026\u0026 threadIdx.y == 0 인 스레드 하나만이 공유 메모리에 데이터를 복사하도록 설정되어 있다. 이 경우, 다른 스레드들은 이 복사가 끝날 때까지 __syncthreads() 에서 대기하게 된다. 이는 비효율적인 방법이다. 복사 작업을 단 하나의 스레드에게만 맡기면, 전체 블록이 이 작업이 끝날 때까지 기다리게 되어 병렬 처리의 장점을 제대로 살리지 못한다.\n더 많은 스레드가 복사 작업에 참여하도록 하여 병렬로 데이터를 복사하게 하면, 복사 시간을 단축할 수 있다. 하지만 모든 스레드가 자신과 관련된 (row, col) 데이터를 복사하게 되면 중복 복사 문제 가 발생할 수 있다. 예를 들어, 행렬 A의 동일한 행(row)을 복사할 때 여러 스레드가 동일한 데이터를 공유 메모리에 중복으로 복사하면 메모리 사용이 비효율적이 된다. 이러한 중복 복사는 메모리 대역폭을 낭비하고, 연산 지연을 초래한다.\n이 문제를 해결하기 위해서는 각 스레드가 서로 다른 부분의 데이터를 복사하도록 작업을 분배해야 한다. 특정 스레드들끼리 복사할 부분을 지정하는 방식으로 중복 복사를 피할 수 있다. 이렇게 하면 모든 스레드가 병렬로 데이터를 복사해 공유 메모리 초기화 시간을 단축할 수 있다. 아래 코드가 그 예시이다.\n__global__ void matMul_kernel(float* A, float* B, float* C) { ... __shared__ float sA[ROW_SIZE][K_SIZE]; __shared__ float sB[K_SIZE][COL_SIZE]; if (col == 0) { // read matrix A for (int k = 0; k \u003c K_SIZE; k++) { sA[row][k] = _A[row * K_SIZE + k]; } } if (row == 0) { // read matrix B for (int k = 0; k \u003c K_SIZE; k++) { sB[k][col] = _B[col + k * COL_SIZE]; } } __syncthreads(); ... } 공유 메모리보다 작은 크기의 데이터를 활용할 때에는 데이터 전체를 공유 메모리에 올려놓아도 문제가 없다. 하지만 공유 메모리보다 큰 데이터를 처리하기 위해서는 또 다른 전략이 필요하다.\n공유 메모리 사용 예제 2: 큰 행렬 작성 예정\n","wordCount":"925","inLanguage":"en","datePublished":"2024-08-25T00:00:00Z","dateModified":"2024-08-25T00:00:00Z","author":{"@type":"Person","name":"5biwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/research/2024-08-25/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[기술] GPU와 CUDA (8) - 공유 메모리</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#스레드-간-공유-메모리와-l1-캐시-활용-방법>스레드 간 공유 메모리와 L1 캐시 활용 방법</a></li><li><a href=#공유-메모리-사용-예제-1-작은-행렬>공유 메모리 사용 예제 1: 작은 행렬</a></li><li><a href=#공유-메모리-사용-예제-2-큰-행렬>공유 메모리 사용 예제 2: 큰 행렬</a></li></ul></nav></div></details></div><div class=post-content><h1 id=공유-메모리>공유 메모리<a hidden class=anchor aria-hidden=true href=#공유-메모리>#</a></h1><p>공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.</p><ul><li>L1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우</li><li>사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어</li><li>사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함</li></ul><h2 id=스레드-간-공유-메모리와-l1-캐시-활용-방법>스레드 간 공유 메모리와 L1 캐시 활용 방법<a hidden class=anchor aria-hidden=true href=#스레드-간-공유-메모리와-l1-캐시-활용-방법>#</a></h2><ol><li>공유 메모리 (Shared Memory)</li></ol><ul><li>역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다. 스레드 간 데이터를 공유하기 위해 중요한 메커니즘으로 활용된다.</li><li>동기화의 필요성: 여러 스레드가 공유 메모리에서 데이터를 읽고 쓰는 과정에서 데이터의 일관성을 유지하기 위해 동기화가 필요하다. 이를 위해 __syncthreads() 함수가 사용되며, 이 함수는 모든 스레드가 특정 지점에 도달할 때까지 대기하게 한다.</li></ul><ol start=2><li>L1 캐시</li></ol><ul><li>역할 및 특징: L1 캐시는 GPU가 자동으로 관리하는 고속 메모리로, 최근 접근한 데이터를 저장해 이후 접근 시 메모리 접근 시간을 줄인다.</li><li>캐시 히트와 미스:<ul><li>캐시 히트: 필요한 데이터가 캐시에 있을 경우, 신속하게 데이터를 가져올 수 있어 성능이 향상된다.</li><li>캐시 미스: 필요한 데이터가 캐시에 없을 경우, 메모리에서 데이터를 가져와야 하므로 레이턴시가 증가하고 성능이 저하된다.</li></ul></li><li>최적화 고려사항: 공간적 및 시간적 지역성을 고려한 데이터 접근 패턴을 설계하면, 캐시 히트율을 높여 성능을 최적화할 수 있다.</li></ul><ol><li>SIMT 구조와 메모리 접근 패턴</li></ol><ul><li>SIMT 구조: GPU는 SIMT (Single Instruction, Multiple Threads) 구조로, 워프(warp) 단위로 스레드들이 동시에 실행된다.</li><li>메모리 접근 최적화:<ul><li>효율적인 접근: 워프 내의 스레드들이 동일한 데이터 블록에 접근할 경우, 메모리 트랜잭션 수가 최소화되어 성능이 향상된다.</li><li>비효율적 접근: 스레드들이 여러 데이터 블록에 분산되어 접근하면, 메모리 트랜잭션 수가 증가해 성능이 저하될 수 있다.</li></ul></li></ul><ol><li>사용자 관리 캐시 (User-managed Cache)</li></ol><ul><li>역할 및 특징: 개발자가 공유 메모리나 레지스터를 활용해 데이터를 수동으로 관리하는 것을 의미한다. 이는 자동으로 관리되는 L1 캐시와 달리, 알고리즘의 특성에 맞게 데이터를 직접 제어하는 방식이다.</li><li>활용 예시: 예를 들어, 행렬 곱셈에서 공유 메모리를 활용하여 데이터를 재사용함으로써 메모리 대역폭을 효율적으로 사용할 수 있다. 이를 통해 커널의 성능을 크게 최적화할 수 있다.</li></ul><h2 id=공유-메모리-사용-예제-1-작은-행렬>공유 메모리 사용 예제 1: 작은 행렬<a hidden class=anchor aria-hidden=true href=#공유-메모리-사용-예제-1-작은-행렬>#</a></h2><p>하나의 블록만 사용는 행렬 곱셉 커널은 아래와 같다.<br>각 스레드의 인덱스는 행렬 C에서 동일 인덱스의 원소를 연산한다.<br>int index는 행렬 C의 원소들을 1차원 형태 인덱스로 표현한 변수이다.</p><pre tabindex=0><code>__global__ void matMul_kernel(float* A, float* B, float* C) {
    int row = threadIdx.x;
    int col = threadIdx.y;
    int index = row * blockDim.y + col;
    
    float result = 0;
    for (int k = 0; k &lt; K_SIZE; k++) {
        result += _A[row * K_SIZE + k] * _B[col + k * COL_SIZE];
    }
    _C[index] = result;
}
</code></pre><p>공유 메모리를 사용자 정의 캐시로 활용하는 핵심은 블록 내 스레드들이 자주 사용하는 데이터들을 공유 메모리에 저장하는 것이다. 이를 통해 전역 메모리의 접근 수를 줄인다. 위 코드에서 행렬 C의 (row, col) 원소를 연산하기 위해서는, 행렬 A의 row와 행렬 B의 col을 한 번씩 읽기가 있다. 그리고 그 결과값을 저장하는 1번의 쓰기가 발생한다.</p><p>그런데 행렬 A의 row는 C(row, 0)에서 C(row, col)까지 연산하는 동안 계속 접근이 필요하다. 따라서 행렬 A의 row는 col번 만큼 접근이 필요하다. 마찬가지로 B의 col은 row번 만큼 접근이 필요하다. 따라서 두 가지 결론이 나온다.</p><ul><li>행렬 A, B의 데이터는 반복해서 접근한다.</li><li>행렬 C는 데이터를 1번만 저장하면 된다.</li></ul><p>자주 사용하는 데이터를 공유 메모리에 저장하는 것이 중요하다고 하였다. 따라서 행렬 A, B를 공유 메모리에 저장하는 것이 성능에 유리하다. (현재 문제는 공유 메모리 크기보다 작은 행렬이므로 공유 메모리 크기 자체는 고려하지 않는다.)</p><p>공유 메모리를 사용하는 행렬 곱셉 커널은 아래와 같다.</p><pre tabindex=0><code>__global__ void matMul_kernel(float* A, float* B, float* C) {
    int row = threadIdx.x;
    int col = threadIdx.y;
    int index = row * blockDim.y + col;

    __shared__ float sA[ROW_SIZE][K_SIZE];
    __shared__ float sB[K_SIZE][COL_SIZE];

    if (threadIdx.x == 0 &amp;&amp; threadIdx.y == 0) {
        for (int r = 0; k &lt; ROW_SIZE; k++) {
            for (int k = 0; k &lt; K_SIZE; k++) {
                sA[r][k] = _A[r * K_SIZE + k];
            }
        }

        for (int c = 0; k &lt; ROW_SIZE; k++) {
            for (int k = 0; k &lt; K_SIZE; k++) {
                sB[k][c] = _B[c + k * K_SIZE];
            }
        }
    }

    __syncthreads();
    ...
    
    float result = 0;
    for (int k = 0; k &lt; K_SIZE; k++) {
        result += _A[row * K_SIZE + k] * _B[col + k * COL_SIZE];
    }
    _C[index] = result;
}
</code></pre><p>코드에서는 블록 내에서 <strong>threadIdx.x == 0 && threadIdx.y == 0</strong> 인 스레드 하나만이 공유 메모리에 데이터를 복사하도록 설정되어 있다. 이 경우, 다른 스레드들은 이 복사가 끝날 때까지 _<strong>_syncthreads()</strong> 에서 대기하게 된다. 이는 비효율적인 방법이다. 복사 작업을 단 하나의 스레드에게만 맡기면, 전체 블록이 이 작업이 끝날 때까지 기다리게 되어 병렬 처리의 장점을 제대로 살리지 못한다.</p><p>더 많은 스레드가 복사 작업에 참여하도록 하여 병렬로 데이터를 복사하게 하면, 복사 시간을 단축할 수 있다. 하지만 모든 스레드가 자신과 관련된 (row, col) 데이터를 복사하게 되면 <strong>중복 복사 문제</strong> 가 발생할 수 있다. 예를 들어, 행렬 A의 동일한 행(row)을 복사할 때 여러 스레드가 동일한 데이터를 공유 메모리에 중복으로 복사하면 메모리 사용이 비효율적이 된다. 이러한 중복 복사는 메모리 대역폭을 낭비하고, 연산 지연을 초래한다.</p><p>이 문제를 해결하기 위해서는 각 스레드가 서로 다른 부분의 데이터를 복사하도록 작업을 분배해야 한다. 특정 스레드들끼리 복사할 부분을 지정하는 방식으로 중복 복사를 피할 수 있다. 이렇게 하면 모든 스레드가 병렬로 데이터를 복사해 공유 메모리 초기화 시간을 단축할 수 있다. 아래 코드가 그 예시이다.</p><pre tabindex=0><code>__global__ void matMul_kernel(float* A, float* B, float* C) {
    ...

    __shared__ float sA[ROW_SIZE][K_SIZE];
    __shared__ float sB[K_SIZE][COL_SIZE];

    if (col == 0) { // read matrix A
        for (int k = 0; k &lt; K_SIZE; k++) {
            sA[row][k] = _A[row * K_SIZE + k];
        }
    }

    if (row == 0) { // read matrix B
        for (int k = 0; k &lt; K_SIZE; k++) {
            sB[k][col] = _B[col + k * COL_SIZE];
        }
    }

    __syncthreads();
    ...
}
</code></pre><p>공유 메모리보다 작은 크기의 데이터를 활용할 때에는 데이터 전체를 공유 메모리에 올려놓아도 문제가 없다. 하지만 공유 메모리보다 큰 데이터를 처리하기 위해서는 또 다른 전략이 필요하다.</p><h2 id=공유-메모리-사용-예제-2-큰-행렬>공유 메모리 사용 예제 2: 큰 행렬<a hidden class=anchor aria-hidden=true href=#공유-메모리-사용-예제-2-큰-행렬>#</a></h2><p><strong>작성 예정</strong></p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on x" href="https://x.com/intent/tweet/?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f&amp;title=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac&amp;summary=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f&title=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on whatsapp" href="https://api.whatsapp.com/send?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on telegram" href="https://telegram.me/share/url?text=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [기술] GPU와 CUDA (8) - 공유 메모리 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%ea%b8%b0%ec%88%a0%5d%20GPU%ec%99%80%20CUDA%20%288%29%20-%20%ea%b3%b5%ec%9c%a0%20%eb%a9%94%eb%aa%a8%eb%a6%ac&u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2024-08-25%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>