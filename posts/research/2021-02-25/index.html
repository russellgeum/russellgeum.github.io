<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다."><meta name=author content="Oppenheimer"><link rel=canonical href=https://russellgeum.github.io/posts/research/2021-02-25/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/posts/research/2021-02-25/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer"><meta property="og:description" content="Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다."><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/posts/research/2021-02-25/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-25T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-25T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer"><meta name=twitter:description content="Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://russellgeum.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer","item":"https://russellgeum.github.io/posts/research/2021-02-25/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer","name":"[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer","description":"Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다.","keywords":[],"articleBody":"Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다. 자연스럽게 다음의 질문이 생긴다. Overall, is it possible to build a single, unified model that simultaneously handles tasks in a variety of domains? Related Work 기존에는 하나의 도메인 또는 제한된 환경에서의 멀티모달 도메인 작업을 수행하였다. (예를 들면 비주얼 + 자연어) 각 태스크에 대한 파인 튜닝을 요구하였다. 태스크 간 파라미터를 공유하지 않았는데, 단순 산술로 N개 태스크에서 N배의 파라미터가 증가한다. 하나의 도메인에서 유사하거나 연관된 멀티 태스킹을 수행하였다. 오직 자연어면 자연어 아니면 비전 + 자연어 이런 형태이다. Contribution 앞서 말했듯이 하나의 모델로 여러 멀티 태스크를 훈련하고 추론한다. VQA, Visual Entailment같은 멀티 태스크가 하나의 모델로 학습하는 것으로부터의 이점을 증명한다. UniT의 구조 이미지는 DETR의 ResNet50과 pretrained 모델로 featuremap을 추출한다. 그리고 이를 트랜스포머 인코더에 넣는다. (트랜스포머의 인코더 구조도 그렇고 전반적으로 DETR을 많이 계승하였음) 텍스트는 BERT의 방법을 많이 따왔다. 텍스트 토큰화는 BERT의 그것과 같다. 그리고 이 토큰 시퀀스는 사전 학습된 BERT의 입력으로 들어간다. 다만 모종의 이유로 이 논문의 구현에서는 허깅페이스 트랜스포머 라이브러리를 사용하였다. 그럼다음 이들을 concatenate하고 공통의 트랜스포머 디코더에서 작업한다. 마지막 끝 단에 태스크 스페시픽한 헤드가 있어서 각자의 태스크에 맞게 추론을 한다. 디코더의 입력으로 태스크 스페시픽한 쿼리 임베딩 벡터를 받는다. 디코더의 구조는 DETR의 트랜스포머 디코더를 사용하였다. 마지막 Task Specific 헤드에 관하여… 오브젝트 디텍션을 예로 들면, 디코더의 출력에다가 배경을 포함하여 어떤 클래스인지 추론하는 분류기와 바운딩 박스를 추론하는 모듈을 추가하였다. 이는 디코더 히든 스테이트에서의 각 자리이다. (무슨 말???) Experiments 실험은 먼저 OD + VQA 훈련에 관한 실험을 하고, 그 다음에는 다른 태스크를 결합한 훈련을 하였다. OD: COCO, Visual Genome, VQA: VQAv2\n그리고 몇 가지 경우로 나누었다. 두 OD 데이터셋에 대해서 각각 VQA와 결합한 것과, 모두 다 합쳐서 학습한 경우이다. 또한 트랜스포머의 디코더 모듈도 파라미터를 쉐어하는 방법과 각 태스크에 맞게 분리하는 경우로 나누었다.\n요약하면 COCOinit를 쓰지 않고 오브젝트 디텍션은 디코더 파라미터를 공유하는 실험에서 가장 성능이 낮았다. VQA 역시 디코더 파마리터를 공유하는 실험에서 가장 성능이 낮았다. 반면에 COCO init를 사용하면서 디코더 파라미터를 공유하는 실험에서 오브젝트 디텍션의 성능이 가장 좋았다. 반면에 디코더 파라미터를 공유하지 않는 실험에서 VQA의 성능이 가장 좋았다. 결론: COCO init를 쓰면 디코더 파라미터를 공유하면서도 전반적인 멀티 모달의 성능을 최대한 끌어올릴 수 있다. 디코더 분리를 해서 VQA의 성능을 좀 더 높게 가져갈 수는 있더라도…\n따라서 파라미터르 공유만 하면 싱글 태스크 보다는 성능이 낮다. 왜냐하면 오브젝티 디텍션의 학습 스케줄은 길고 shared param이 멀티 데이터셋 간의 관계를 파악하기 위해서는 시간이 더 필요하다. 학습을 가속시키기 위해 그래서 COCO init를 사용하였다.\nConclusion 디코더 레이어 수는 직관적으로 8이 적당하다. 4는 너무 적고, 12는 더 나은 성능에 영향을 주지 않았다. (데이터셋이 클수록 디코더 레이어가 큰게 좋긴 한데, 12개의 레이어는 이미 모델 카파시티가 너무 큼) 버트의 모든 출력을 쓰는 것은 computational cost를 늘리지만 성능에 큰 의미 없었고 그냥 풀링된 벡터를 쓰는게 좋은 듯하다. ","wordCount":"498","inLanguage":"en","datePublished":"2021-02-25T00:00:00Z","dateModified":"2021-02-25T00:00:00Z","author":{"@type":"Person","name":"Oppenheimer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/posts/research/2021-02-25/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#contribution>Contribution</a></li><li><a href=#experiments>Experiments</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><ul><li>UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을
동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각
태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투
엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크
스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고
서로 다른 도메인 문제를 핸들링할 수 있다.</li><li>트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰
성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을
보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를
연결하는 시도는 잘 없었다. 자연스럽게 다음의 질문이 생긴다. Overall, is
it possible to build a single, unified model that simultaneously handles
tasks in a variety of domains?</li></ul><h2 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><ol><li>기존에는 하나의 도메인 또는 제한된 환경에서의 멀티모달 도메인 작업을
수행하였다. (예를 들면 비주얼 + 자연어)</li><li>각 태스크에 대한 파인 튜닝을 요구하였다. 태스크 간 파라미터를
공유하지 않았는데, 단순 산술로 N개 태스크에서 N배의 파라미터가
증가한다.</li><li>하나의 도메인에서 유사하거나 연관된 멀티 태스킹을 수행하였다. 오직
자연어면 자연어 아니면 비전 + 자연어 이런 형태이다.</li></ol><h2 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h2><ul><li>앞서 말했듯이 하나의 모델로 여러 멀티 태스크를 훈련하고 추론한다.</li><li>VQA, Visual Entailment같은 멀티 태스크가 하나의 모델로 학습하는 것으로부터의 이점을 증명한다.</li><li>UniT의 구조<ol><li>이미지는 DETR의 ResNet50과 pretrained 모델로 featuremap을 추출한다.
그리고 이를 트랜스포머 인코더에 넣는다. (트랜스포머의 인코더 구조도
그렇고 전반적으로 DETR을 많이 계승하였음)</li><li>텍스트는 BERT의 방법을 많이 따왔다. 텍스트 토큰화는 BERT의 그것과
같다. 그리고 이 토큰 시퀀스는 사전 학습된 BERT의 입력으로 들어간다. 다만
모종의 이유로 이 논문의 구현에서는 허깅페이스 트랜스포머 라이브러리를
사용하였다.</li><li>그럼다음 이들을 concatenate하고 공통의 트랜스포머 디코더에서
작업한다. 마지막 끝 단에 태스크 스페시픽한 헤드가 있어서 각자의 태스크에
맞게 추론을 한다. 디코더의 입력으로 태스크 스페시픽한 쿼리 임베딩 벡터를
받는다. 디코더의 구조는 DETR의 트랜스포머 디코더를 사용하였다.</li><li>마지막 Task Specific 헤드에 관하여… 오브젝트 디텍션을 예로 들면,
디코더의 출력에다가 배경을 포함하여 어떤 클래스인지 추론하는 분류기와
바운딩 박스를 추론하는 모듈을 추가하였다. 이는 디코더 히든
스테이트에서의 각 자리이다. (무슨 말???)</li></ol></li></ul><h2 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h2><p>실험은 먼저 OD + VQA 훈련에 관한 실험을 하고, 그 다음에는 다른
태스크를 결합한 훈련을 하였다. OD: COCO, Visual Genome, VQA: VQAv2</p><p>그리고 몇 가지 경우로 나누었다. 두 OD 데이터셋에 대해서 각각 VQA와
결합한 것과, 모두 다 합쳐서 학습한 경우이다. 또한 트랜스포머의 디코더
모듈도 파라미터를 쉐어하는 방법과 각 태스크에 맞게 분리하는 경우로
나누었다.</p><p>요약하면 COCOinit를 쓰지 않고 오브젝트 디텍션은 디코더 파라미터를
공유하는 실험에서 가장 성능이 낮았다. VQA 역시 디코더 파마리터를
공유하는 실험에서 가장 성능이 낮았다. 반면에 COCO init를 사용하면서
디코더 파라미터를 공유하는 실험에서 오브젝트 디텍션의 성능이 가장
좋았다. 반면에 디코더 파라미터를 공유하지 않는 실험에서 VQA의 성능이
가장 좋았다. 결론: COCO init를 쓰면 디코더 파라미터를 공유하면서도
전반적인 멀티 모달의 성능을 최대한 끌어올릴 수 있다. 디코더 분리를 해서
VQA의 성능을 좀 더 높게 가져갈 수는 있더라도…</p><p>따라서 파라미터르 공유만 하면 싱글 태스크 보다는 성능이 낮다.
왜냐하면 오브젝티 디텍션의 학습 스케줄은 길고 shared param이 멀티
데이터셋 간의 관계를 파악하기 위해서는 시간이 더 필요하다. 학습을
가속시키기 위해 그래서 COCO init를 사용하였다.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ol><li>디코더 레이어 수는 직관적으로 8이 적당하다. 4는 너무 적고, 12는
더 나은 성능에 영향을 주지 않았다. (데이터셋이 클수록 디코더 레이어가
큰게 좋긴 한데, 12개의 레이어는 이미 모델 카파시티가 너무 큼)</li><li>버트의 모든 출력을 쓰는 것은 computational cost를 늘리지만 성능에 큰
의미 없었고 그냥 풀링된 벡터를 쓰는게 좋은 듯하다.</li></ol></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Transformer%20is%20All%20You%20need%2c%20Multimodal%20Multitask%20Learning%20with%20a%20Unified%20Transformer&u=https%3a%2f%2frussellgeum.github.io%2fposts%2fresearch%2f2021-02-25%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>