<!doctype html><html lang=en dir=auto><head></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] Survey: Large Language Models Compression</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction--method>Introduction & Method</a><ul><li><a href=#1-pruning>1. Pruning</a></li><li><a href=#2-knowledge-distillation>2. Knowledge Distillation</a></li><li><a href=#3-quantization>3. Quantization</a></li><li><a href=#4-low-rank-factorization>4. Low-Rank Factorization</a></li></ul></li><li><a href=#metrics-and-benchmakrs>Metrics and Benchmakrs</a><ul><li><a href=#1-number-of-parameters>1. Number of Parameters</a></li><li><a href=#2-model-size>2. Model Size</a></li><li><a href=#3-compression-ratio>3. Compression Ratio</a></li><li><a href=#4-inference-time>4. Inference Time</a></li><li><a href=#5-flops>5. FLOPs</a></li><li><a href=#6-common-benchmarks>6. Common Benchmarks</a></li><li><a href=#7-hulk>7. HULK</a></li><li><a href=#8-elue>8. ELUE</a></li></ul></li><li><a href=#challenges-and-future-directions>Challenges and Future Directions</a><ul><li><a href=#1-specialized-benchmarks>1. Specialized Benchmarks</a></li><li><a href=#2-performance-size-trade-offs>2. Performance-Size Trade-offs</a></li><li><a href=#3-dynamic-llm-compression>3. Dynamic LLM Compression</a></li><li><a href=#4-explainability>4. Explainability</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h1 id=대규모-언어-모델의-경량화-동향>대규모 언어 모델의 경량화 동향<a hidden class=anchor aria-hidden=true href=#대규모-언어-모델의-경량화-동향>#</a></h1><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><ul><li>LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬</li><li>LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함</li><li>Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌</li><li>압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함</li></ul><h2 id=introduction--method>Introduction & Method<a hidden class=anchor aria-hidden=true href=#introduction--method>#</a></h2><p>대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다. Half-Precision (FP16) 으로 저장하면 320GB 크기이다. 추론만 하더라도 80GB VRAM의 A100 GPU가 최소 5개 이상을 필요하다.<br>이 이슈를 모델 압축 접근법으로 해결한다. 모델 압축은 큰 리소스를 소모하는 모델을 제한된 리소스에 배포하기 위해 필요한 모든 과정을 말한다. 압축에서 트레이드오프가 발생하는 조건들이 있지만, 공통의 목표는 아래와 같이 정리된다.</p><ol><li>추론 실행 속도를 높인다.</li><li>모델 저장 공간을 줄인다.</li></ol><p>이 서베이의 LLM 압축 방법은 크게 4가지로 나눈다. Pruning, Knowledge Distillation, Quantization, Low-Rank Factorization이다. 물론 꼭 이 4가지 방법으로 정확하게 기술들이 나누어지지는 않는다. 여러 방법을 섞은 연구들도 있고, 이 외의 방법들도 존재할 수 있다. 다만 이 서베이의 관점이 그렇다.</p><h3 id=1-pruning>1. Pruning<a hidden class=anchor aria-hidden=true href=#1-pruning>#</a></h3><p>Pruning는 Structured pruning, Unstructured pruning으로 나눈다. 전자는 모델의 구조적 측면에서 잘라내는 방법이고, 후자는 개별 파라미터에 프루닝을 진행한다. LLM 경량화 또한 최근 Pruning이 적용된 연구들이 있다. Unstructured pruning은 정확도에 영향을 적게 끼치는 파라미터들을 0으로 만드는 방법이지만, Sparsity 모델을 효율적으로 저장히기 위해 특수한 압축 알고리즘이 필요하다. 또한 Retraining을 요구하기도 하는데, 이는 LLM에서는 쉽지 않은 방법이다.<br>SparseGPT[1]는 재훈련이 필요없는 One-shot pruning 방법을 제안한다. 이 연구는 Pruning을 Sparse-regression 문제로 접근하였고, GPT-like한 LLM에서 좋은 성능을 보여주었다. [2]는 최소한의 Retraining만 필요로 하는 Iterative pruning 방법을 LLM에 적용하였다. LoRAPrune[3]은 Parameter-Efficietn Tuning (PEFT) 방법과 Pruning을 결합하여 다운스트림 태스크에서 우수함을 보여주었다. 또한 이 연구에서는 Low-Rank Adaptation(LoRA)[4]도 활용한다. Wanda[4]는 LLM pruning을 위한 새로운 기준을 제안한다. Wanda는 각 weight를 입력된 활성함수 값의 크기와 노름의 곱으로 평가하여 중요도를 랭킹한다. 이 랭킹은 적은 calibration dataset으로 근사 가능하다. Wanda를 통해 LLM에서 중요성이 낮은 weight를 알아낼 수 있다.<br>Structured pruning은 LLM의 구조 자체를 끊어낼 수 있다. 채널/레이어 레벨에서 프루닝을 하기에, 모델 복잡도를 감소하는 효과가 있고 메모리 풋프린트에서도 유리하다. 이 갈래에서 중요한 연구는 LLM-Pruner[5]가 있다. [5]는 모델 내의 종속성을 알기 위해 탐지 알고리즘을 제안한다. 또한 first-order information과 hessian을 활용하여, 모델 내의 중요도가 높은 그룹을 선정하는 전략을 보여준다.</p><p>[1] <a href=https://arxiv.org/abs/2301.00774>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot</a><br>[2] <a href="https://openreview.net/forum?id=cKlgcx7nSZ">Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models</a><br>[3] <a href=https://arxiv.org/abs/2305.18403>Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning</a><br>[4] <a href=https://arxiv.org/abs/2306.11695>A Simple and Effective Pruning Approach for Large Language Models</a><br>[5] <a href=https://arxiv.org/abs/2305.11627>LLM-Pruner: On the Structural Pruning of Large Language Models</a></p><h3 id=2-knowledge-distillation>2. Knowledge Distillation<a hidden class=anchor aria-hidden=true href=#2-knowledge-distillation>#</a></h3><p>LLM에서 KD는 2가지 방법으로 나눌 수 있다.</p><ol><li>Basic Knowledge Distillation (KD)</li><li>Emergent Abilities based Knowledge Distillation (EA-based KD)</li></ol><p>특히 EA는 In-Context Learning, Chain-of-Thought, Instruction Tuning과 같이 LLM으로부터 원하는 답변을 가이던스하는 방법들을 총칭한다. 먼저 LLM에서 클래식한 KD를 적용하는 방법은 Output distribution과 Feature information을 Teacher LLM애서 Student LLM으로 Transfer learning하는 것이 기본이다.<br>몇 연구로 MINILLM[1]은 Student model이 Tearcher model의 Low-probability region을 과대평가하지 못하도록 클래식한 KD의 KL-divergence 연산을 reverse-KLD로 학습하는 방법을 제안한다. 한편 GKD[2]에서는 Auto-regressive model로부터의 KD 방법을 탐구한다. 두 가지 문제를 관찰하는데, 첫 번째는 Student modeld의 훈련/배포 간의 출력 시퀀스 분포가 불일치 하다는 것. 두 번째는 Student model이 Teacher model만큼의 강력한 표현력을 가지지 못하는 점이다. 따라서 [2]는 훈련 중 Student model이 생성한 출력 분포를 샘플링하여 불일치 문제를 해결하고, Reverse-KLD을 최적화하여 Student model의 부족한 표현력을 채운다.<br>한편 EA-based KD는 단순히 LLM의 knowledge를 Distillation하는 것만 아니라 창발적인 능력까지 Distillation하는 것에 포커싱한다. 최근 연구들은 GPT-3 (175B), PaLM (540B) 같은 LLM들이 GPT-2 (1.5B), BERT (330M) 모델에 비해 창발성을 보여준다. [3, 4, 5]. LLM의 창발성을 앞서 말했듯이 아래 키워드로 연관되는데 레퍼런스를 참조하면 좋다. [6, 7, 8, 9, 10, 11]</p><ol><li>In-Context Learning (ICL)</li><li>Chain-of-Thought (CoT)</li><li>Instruction-Following (IF)</li></ol><p>In-Context Learning은 LLM에게 몇 작업 예시와 설명으로 그래디언트 업데이트없이 새로운 태스크를 수행하도록 유도한다. Huang의 연구가 ICL을 통한 distillation 방법이다 [12]. 구체적으로 이 연구는 몇 가지 예제 (Few-shot)을 통한 ICL 기법으로 teacher LLLM에서 student LLM으로 distillation이 어떻게 가능한지를 보여준다. 저자들은 ICL distillation이 두 가지 few-shot 패러다임 아래에 있는 것을 고민한다. 첫 번째는 Meta In-context Learning이고 두 번째는 Multitask In-context Learning이다. 두 방법을 간략히 비교하자면 Multiask-ICT는 Meta-ICT보다 퍼포먼스가 우수하지만, Task-adaptation 동안 더 많은 계산량을 필요로 한다.</p><p>CoT는 ICL처럼 즉각적인 답을 포함하는 입출력 쌍 대신 중간 단계 출력을 최종 출력을 위한 입력으로 사용한다. CoT를 활용한 Distllation 연구는 다음 레퍼런스를 따른다 [13, 14, 15, 16, 17, 18, 19]. MT-COT[13]은 LLM이 생성한 설명을 활용하여 Small model의 훈련을 향상시키는 것을 목표한다. Small model에 강력한 추론 및 설명 능력을 부여하기 위해, Multitask learning 프레임워크를 활용한다. Fine-tune-CoT[14]는 스토캐스틱한 샘플링을 통해 여러 개의 추론 솔루션을 생성한 후, 학습 데이터로 활용한다. 이는 Studnet model을 위한 Data augemntation 관점이다. SOCRATIC CoT[16]은 두 개의 Distillated model를 훈련한다. 첫 번째는 큰 문제를 작은 문제로 쪼개는 모델이고, 두 번째는 각 서브 문제를 해결하는 모델이다. 이 외에도 DISCO[17], SCOTT[18] Step-by-Step[19] 같이 CoT를 적극 Distillation에 관한 연구들이 있으니 참조한다.</p><p>Instruction Following은 모델의 새로운 태스크 수행 능력을 향상시키기 위해, 바로 태스크 설명 기반으로 Fine-tuning을 거친다. 여러 태스크를 지시문 형태로 나열하면, 모델은 처음 보는 태스크라도 정확하게 작업을 수행한다. 이에 관한 연구로 [20]이 있다. 이 연구는 LLM이 Student model 능력을 향상시키기 위해 내재된 Adaptable한 성질을 활용한다. 구체적으로 LLM은 어려운 프롬프트를 구별하고 생성하고, 이를 Student model의 능력이 향상되도록 활용한다. 이 접근은 LLM의 확장 가능한 성질이 student model이 복잡한 지시/작업을 해결하는 컨트롤러로 쓰인다.</p><p>[1] <a href=https://arxiv.org/abs/2306.08543>Knowledge Distillation of Large Language Models</a><br>[2] <a href=https://arxiv.org/abs/2306.13649>GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models</a><br>[3] <a href=https://arxiv.org/abs/2206.07682>Emergent Abilities of Large Language Models</a><br>[4] <a href=https://arxiv.org/abs/2304.15004>Are Emergent Abilities of Large Language Models a Mirage?</a><br>[5] <a href=https://arxiv.org/abs/2303.18223>A Survey of Large Language Models</a><br>[6] <a href=https://arxiv.org/abs/2301.00234>A Survey on In-context Learning</a><br>[7] <a href=https://arxiv.org/abs/2201.11903>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a><br>[8] <a href=https://arxiv.org/abs/2203.11171>Self-Consistency Improves Chain of Thought Reasoning in Language Models</a><br>[9] <a href=https://arxiv.org/abs/2210.03057>Language Models are Multilingual Chain-of-Thought Reasoners</a><br>[10] <a href=https://arxiv.org/abs/2203.02155>Training language models to follow instructions with human feedback</a><br>[11] <a href=https://arxiv.org/abs/2211.09800>InstructPix2Pix: Learning to Follow Image Editing Instructions</a><br>[12] <a href=https://arxiv.org/abs/2212.10670>In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models</a><br>[13] <a href=https://arxiv.org/abs/2210.06726>Explanations from Large Language Models Make Small Reasoners Better</a><br>[14] <a href=https://arxiv.org/abs/2212.10071>Large Language Models Are Reasoning Teachers</a><br>[15] <a href=https://arxiv.org/abs/2301.12726>Specializing Smaller Language Models towards Multi-Step Reasoning</a><br>[16] <a href=https://arxiv.org/abs/2212.00193>Distilling Reasoning Capabilities into Smaller Language Models</a><br>[17] <a href=https://arxiv.org/abs/2212.10534>DISCO: Distilling Counterfactuals with Large Language Models</a><br>[18] <a href=https://arxiv.org/abs/2305.01879>SCOTT: Self-Consistent Chain-of-Thought Distillation</a><br>[19] <a href=https://arxiv.org/abs/2305.02301>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</a><br>[20] <a href=https://arxiv.org/abs/2305.12870>Lion: Adversarial Distillation of Closed-Source Large Language Model</a></p><h3 id=3-quantization>3. Quantization<a hidden class=anchor aria-hidden=true href=#3-quantization>#</a></h3><p>Quantization(양자화)는 딥러닝 모델의 저장/계산 비용을 줄이기 위해 가장 널리 연구된 분야이다. 딥러닝 모델은 원래 FP32/FP16 데이터형을 사용하지만, 양자화을 통해 부동소수점 표현을 int형이나 Discretization 표현으로 변환된다. 이 변환에서 정확도와 계산 복잡도 간 트레이드 오프가 있지만, 적절한 양자화 기술을 사용하면 최소한의 정확도 손실만으로도 좋은 압축률 가진 모델을 얻을 수 있다. 양자화는 아래의 기술로 나눈다. (개인적으로는 1. 2. 가 충분한 갈래라고 생각한다.)</p><ol><li>Quantization-Aware Training [1, 2, 3]</li><li>Post-Training Quantization [4, 5]</li><li>Quantization-Aware Fine-tuning [6, 7, 8]</li></ol><p>각각 차이는 모델 훈련 - 추론 - 배포 단계에서 양자화 기법이 어느 단계에 들어가는지의 차이이다. QAT는 훈련과 동시에 양자화를 적용한다. QAF는 사전 학습된 모델의 Fine-tuning 과정에서 양자화를 적용한다. PTQ는 모델의 훈련이 끝난 후, 양자화를 적용한다. 최근의 양자화 연구는 LLM에 녹여드는 중이며, 연구들을 8-bit / Lower precision 카테고리별로 분류하였다.</p><ul><li>8-bit Quantization<ol><li><a href=https://arxiv.org/abs/2206.09557>LUT-GEMM</a></li><li><a href=https://arxiv.org/abs/2208.07339>LLM.int8()</a></li><li><a href=https://arxiv.org/abs/2206.01861>ZeroQuant</a></li><li><a href=https://arxiv.org/abs/2211.10438>SmoothQuant</a></li></ol></li><li>Lower-bit Quantization<ol><li><a href=https://arxiv.org/abs/2305.17888>LLM-QAT</a></li><li><a href=https://arxiv.org/abs/2305.14152>PEQA</a></li><li><a href=https://arxiv.org/abs/2305.14314>QLoRA</a></li><li><a href=https://arxiv.org/abs/2210.17323>GPTQ</a></li><li><a href=https://arxiv.org/abs/2306.00978>AWQ</a></li><li><a href=https://arxiv.org/abs/2306.03078>SpQR</a></li><li><a href=https://arxiv.org/abs/2304.01089>RPTQ</a></li><li><a href=https://arxiv.org/abs/2304.07493>OliVe</a></li><li><a href=https://arxiv.org/abs/2209.13325>Outlier Suppression</a></li><li><a href=https://arxiv.org/abs/2306.02272>OWQ</a></li><li><a href=https://arxiv.org/abs/2307.09782>ZeroQuant-FP</a></li></ol></li></ul><p>QAT의 최종 목표는 양자화 알고리즘이 훈련 루프에 통합되는 것이다. 이 방식은 모델이 Low-precision에도 Adaptive하도록 훈련을 돕는다. 그래서 QAT는 상대적으로 높은 성능을 보여준다. 대표적 연구는 LLM-QAT가 있다. 이 연구는 먼저 훈련 데이터 수집의 한계를 사전 학습된 모델의 생성 결과를 활용하여 데이터 프리한 KD를 수행한다. 또한 LLM-QAT는 weight, activation뿐만 아니라 KV cache까지 양자화한다. 이 방법은 throughput을 향상시키고, 더 긴 시퀀스의 의존성을 지원한다. LLM-QAT의 성취는 Quantized weight와 KV cache를 가진 LLaMA모델을 4비트로 압축하고 Distiilation할 수 있는 것이다. QAT를 적용한 최초의 4비트 LLM 연구이다.</p><p>QAF는 Fine-tuning 과정에서 LLM을 양자화한다. 목표는 Low-precision 양자화 후에도, 성능을 유지하는 것이다. PEQA, QLoRA 연구가 이 갈래에 속한다. 이 연구들은 모델 가속/추론 가속 측면 모두에 중점을 둔다. PEQA는 2단계의 프로세스를 거치는데, 첫 번째는 FCN의 파라미터를 Low-bit 행렬과 스칼라 벡터로 양자화한다. 두 번째로 각각의 다운스트림 태스크의 스칼라 벡터에서 Fine-tuning을 진행한다. QLoRA는 새로운 데이터 타입, 이중 양자화 그리고 Paged optimizer 같은 개념을 도입한다. 이 연구의 아이디어들은 퍼포먼스를 보장하면서 메모리 풋프린트를 줄이는 것에 집중한다. QLoRA의 경우, LLM을 하나의 GPU에서 실행할 수 있으며, Vicuna 벤치마크에서 SOTA를 달성하였다.</p><p>PTQ는 LLM을 학습한 후에 양자화한다. PTQ의 중요한 목표는 LLM 아키텍처의 수정이나 재훈련 과정이 없으면서 모델 복잡도 및 메모리 사용량을 줄이는 것이다. PTQ의 최대 장점은 과정의 단순성과 효율성이다. 하지만 완전 학습 후에 양자화를 하는 만큼 손실도가 큰 단점이 있다. PTQ는 추가적 리소스나 계산 비용없이 깔끔하게 LLM 양자화의 효율성을 가져올 수 있는 것에 의미가 크다.</p><p>PTQ에서 일부 접근 방식은 LLM의 weight만 양자화하는 형태로 필요 계산량을 줄이는 것에 중점을 둔다. LUT-GEMM은 weight만 양자화하고, BCQ 방법을 사용하여 LLM의 행렬 곱셉을 최적화하고 레이턴시를 줄였다. LLM.int8()은 트랜스포머 내의 행렬 곱셉에 8비트 양자화를 적용하여, GPU 추론에서 성능을 유지하면서 메모리 사용량을 절반으로 줄였다. 이 연구는 175B 모델에서도 큰 성능 저하없이 양자화 추론이 가능하다. ZeroQuant는 HW-aware Quantization, Layer-by-Layer Distillation, Optimized Quantization 등을 통합하여 트랜스포머 기반 모델의 weight와 activation을 최소한의 정확도 손실로 int8 양자화를 하였다.</p><p>GPTQ는 위 방법이 8비트같이 낮은 압축에는 잘 작동하지만, 더 높은 속도에서 정확도를 유지하는데 어려움이 있다고 주장한다. 따라서 GPTQ는 대략적인 2차 정보를 기반으로 하는 새로운 레이어별 양자화 방법을 제안한다. 그 결과 Full-precision model에 비해 정확도 손실은 최소화하면서 per-weight로 bitwidth를 3~4비트로 줄였다.</p><p>AWQ는 LLM 성능에서 모든 weight가 동등하게 중요하지 않다고 주장한다. 그래서 중요한 weight만 잘 보호해도, 양자화 오류를 크게 줄일 수 있다고 판단하였다. 이 인사이트를 기반으로 AWQ는 Activation-aware한 접근을 고민한다. 이는 크기가 큰 Activation에 대응하는 weight channel의 중요성을 고려하는 방법이다. 따라서 양자화 오류를 최소화하는 per-channel 레벨로 최적의 스케일링 팩터를 찾는 것을 제안한다. 더 나아가 3~4비트 레벨로 LLM 양자화를 위해서, SpQR은 Outlier weight를 High-precision으로 저장하고 다른 모든 weight들을 3~4비트로 양자화하였다.</p><p>LLM의 weight만 양자화하는 연구들을 제외하면, PTQ의 많은 연구들이 weight, activation 모두 양자화를 시도한다. 구체적으로 SmoothQuant는 Outlier로 인해 더 복잡해지는 Activation을 양자화한다. 서로 다른 토큰이 채널 전체적으로 비슷한 변동성을 보이는 관찰에 착안하여, SmoothQuant는 채널별 스케일링 변환을 통해 모델을 더욱 양자화하기 쉽도록 만든다.</p><p>RPTQ는 Outlier가 전 채널에 걸쳐 고르지 않는 범위에서 관찰되는 문제에 집중한다. 이를 해결하기 위해 채널을 클러스터하여 양자화함으로써 채널 간 범위 불일치 문제를 완화하였다. 또한 채널 재정렬을 Layer-norm 연산과 Linear layer weight에 통합하여 오버헤드를 최소화하였다.</p><p>OliVe는 Outlier는 중요하지만, 주변의 normal weight는 그렇지 않다고 판단한다. 그래서 Outlier-victim pair Quantization을 통해, 낮은 HW 오버헤드를 갖는 Outlier를 로컬리하게 핸들링하고 높은 퍼포먼스를 보여준다. Outlier Suppresion+는 activation의 유해한 Outlier가 특정 채널에 집중되는 비대칭적 분포를 보이는 것을 관찰한다. 그래서 Outlier의 비대칭성을 해결하고, 문제가 될 채널 영향을 줄이기 위해 Per channel shift, scaling 방법을 제안한다.</p><p>OWQ는 Activation outlier가 어떻게 weight 양자화 오류를 누적시키는지 이론적 해석을 덧붙였다. 이 분석에서 OWQ는 Activation outlier에 의해 양자화에 민감한 weight에 더 높은 정밀도를 주는 방식으로 Mixed-precision을 제안한다.</p><p>또한 ZeroQuant-FP는 FP8/FP4 양자화 가능성에 집중한다. 이 연구에는 FP8 activiation이 int8을 능가하는 반면, weight 양자화 측면에서는 비슷한 성능을 보여줌을 주장한다. weight와 activiation 차이로 생기는 문제를 모든 스케일링 팩터를 2의 거듭제곱으로 두어 해결한다. 또한 Low Rank Comepnsation 전략으로 양자화 효율성을 더 끌어올렸다.</p><p>[1] <a href=https://arxiv.org/abs/2211.11014>Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders</a><br>[2] <a href=https://arxiv.org/abs/2008.05000>Degree-Quant: Quantization-Aware Training for Graph Neural Networks</a><br>[3] <a href=https://arxiv.org/abs/2203.15952>4-bit Conformer with Native Quantization Aware Training for Speech Recognition</a><br>[4] <a href=https://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf>On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-tuning</a><br>[5] <a href=https://arxiv.org/abs/1905.03696>HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision</a><br>[6] <a href=https://arxiv.org/abs/2106.14156>Post-Training Quantization for Vision Transformer</a><br>[7] <a href=https://arxiv.org/abs/2004.10568>Up or Down? Adaptive Rounding for Post-Training Quantization</a><br>[8] <a href=https://arxiv.org/abs/2002.00104>Post-Training Piecewise Linear Quantization for Deep Neural Networks</a></p><h3 id=4-low-rank-factorization>4. Low-Rank Factorization<a hidden class=anchor aria-hidden=true href=#4-low-rank-factorization>#</a></h3><p>Low-Rank Factorization은 주어진 weight matrix를 2개 이상의 저차원 행렬로 분해하여 근사하는 모델 압축 기술이다. 저차원 분해의 핵심 아이디어는 큰 weight matrix W를 작은 행렬 U, V로 분해하는 것이며, U와 V의 크기는 각각 m x k / k x n이다. 이때 k는 m, n보다 훨씬 작은 값이다. W ~ UV로 원래의 weight matrix를 근사하며, 파라미터 수와 계산 오버헤드를 감소시킨다. 특히 LLM에서 저차원 분해는 효율적으로 모델을 튜닝하기 위해 널리 사용되었다. LoRA를 필두로 그 파생 방법들이 있다. LLM 압축 분야에서 이 방법은 다양한 기술과 결합된 모습으로 나타난다. LoRAPrune, ZeroQunat-FP 등이 그것이다. LLM에서 저차원 분해를 더 잘 활용하기 위한 지속적인 연구가 필요하다.<br>[1] <a href="https://openreview.net/forum?id=nZeVKeeFYf9">LoRA: Low-Rank Adaptation of Large Language Models</a><br>[2] <a href=https://arxiv.org/abs/2305.18403>LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning</a><br>[3] <a href=https://arxiv.org/abs/2206.01861>ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a><br>[4] <a href=https://arxiv.org/abs/2210.07558>DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation</a><br>[5] <a href=https://arxiv.org/abs/2303.10512>Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a><br>[6] <a href=https://arxiv.org/abs/2306.07967>One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning</a></p><h2 id=metrics-and-benchmakrs>Metrics and Benchmakrs<a hidden class=anchor aria-hidden=true href=#metrics-and-benchmakrs>#</a></h2><h3 id=1-number-of-parameters>1. Number of Parameters<a hidden class=anchor aria-hidden=true href=#1-number-of-parameters>#</a></h3><ul><li>LLM 파라미터는 최적화 및 훈련해야하는 weight의 총 갯수</li><li>스케일이 클수록 LLM 표현력이 좋지만, 그만큼 훈련, 추론에 많은 리소스 및 메모리가 필요</li></ul><h3 id=2-model-size>2. Model Size<a hidden class=anchor aria-hidden=true href=#2-model-size>#</a></h3><ul><li>LLM 모델 크기는 필요한 디스크 저장 공간을 가리킴</li><li>파라미터 수와 비례하며, 더 많은 파라미터가 곧 더 큰 저장 공간을 의미</li><li>메모리 풋프린트에 끼치는 요인으로 파라미터 뿐만 아니라, 데이터 타입 및 모델 아키텍처도 포함</li></ul><h3 id=3-compression-ratio>3. Compression Ratio<a hidden class=anchor aria-hidden=true href=#3-compression-ratio>#</a></h3><ul><li>압츅륲은 원래 모델 대비, 압축된 모델의 비율을 의미</li><li>더 높은 압축률은 성능을 유지하면서 더 크게 경량화되었다는 것을 의미</li></ul><h3 id=4-inference-time>4. Inference Time<a hidden class=anchor aria-hidden=true href=#4-inference-time>#</a></h3><ul><li>추론 시간 또는 레이턴시는 추론 중에 프롬프트를 처리하고, 응답 생성에 걸리는 시간을 측정</li><li>추론 시간은 실시간으로 대량 데이터를 처리하는 애플리케이션 레벨에서 중요</li></ul><h3 id=5-flops>5. FLOPs<a hidden class=anchor aria-hidden=true href=#5-flops>#</a></h3><ul><li>FLOPs는 LLM이 추론할 때, 실행하는 부동 소수점 연산의 수를 의미</li><li>FLOPs가 절대적이지는 않지만, 서로 다른 LLM 모델 간의 경량화 우위를 비교할때 유용한 지표</li></ul><h3 id=6-common-benchmarks>6. Common Benchmarks<a hidden class=anchor aria-hidden=true href=#6-common-benchmarks>#</a></h3><ul><li>다수의 연구는 압축된 LLMs의 성능을 잘 알려진 NLP 벤치마크에서 평가</li><li>GLUE와 SuperGLUE는 언어 모델 성능을 다양한 자연어 이해 태스크에서 평가하기 위해 설계</li><li>LAMBADA는 언어 모델의 문맥 의존적 이해를 평가하기 위해 설계</li><li>LAMA와 StrategyQA는 언어 모델의 추론 능력을 평가하기 위해 설계</li><li>SQuAD는 기계 독해 태스크를 위해 설계</li></ul><h3 id=7-hulk>7. HULK<a hidden class=anchor aria-hidden=true href=#7-hulk>#</a></h3><ul><li>HULK는 사전 훈련된 자연어 모델의 에너지 효율성을 평가</li><li>이 벤치마크는 커뮤니티에서 널리 사용되는 레거시한 데이터셋들을 포함</li><li>멀티 태스크 접근은 사전 훈련, 튜닝, 추론 단계 등에서 특정 퍼포먼스를 위한 시간, 비용을 측정</li></ul><h3 id=8-elue>8. ELUE<a hidden class=anchor aria-hidden=true href=#8-elue>#</a></h3><ul><li>ELU는 성능-효율성의 트레이드오프 측면에서 방법 간 비교를 제공</li><li>감정 분석, 자연어 추론, 유사도, 패러프레이징 태스크에서 6가지 데이터셋을 포함</li><li>파라미터 수를 기반으로, 다양한 FLOPs 설정에서 ElasticBERT와의 비교를 제공</li></ul><h2 id=challenges-and-future-directions>Challenges and Future Directions<a hidden class=anchor aria-hidden=true href=#challenges-and-future-directions>#</a></h2><h3 id=1-specialized-benchmarks>1. Specialized Benchmarks<a hidden class=anchor aria-hidden=true href=#1-specialized-benchmarks>#</a></h3><ul><li>모델 압축의 성능 평가 기준은 아직 표준화된 방법이 없음</li><li>LAMA, StrategyQA 같은 일반적인 벤치마크는 모바일 장치에서 알맞지 않음</li><li>사전 훈련된 모델을 위한 벤치마크가 LLM에 적합하지 않을 수 있음</li></ul><h3 id=2-performance-size-trade-offs>2. Performance-Size Trade-offs<a hidden class=anchor aria-hidden=true href=#2-performance-size-trade-offs>#</a></h3><ul><li>제한된 하드웨어 조건과 최적의 퍼포먼스 간의 트레이드오프에서 충분한 연구가 부족함</li></ul><h3 id=3-dynamic-llm-compression>3. Dynamic LLM Compression<a hidden class=anchor aria-hidden=true href=#3-dynamic-llm-compression>#</a></h3><ul><li>LLM에 NAS와 같은 기술을 접목하여, 자동화된 압축 방법의 연구가 필요함</li></ul><h3 id=4-explainability>4. Explainability<a hidden class=anchor aria-hidden=true href=#4-explainability>#</a></h3><ul><li>설명 가능한 압축 방법을 연구하는 것이 필요함. 실제품 레벨에서 신뢰성을 높일 수 있음</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ul><li>이 서베이는 대규모 언어 모델에 대한 모델 압축 기술을 포함</li><li>구체적으로 압축 방법, 평가 지표 및 벤치마크 데이터셋</li><li>LLM 압축 기술이 발전함에 따라 LLM에 특화된 기법에 대한 연구가 더욱 필요</li></ul></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression&amp;source=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression%20-%20https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression&amp;url=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] Survey: Large Language Models Compression on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20Survey%3a%20Large%20Language%20Models%20Compression&u=https%3a%2f%2frussellgeum.github.io%2fposts%2f2023-08-29%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>