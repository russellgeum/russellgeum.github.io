---
date: 2024-02-04
author: "oppenheimer1223"
title: "[논문] Survey: Large Multimodal Models"
categories: "논문"
weight: 10
---


## 개요
최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 
구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후,
이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
## 중요한 트렌드
- **멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)**  
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT)
- **Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인**  
(예시: BLIP-2 → InstructBLIP → DRESS)
- **다양한 모달리티으로의 확장**  
(예시: BLIP-2 → X-LLM, InstructBLIP → X-InstructBLIP)
- **높은 품질의 데이터셋 활용**  
(예시: LLaVA → LLaVA-1.5, 2024. 01. 30 기준 LLaVA 1.6)
## 훈련 파이프라인
- **Multimodal Pre-Training (MM PT)**  
일반적으로 사전 훈련 단계에서 모델은 (X - Text) 데이터셋을 통하여 다양한 모달리티 간의 임베딩을 정렬하고, 최종적으로 미리 정의된 목표를 최적화한다. 이때 데이터셋은 (이미지, 텍스트), (비디오, 텍스트), (오디오, 텍스트) 유형으로 나눈다. 더 나아가 (이미지, 텍스트)는 (<이미지1, 텍스트1> <이미지2, 텍스트2> <이미지3, 텍스트3> ...) 형태로 구성될 수 있다. 
- **Multomodal Instruction Tuning (MM IT)**  
지시문 구조의 데이터셋을 사용하여 사전 훈련된 MM-LLMs를 튜닝하는 방법론이다. 이 과정을 통하여 MM-LLMs는 새로운 지시에 따른 작업을 일반화하고, 제로샷 성능을 향상시킬 수 있습니다. 이 간단하지만 중요한 개념은 NLP 분야에서의 성공적인 방법을 따르는 형태이다. 자연어 처리처럼 다시 MM IT는 지도학습 기반 튜닝 (SFT)과 인간 피드백으로부터의 강화 학습 (RLHF)으로 나눌 수 있다. 최종적으로 인간의 의도나 선호도를 모델의 출력과 일치시키고 MM-LLMs의 상호 작용 능력을 향상하는 것이 목표이다.  
## 훈련 레시피
- 고해상도 이미지는 모델에 더 많은 시각적 특징을 부여한다.  
그러나 높은 해상도는 더 긴 토큰 시퀀스로 이어져 훈련, 추론 비용이 커진다.
- 고품질 SFT 데이터셋은 특정 작업에서의 성능을 크게 향상시킨다.
- LLM 백본에 PEFT를 수행하는 것은 ICL에 중요한 임베딩 정렬을 도와준다.
- 인터리브 이미지 - 텍스트 데이터는 유용하지만, (이미지, 텍스트) 쌍만으로는 부족하다.
- SFT 동안 텍스트 지시 데이터를 (이미지, 텍스트) 데이터와 혼합하는 것은  
텍스트 작업 저하를 해결할 뿐만 아니라 Vision-Language 작업 정확도를 올린다.

## 미래
- 더 강력한 모델  
- 더 어려운 벤치마크
- 구체화된 지능의 구현
- 모델 경량화와 On-Device 배포
- 지속적인 Instruction Tuning 기법

## 서베이
[1] [A Survey of Resource-efficient LLM and Multimodal Foundation Models](https://arxiv.org/abs/2401.08092)  
[2] [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601)  

## 주요 모델
[1] [CogVLM](https://arxiv.org/abs/2311.03079)  
[2] [DRESS](https://arxiv.org/abs/2311.10081)  
[3] [MiniGPT-5](https://arxiv.org/abs/2310.02239)  
[4] [LLaVA 1.5](https://arxiv.org/abs/2310.03744)  
[5] [NExT-GPT](https://arxiv.org/abs/2309.05519)  
[6] [Video-LLaMA](https://arxiv.org/abs/2306.02858)  
[7] [Shikra](https://arxiv.org/abs/2306.15195)  
[8] [InstructBLIP](https://arxiv.org/abs/2305.06500)  
[9] [PandaGPT](https://arxiv.org/abs/2305.16355)  
[10] [PaLI-X](https://arxiv.org/abs/2305.18565)  
[11] [LLaVA](https://arxiv.org/abs/2304.08485)  
[12] [MiniGPT-4](https://arxiv.org/abs/2304.10592)    
[13] [BLIP-2](https://arxiv.org/abs/2301.12597)  
[14] [Flamingo](https://arxiv.org/abs/2204.14198)  

## 주요 벤치마크
[1] [LLaVA-Bench](https://arxiv.org/abs/2310.03744)  
[2] [MM-Vet](https://arxiv.org/abs/2308.02490)  
[3] [Q-Bench](https://arxiv.org/abs/2309.14181)  
[4] [MMBench](https://arxiv.org/abs/2307.06281)  
[5] [SEED-Bench](https://arxiv.org/abs/2307.16125)  
[6] [MME](https://arxiv.org/abs/2306.13394)  
[7] [A-OKVQA](https://arxiv.org/abs/2206.01718)  
[8] [IconQA](https://arxiv.org/abs/2110.13214)  
[9] [VizWiz](https://arxiv.org/abs/1802.08218)  
[10] [VQA v2.0](https://arxiv.org/abs/1612.00837)  