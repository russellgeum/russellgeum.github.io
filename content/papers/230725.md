---
date: 2023-07-25
author: "oppenheimer1223"
title: "[논문] ICML 2023 관심 논문 리스트업"
categories: "논문"
weight: 10
---

## ICML 2023
### Papers
ICML 2023이 열리고 있다.  
Distillation, Quantization, HW-aware Deep Learning 위주로 트래킹 중이다.  

1. [COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models](https://arxiv.org/abs/2305.17235)  
Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan

2. [DIVISION: Memory Efficient Training via Dual Activation Precision](https://openreview.net/forum?id=6Pv8AMSylux)  
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu

3. [Fast Private Kernel Density Estimation via Locality Sensitive Quantization](https://arxiv.org/abs/2307.01877)  
Tal Wagner, Yonatan Naamad, Nina Mishra

4. [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning](https://arxiv.org/abs/2307.08941)  
Tianxin Wei, Zeming Guo, Yifan Chen, Jingrui He

5. [On the Impact of Knowledge Distillation for Model Interpretability](https://arxiv.org/abs/2305.15734)  
Hyeongrok Han, Siwon Kim, Hyun-Soo Choi, Sungroh Yoon

6. [Quantized Distributed Training of Large Models with Convergence Guarantees](https://arxiv.org/abs/2302.02390)  
Ilia Markov, Adrian Vladu, Qi Guo, Dan Alistarh

7. [Random Teachers are Good Teachers](https://arxiv.org/abs/2302.12091)  
Felix Sarnthein, Gregor Bachmann, Sotiris Anagnostidis, Thomas Hofmann

8. [Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks](https://arxiv.org/abs/2305.08842)  
Minyoung Huh, Brian Cheung, Pulkit Agrawal, Phillip Isola

9. [The case for 4-bit precision: k-bit Inference Scaling Laws](https://arxiv.org/abs/2212.09720)  
Tim Dettmers, Luke Zettlemoyer

10. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)  
Yaniv Leviathan, Matan Kalman, Yossi Matias

11. [Scaling Vision Transformers to 22 Billion Parameters](https://arxiv.org/abs/2302.05442)  
Mostafa Dehghani et al

12. [BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models](https://openreview.net/forum?id=HVKmLi1iR4)  
Taebum Kim, Hyoungjoo Kim, Gyeong-In Yu, Byung-Gon Chun

13. [Brainformers: Trading Simplicity for Efficiency](https://arxiv.org/abs/2306.00008)  
Yanqi Zhou et al

14. [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)  
Jonas Geiping, Tom Goldstein

15. [LookupFFN: Making Transformers Compute-lite for CPU inference](https://openreview.net/forum?id=MmYoDC7dH9)  
Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan Sankaralingam, Vikas Singh

16. [Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch](https://arxiv.org/abs/2307.01236)  
Xunyi Zhao, Théotime Le Hellard, Lionel Eyraud, Julia Gusak, Olivier Beaumont

17. [SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge](https://arxiv.org/abs/2302.04852)  
Mahdi Nikdan, Tommaso Pegolotti, Eugenia Iofinova, Eldar Kurtic, Dan Alistarh

18. [Understanding INT4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)  
Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He

19. [Are Large Kernels Better Tearchers than Transformers for ConvNets?](https://arxiv.org/abs/2305.19412)  
Tianjin Huang, Lu Yin, Zhenyu Zhang, Li Shen, Meng Fang, Mykola Pechenizkiy, Zhangyang Wang, Shiwei Liu


### Workshops
1. [Knowledge and Logical Reasoning in the Era of Data-driven Learning](https://klr-icml2023.github.io/)
2. [Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities](https://fl-icml2023.github.io/)
3. [Challenges of Deploying Generative AI](https://deployinggenerativeai.github.io/)
4. [Efficient Systems for Foundation Models](https://es-fomo.com/)
5. [Neural Compression](https://neuralcompression.github.io/workshop23)


### 추가로 살펴보면 좋은 레퍼런스들
[1] [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129)  
[2] [Backprop with Approximate Activations for Memory-efficient Network Training](https://arxiv.org/abs/1901.07988)  
[3] [AC-GC: Lossy Activation Compression with Guaranteed Convergence](https://openreview.net/forum?id=MwFdqFRxIF0)  
[4] [More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity](https://arxiv.org/abs/2207.036200)  
[5] [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)  
[7]] [Dead Pixel Test Using Effective Receptive Field](https://arxiv.org/abs/2108.13576)  
[8] [Demystify Transformers & Convolutions in Modern Image Deep Networks](https://arxiv.org/abs/2211.05781)  
[9] [Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs](https://arxiv.org/abs/2210.04020)  
[10] [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)