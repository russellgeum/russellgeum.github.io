---
date: 2021-03-05
author: "Oppenheimer"
title: "[논문] ViViT, A Video Vision Transformer"
categories: "논문"
weight: 10
---

## Motivation
비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안  
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
    
## Related Works
생략

## Contribution
1. 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안
2. 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론
3. regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌

### 비디오 임베딩
1. ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing
2. 다른 하나는 토큰 차원을 temporal로 확장해서 사용

### 세 가지 구조
- 모델 1
    - Is Space Time Attention is All You Need for Video Understanding? 을 참고
    - CNN과는 반대로 트랜스포머 레이어 수에 맞게 receptive field가 선형적으로 증가
    - 각 트랜스포머는 모든 비디오 프레임의 토큰에 대해서 interaction을 계산 → long range video interaction을 보증
    - 그러나 all-pair interaction 때문에 MHSA가 쿼드라틱하게 연산량이 증가하는 것이 단점
    - 토큰의 수가 입력 프레임에 따라 선형적으로 증가하고, 더 효율적인 비디오 프레임워크 개발에 동기를 부여
    
- 모델 2
    - 두 기능을 하는 트랜스포머가 스택되어 있음
    - 먼저 spatial encoder는 같은 temporal index에서 (즉 프레임 한 장)
    추출한 token 간의 interaction을 계산
    - 각 템포랄 인덱스부터 계산한 프레임 레벨의 표현들은 H로 concatenated
    된다.
    - *hi* ∈ *ℝ*,  *concat*(*hi*) = *H* → *H* ∈ ℝ*nt* × *d*
    - 이 H는 이제 temporal encoder로 포워드 되는데, 이 템포랄 인코더는 Lt 의 트랜스포머 레이어로 구성 → 서로 다른 템포랄 인덱스 간 interaction을 고려
    - 이 모델은 Model 1보다 파라미터 수가 많지만 연산 수가 적다 → 왜? 분리된 두 트랜스포머 네트워크 복잡도가 O((ab) + c) ≤ O(abc) 형태이기 때문
    
- 모델 3
    - 이 모델은 반대로 모델 1과 같은 트랜스포머 레이어를 포함한다. 그러나 모든 token pair에 대해서 MHSA하는 것 과는 달리, 두 개의 부분으로 나눈다.
    - 먼저 같은 템포랄 인덱스에서 나온 representation으로 spatial attention을 하고, 이제 같은 spatail index에서 temporal에 걸쳐 temporal attention을 연산
    - 각 셀프 어텐션 블럭은 spartio-temporal interaction을 하게끔 설계 → 모델 1보다 연산 측면에서 더 효율적
    - spatial-temporal하고 temporal-spatial하고 차이가 있을까? → 논문은 차이가 없다고 말함
    - 이 모델에서는 cls token을 사용하지 않음 왜냐하면 spatial-temporal 입력 토큰을 reshaping할 때 모호성을 피하기 위해
    
- 모델 4
    - 마지막으로 모델2, 모델3과 같은 계산 복잡도를 가진 모델 4
    - factorization 관점은 모델 3의 그것과 유사, 다만 차이는 한 트랜스포머 레이어 내에서 병렬로 spatial heads 와 temporal heads만으로 attentiond을 계산하고 concat
    - 메인 아이디어는 각 쿼리를 위한 K, V가 프레임 마다의 spatial dim에서 연산하는 spatial attention과 temporal에 걸쳐있는 temporal attention을 쓰는 것
    
- Positional Encoding
    - 비디오 모델은 temporal dimension T가 있기 때문에 이것을 고려해서 positional encoding
    - 논문은 단순히 프레임마다 positional embedding을 repeat하였다. n_w * n_h x d → n_t * n_h * n_w x d 형태
    - 따라서 모든 토큰들은 공간적으로 같은 index 속성을 부여 받는다.
    
- Embedding Weights E
    - 비디오 임베딩은 3D tensor를 요구함 만약 프레임이 temporal을 따라 T개라면 [B, T, C, H, W]가 된다.
    - 이때 문제는 [B, 1, C, H, W] 필터를 어떻게 초기화 할까의 문제 → 기존의
    방법은 다음과 같이 해결
    - Pretrained 된 이미지 모델의 2D Conv 필터를 아래와 같이 3D Conv로
    inflate하고 프레임 수 만큼 나누어서 mean을 취하는 것
    - 좀 더 aggregate하게 temporal information을 배울 수 있음
        
## Experiments
1. Filter Inftlation은 비디오 비전에서의 흔한 초기화 방법 → 이보다 논문에서 제안한 방식이 더 좋음
2. Model2, Model4가 연산량이 적고, 파라미터 수도 더 적음. Model 1은 좋지 않음

## Conclusion
생략