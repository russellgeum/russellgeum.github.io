---
date: 2024-01-07
author: "oppenheimer1223"
title: "[논문] Survey: Efficient Large Language Models"
categories: "논문"
weight: 10
---


## 개요
대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.


## Model Compression
### Weight-Only Quantization (PTQ)
- GPTQ: Accurate Quantization for Generative Pre-trained Transformers,  [[Paper](https://openreview.net/forum?id=tcbBPnfwxS)] [[Code](https://github.com/IST-DASLab/gptq)]
    ICLR, 2023
    
- QuIP: 2-Bit Quantization of Large Language Models With Guarantees,  [[Paper](https://arxiv.org/abs/2307.13304)] [[Code](https://github.com/jerry-chee/QuIP)]
    arXiv, 2023
    
- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,  [[Paper](https://arxiv.org/abs/2306.00978)] [[Code](https://github.com/mit-han-lab/llm-awq)]
    arXiv, 2023
    
- OWQ: Lessons Learned from Activation Outliers for Weight Quantization in Large Language Models,  [[Paper](https://arxiv.org/abs/2306.02272)] [[Code](https://github.com/xvyaward/owq)]
    arXiv, 2023
    
- SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,  [[Paper](https://arxiv.org/pdf/2306.03078)] [[Code](https://github.com/Vahe1994/SpQR)]
    arXiv, 2023
    
- FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs,  [[Paper](https://arxiv.org/abs/2308.09723)]
    NeurIPS-ENLSP, 2023
    
- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,  [[Paper](https://openreview.net/forum?id=dXiGWqBoxaD)] [[Code](https://github.com/TimDettmers/bitsandbytes)]
    NeurlPS, 2022
    
- Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning,  [[Paper](https://arxiv.org/abs/2208.11580)] [[Code](https://github.com/IST-DASLab/OBC)]
    NeurIPS, 2022
    

### Weight-Activation Co-Quantization (PTQ)
- Intriguing Properties of Quantization at Scale,  [[Paper](https://arxiv.org/abs/2305.19268)]
    NeurIPS, 2023
    
- ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation,  [[Paper](https://arxiv.org/abs/2303.08302)] [[Code](https://github.com/microsoft/DeepSpeed)]
    arXiv, 2023
    
- ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats,  [[Paper](https://arxiv.org/abs/2307.09782)] [[Code](https://github.com/microsoft/DeepSpeed)]
    NeurIPS-ENLSP, 2023
    
- OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization,  [[Paper](https://arxiv.org/abs/2304.07493)] [[Code](https://github.com/clevercool/ANT-Quantization)]
    ISCA, 2023
    
- RPTQ: Reorder-based Post-training Quantization for Large Language Models,  [[Paper](https://arxiv.org/abs/2304.01089)] [[Code](https://github.com/hahnyuan/RPTQ4LLM)]
    arXiv, 2023
    
- Outlier Suppression+: Accurate Quantization of Large Language Models by Equivalent and Optimal Shifting and Scaling,  [[Paper](https://arxiv.org/abs/2304.09145)] [[Code](https://github.com/ModelTC/Outlier_Suppression_Plus)]
    arXiv, 2023
    
- QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models,  [[Paper](https://arxiv.org/abs/2310.08041)]
    arXiv, 2023
    
- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,  [[Paper](https://arxiv.org/abs/2211.10438)] [[Code](https://github.com/mit-han-lab/smoothquant)]
    ICML, 2023
    
- ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers,  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html)]
    NeurIPS, 2022
    

### Quantization-Aware Training (QAT)
- BitNet: Scaling 1-bit Transformers for Large Language Models,  [[Paper](https://arxiv.org/abs/2310.11453)]
    arXiv, 2023
    
- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,  [[Paper](https://arxiv.org/abs/2305.17888)] [[Code](https://github.com/facebookresearch/LLM-QAThttps://github.com/facebookresearch/LLM-QAT)]
    arXiv, 2023
    
- Compression of Generative Pre-trained Language Models via Quantization,  [[Paper](https://aclanthology.org/2022.acl-long.331.pdf)]
    ACL, 2022
    

### Pruning: Structured Pruning
- LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery,  [[Paper](https://arxiv.org/abs/2310.18356)]
    arXiv, 2023
    
- LLM-Pruner: On the Structural Pruning of Large Language Models,  [[Paper](https://arxiv.org/abs/2305.11627)] [[Code](https://github.com/horseee/LLM-Pruner)]
    NeurIPS, 2023
    
- Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning,  [[Paper](https://arxiv.org/abs/2310.06694)] [[Code](https://github.com/princeton-nlp/LLM-Shearing)]
    NeurIPS-ENLSP, 2023
    
- LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning,  [[Paper](https://doi.org/10.48550/arXiv.2305.18403)]
    arXiv, 2023
    

### Pruning: Unstructured Pruning
- SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,  [[Paper](https://arxiv.org/abs/2301.00774)] [[Code](https://github.com/IST-DASLab/sparsegpt)]
    ICML, 2023
    
- A Simple and Effective Pruning Approach for Large Language Models,  [[Paper](https://arxiv.org/abs/2306.11695)] [[Code](https://github.com/locuslab/wanda)]
    arXiv, 2023
    
- One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models,  [[Paper](https://arxiv.org/pdf/2310.09499v1.pdf)]
    arXiv, 2023
    

### Pruning: Low-Rank Approximation
- TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition,  [[Paper](https://doi.org/10.48550/arXiv.2307.00526)]
    arXiv, 2023
    
- LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation,  [[Paper](https://arxiv.org/abs/2306.11222)] [[Code](https://github.com/yxli2123/LoSparse)]
    ICML, 2023
    

### White-Box KD
- Towards the Law of Capacity Gap in Distilling Language Models,  [[Paper](https://arxiv.org/abs/2311.07052)] [[Code](https://github.com/GeneZC/MiniMA)]
    arXiv, 2023
    
- Baby Llama: Knowledge Distillation from an Ensemble of Teachers Trained on a Small Dataset with no Performance Penalty,  [[Paper](https://arxiv.org/abs/2308.02019)]
    arXiv, 2023
    
- Knowledge Distillation of Large Language Models,  [[Paper](https://arxiv.org/abs/2306.08543)] [[Code](https://github.com/microsoft/LMOps/tree/main/minillm)]
    arXiv, 2023
    
- GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models,  [[Paper](https://arxiv.org/abs/2306.13649)]
    arXiv, 2023
    
- Propagating Knowledge Updates to LMs Through Distillation,  [[Paper](https://arxiv.org/abs/2306.09306)] [[Code](https://github.com/shankarp8/knowledge_distillation)]
    arXiv, 2023
    
- Less is More: Task-aware Layer-wise Distillation for Language Model Compression,  [[Paper](https://arxiv.org/pdf/2210.01351.pdf)]
    ICML, 2023
    
- Token-Scaled Logit Distillation for Ternary Weight Generative Language Models,  [[Paper](https://arxiv.org/abs/2308.06744)]
    arXiv, 2023
    

### Black-Box KD
- Zephyr: Direct Distillation of LM Alignment,  [[Paper](https://arxiv.org/abs/2312.09571)]
    arXiv, 2023
    
- Instruction Tuning with GPT-4,  [[Paper](https://arxiv.org/abs/2304.03277)] [[Code](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)]
    arXiv, 2023
    
- Lion: Adversarial Distillation of Closed-Source Large Language Model,  [[Paper](https://arxiv.org/abs/2305.12870)] [[Code](https://github.com/YJiangcm/Lion)]
    arXiv, 2023
    
- Specializing Smaller Language Models towards Multi-Step Reasoning,  [[Paper](https://aclanthology.org/2022.findings-naacl.169.pdf)] [[Code](https://github.com/FranxYao/FlanT5-CoT-Specialization)]
    ICML, 2023
    
- Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,  [[Paper](https://arxiv.org/abs/2305.02301)]
    ACL, 2023
    
- Large Language Models Are Reasoning Teachers,  [[Paper](https://arxiv.org/abs/2212.10071)] [[Code](https://github.com/itsnamgyu/reasoning-teacher)]
    ACL, 2023
    
- SCOTT: Self-Consistent Chain-of-Thought Distillation,  [[Paper](https://arxiv.org/abs/2305.01879)] [[Code](https://github.com/wangpf3/consistent-CoT-distillation)]
    ACL, 2023
    
- Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step,  [[Paper](https://arxiv.org/abs/2306.14050)]
    ACL, 2023
    
- Distilling Reasoning Capabilities into Smaller Language Models,  [[Paper](https://aclanthology.org/2023.findings-acl.441/)] [[Code](https://github.com/kumar-shridhar/Distiiling-LM)]
    ACL, 2023
    
- In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,  [[Paper](https://arxiv.org/abs/2212.10670)]
    arXiv, 2022
    
- Explanations from Large Language Models Make Small Reasoners Better,  [[Paper](https://arxiv.org/abs/2210.06726)]
    arXiv, 2022
    
- DISCO: Distilling Counterfactuals with Large Language Models,  [[Paper](https://arxiv.org/abs/2212.10534)] [[Code](https://github.com/eric11eca/disco)]
    arXiv, 2022
    

## Efficient Pre-Training
### Mixed Precision Acceleration
- GACT: Activation Compressed Training for Generic Network Architectures,  [[Paper](https://proceedings.mlr.press/v162/liu22v/liu22v.pdf)] [[Code](https://github.com/LiuXiaoxuanPKU/GACT-ICML)]
    ICML, 2022
    
- Mesa: A Memory-saving Training Framework for Transformers,  [[Paper](https://arxiv.org/abs/2111.11124)] [[Code](https://github.com/ziplab/Mesa)]
    arXiv, 2021
    
- Bfloat16 Processing for Neural Networks,  [[Paper](https://ieeexplore.ieee.org/document/8877390)]
    ARITH, 2019
    
- A Study of BFLOAT16 for Deep Learning Training,  [[Paper](https://arxiv.org/abs/1905.12322)]
    arXiv, 2019
    
- Mixed Precision Training,  [[Paper](https://openreview.net/forum?id=r1gs9JgRZ)]
    ICLR, 2018
    

### Scaling Models
- Learning to Grow Pretrained Models for Efficient Transformer Training,  [[Paper](https://openreview.net/pdf?id=cDYRS5iZ16f)] [[Code](https://github.com/VITA-Group/LiGO)]
    ICLR, 2023
    
- 2x Faster Language Model Pre-training via Masked Structural Growth,  [[Paper](https://arxiv.org/abs/2305.02869)]
    arXiv, 2023
    
- Reusing Pretrained Models by Multi-linear Operators for Efficient Training,  [[Paper](https://openreview.net/pdf?id=RgNXKIrWyU)]
    NeurIPS, 2023
    
- FLM-101B: An Open LLM and How to Train It with $100 K Budget,  [[Paper](https://arxiv.org/pdf/2309.03852.pdf)] [[Code](https://huggingface.co/CofeAI/FLM-101B)]
    arXiv, 2023
    
- Knowledge Inheritance for Pre-trained Language Models,  [[Paper](https://aclanthology.org/2022.naacl-main.288/)] [[Code](https://github.com/thunlp/Knowledge-Inheritance)]
    NAACL, 2022
    
- Staged Training for Transformer Language Models,  [[Paper](https://proceedings.mlr.press/v162/shen22f/shen22f.pdf)] [[Code](https://github.com/allenai/staged-training)]
    ICML, 2022
    

### Initialization Techniques
- Deepnet: Scaling transformers to 1,000 layers,  [[Paper](https://arxiv.org/abs/2203.00555)] [[Code](https://github.com/microsoft/torchscale)]
    arXiv, 2022
    
- ZerO Initialization: Initializing Neural Networks with only Zeros and Ones,  [[Paper](https://openreview.net/pdf?id=1AxQpKmiTc)] [[Code](https://github.com/jiaweizzhao/ZerO-initialization)]
    TMLR, 2022
    
- Rezero is All You Need: Fast Convergence at Large Depth,  [[Paper](https://proceedings.mlr.press/v161/bachlechner21a/bachlechner21a.pdf)] [[Code](https://github.com/majumderb/rezero)]
    UAI, 2021
    
- Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks,  [[Paper](https://papers.neurips.cc/paper/2020/file/e6b738eca0e6792ba8a9cbcba6c1881d-Paper.pdf)]
    NeurIPS, 2020
    
- Improving Transformer Optimization Through Better Initialization,  [[Paper](https://proceedings.mlr.press/v119/huang20f/huang20f.pdf)] [[Code](https://github.com/layer6ai-labs/T-Fixup)]
    ICML, 2020
    
- Fixup Initialization: Residual Learning without Normalization,  [[Paper](https://openreview.net/pdf?id=H1gsz30cKX)]
    ICLR, 2019
    
- On Weight Initialization in Deep Neural Networks,  [[Paper](https://arxiv.org/abs/1704.08863)]
    arXiv, 2017
    

### Optimization Strategies
- Symbolic Discovery of Optimization Algorithms,  [[Paper](https://arxiv.org/abs/2302.06675)]
    arXiv, 2023
    
- Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training,  [[Paper](https://arxiv.org/abs/2305.14342)] [[Code](https://github.com/Liuhong99/Sophia)]
    arXiv, 2023
    

## Efficient Fine-Tuning
### PEFT: Adapter-based Tuning
- OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models,  [[Paper](https://aclanthology.org/2023.acl-demo.26/)] [[Code](https://github.com/thunlp/OpenDelta)]
    ACL Demo, 2023
    
- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,  [[Paper](https://arxiv.org/pdf/2304.01933.pdf)] [[Code](https://github.com/AGI-Edgerunners/LLM-Adapters)]
    EMNLP, 2023
    
- Compacter: Efficient Low-Rank Hypercomplex Adapter Layers,  [[Paper](https://openreview.net/forum?id=bqGK5PyI6-N)] [[Code](https://github.com/rabeehk/compacter)]
    NeurIPS, 2023
    
- Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning,  [[Paper](https://openreview.net/forum?id=rBCvMG-JsPd)] [[Code](https://github.com/r-three/t-few)]
    NeurIPS, 2022
    
- Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning,  [[Paper](https://openreview.net/forum?id=BCGNf-prLg5)]
    AutoML, 2022
    
- AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning,  [[Paper](https://aclanthology.org/2022.emnlp-main.388/)] [[Code](https://github.com/microsoft/AdaMix)]
    EMNLP, 2022
    
- SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters,  [[Paper](https://aclanthology.org/2022.findings-emnlp.160/)] [[Code](https://github.com/Shwai-He/SparseAdapter)]
    EMNLP, 2022
    

### PEFT: Low-Rank Adaptation
- LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning,  [[Paper](https://arxiv.org/abs/2308.03303)]
    arXiv, 2023
    
- LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition,  [[Paper](https://arxiv.org/abs/2307.13269)] [[Code](https://github.com/sail-sg/lorahub)]
    arXiv, 2023
    
- LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,  [[Paper](https://arxiv.org/abs/2309.12307)] [[Code](https://github.com/dvlab-research/LongLoRA)]
    arXiv, 2023
    
- Multi-Head Adapter Routing for Cross-Task Generalization,  [[Paper](https://arxiv.org/abs/2211.03831)] [[Code](https://github.com/microsoft/mttl)]
    NeurIPS, 2023
    
- Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning,  [[Paper](https://arxiv.org/pdf/2303.10512)]
    ICLR, 2023
    
- DyLoRA: Parameter-Efficient Tuning of Pretrained Models using Dynamic Search-Free Low Rank Adaptation,  [[Paper](https://aclanthology.org/2023.eacl-main.239/)] [[Code](https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA)]
    EACL, 2023
    
- Tied-Lora: Enhacing Parameter Efficiency of LoRA with Weight Tying,  [[Paper](https://arxiv.org/abs/2311.09578)]
    arXiv, 2023
    
- LoRA: Low-Rank Adaptation of Large Language Models,  [[Paper](https://openreview.net/forum?id=nZeVKeeFYf9)] [[Code](https://github.com/microsoft/LoRA)]
    ICLR, 2022
    

### Prefix Tuning
- LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,  [[Paper](https://doi.org/10.48550/arXiv.2303.16199)] [[Code](https://github.com/ZrrSkywalker/LLaMA-Adapter)]
    arXiv, 2023
    
- Prefix-Tuning: Optimizing Continuous Prompts for Generation  [[Paper](https://aclanthology.org/2021.acl-long.353/)] [[Code](https://github.com/XiangLi1999/PrefixTuning)]
    ACL, 2021
    

### Prompt Tuning
- Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt,  [[Paper](https://arxiv.org/abs/2305.11186v2)]
    arXiv, 2023
    
- GPT Understands, Too,  [[Paper](https://doi.org/10.1016/j.aiopen.2023.08.012)] [[Code](https://github.com/THUDM/P-tuning)]
    AI Open, 2023
    
- Multi-Task Pre-Training of Modular Prompt for Few-Shot Learning  [[Paper](https://arxiv.org/abs/2210.07565)] [[Code](https://github.com/Hzfinfdu/MPMP)]
    ACL, 2023
    
- Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,  [[Paper](https://arxiv.org/abs/2303.02861)]
    ICLR, 2023
    
- PPT: Pre-trained Prompt Tuning for Few-shot Learning,  [[Paper](https://arxiv.org/abs/2109.04332)] [[Code](https://github.com/thu-coai/PPT)]
    ACL, 2022
    
- Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers,  [[Paper](https://arxiv.org/abs/2207.07087)] [[Code](https://github.com/THUDM/P-tuning-v2/tree/main/PT-Retrieval)]
    EMNLP-Findings, 2022
    
- P-Tuning v2: Prompt Tuning Can Be Comparable to Finetuning Universally Across Scales and Tasks， [[Paper](https://aclanthology.org/2022.acl-short.8/)] [[Code](https://github.com/THUDM/P-tuning-v2)]
    ACL-Short, 2022
    
- The Power of Scale for Parameter-Efficient Prompt Tuning,  [[Paper](https://arxiv.org/abs/2104.08691)]
    EMNLP, 2021
    

### Memory-Efficient Fine-Tuning
- Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,  [[Paper](https://arxiv.org/abs/2305.15265)] [[Code](https://github.com/zirui-ray-liu/WTACRS)]
    NeurIPS, 2023
    
- Memory-Efficient Selective Fine-Tuning,  [[Paper](https://openreview.net/forum?id=zaNbLceVwm)]
    ICML Workshop, 2023
    
- Full Parameter Fine-tuning for Large Language Models with Limited Resources,  [[Paper](https://arxiv.org/abs/2306.09782)] [[Code](https://github.com/OpenLMLab/LOMO)]
    arXiv, 2023
    
- Fine-Tuning Language Models with Just Forward Passes,  [[Paper](https://arxiv.org/abs/2305.17333)] [[Code](https://github.com/princeton-nlp/MeZO)]
    NeurIPS, 2023
    
- Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization,  [[Paper](https://arxiv.org/abs/2305.14152)]
    NeurIPS, 2023
    
- LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models,  [[Paper](https://arxiv.org/abs/2310.08659)] [[Code](https://github.com/yxli2123/LoftQ)]
    arXiv, 2023
    
- QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models,  [[Paper](https://arxiv.org/abs/2309.14717)] [[Code](https://github.com/yuhuixu1993/qa-lora)]
    arXiv, 2023
    
- QLoRA: Efficient Finetuning of Quantized LLMs,  [[Paper](https://arxiv.org/abs/2305.14314)] [[Code1](https://github.com/artidoro/qlora)] [[Code2](https://github.com/TimDettmers/bitsandbytes)]
    NeurIPS, 2023
    

## Efficient Inference
### Speculative Decoding
- PaSS: Parallel Speculative Sampling,  [[Paper](https://arxiv.org/abs/2311.13581)]
    NeurIPS Workshop, 2023
    
- Accelerating Transformer Inference for Translation via Parallel Decoding,  [[Paper](https://aclanthology.org/2023.acl-long.689/)] [[Code](https://github.com/hao-ai-lab/LookaheadDecoding)]
    ACL, 2023
    
- Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads,  [[Blog](https://sites.google.com/view/medusa-llm)] [[Code](https://github.com/FasterDecoding/Medusa)]
    Blog, 2023
    
- Fast Inference from Transformers via Speculative Decoding,  [[Paper](https://arxiv.org/abs/2211.17192)]
    ICML, 2023
    
- Accelerating LLM Inference with Staged Speculative Decoding,  [[Paper](https://arxiv.org/abs/2308.04623)]
    ICML Workshop, 2023
    
- Accelerating Large Language Model Decoding with Speculative Sampling,  [[Paper](https://arxiv.org/abs/2302.01318)]
    arXiv, 2023
    
- Speculative Decoding with Big Little Decoder,  [[Paper](https://arxiv.org/abs/2302.07863)] [[Code](https://github.com/kssteven418/BigLittleDecoder)]
    NeurIPS, 2023
    
- SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification,  [[Paper](https://doi.org/10.48550/arXiv.2305.09781)] [[Code](https://github.com/flexflow/FlexFlow)]
    arXiv, 2023
    
- Inference with Reference: Lossless Acceleration of Large Language Models,  [[Paper](https://arxiv.org/abs/2304.04487)] [[Code](https://github.com/microsoft/LMOps/tree/main/llma)]
    arXiv, 2023
    

### KV-Cache Optimization
- Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs,  [[Paper](https://paperswithcode.com/paper/model-tells-you-what-to-discard-adaptive-kv)]
    arXiv, 2023
    
- SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference,  [[Paper](https://arxiv.org/abs/2307.02628)]
    arXiv, 2023
    
- H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models,  [[Paper](https://arxiv.org/abs/2306.14048)]
    NeurIPS, 2023
    
- Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time,  [[Paper](https://arxiv.org/abs/2305.17118)]
    NeurIPS, 2023
    
- Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,  [[Paper](https://arxiv.org/abs/2305.15805)]
    arXiv, 2023
    

## Efficient Architecture
### Sharing-based Attention
- GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,  [[Paper](https://arxiv.org/abs/2305.13245)]
    EMNLP, 2023
    
- Fast Transformer Decoding: One Write-Head is All You Need,  [[Paper](https://arxiv.org/abs/1911.02150)]
    arXiv, 2019
    

### Feature Information Reduction
- Nyströmformer: A nyström-based algorithm for approximating self-attention,  [[Paper](https://arxiv.org/abs/2102.03902)] [[Code](https://github.com/mlpen/Nystromformer)]
    AAAI, 2021
    
- Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing,  [[Paper](https://arxiv.org/abs/2006.03236)] [[Code](https://github.com/laiguokun/Funnel-Transformer)]
    NeurIPS, 2020
    
- Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,  [[Paper](https://arxiv.org/abs/1810.00825)]
    ICML, 2019
    

### Kernelization or Low-Rank
- Sumformer: Universal Approximation for Efficient Transformers,  [[Paper](https://arxiv.org/abs/2307.02301)]
    ICML Workshop, 2023
    
- FLuRKA: Fast fused Low-Rank & Kernel Attention,  [[Paper](https://arxiv.org/abs/2306.15799)]
    arXiv, 2023
    
- Scatterbrain: Unifying Sparse and Low-rank Attention,  [[Paper](https://openreview.net/forum?id=SehIKudiIo1)] [[Code](https://github.com/HazyResearch/fly)]
    NeurlPS, 2021
    
- Rethinking Attention with Performers,  [[Paper](https://openreview.net/forum?id=Ua6zuk0WRH)] [[Code](https://github.com/lucidrains/performer-pytorch)]
    ICLR, 2021
    
- Random Feature Attention,  [[Paper](https://arxiv.org/abs/2103.02143)]
    ICLR, 2021
    
- Linformer: Self-Attention with Linear Complexity,  [[Paper](https://arxiv.org/abs/2006.04768)] [[Code](https://github.com/lucidrains/linformer)]
    arXiv, 2020
    
- Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer,  [[Paper](https://arxiv.org/abs/1910.13923)]
    ICASSP, 2020
    
- Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,  [[Paper](https://arxiv.org/abs/2006.16236)] [[Code](https://github.com/idiap/fast-transformers)]
    ICML, 2020
    

### Fixed Pattern Strategies
- Faster Causal Attention Over Large Sequences Through Sparse Flash Attention,  [[Paper](https://arxiv.org/abs/2306.01160)]
    ICML Workshop, 2023
    
- Poolingformer: Long Document Modeling with Pooling Attention,  [[Paper](https://arxiv.org/abs/2105.04371)]
    ICML, 2021
    
- Big Bird: Transformers for Longer Sequences,  [[Paper](https://arxiv.org/abs/2007.14062)] [[Code](https://github.com/google-research/bigbird)]
    NeurIPS, 2020
    
- Longformer: The Long-Document Transformer,  [[Paper](https://arxiv.org/abs/2004.05150)] [[Code](https://github.com/allenai/longformer)]
    arXiv, 2020
    
- Blockwise Self-Attention for Long Document Understanding,  [[Paper](https://arxiv.org/abs/1911.02972v)] [[Code](https://github.com/xptree/BlockBERT)]
    EMNLP, 2020
    
- Generating Long Sequences with Sparse Transformers,  [[Paper](https://arxiv.org/abs/1904.10509)]
    arXiv, 2019
    

### Learnable Pattern Strategies
- HyperAttention: Long-context Attention in Near-Linear Time,  [[Paper](https://arxiv.org/abs/2310.05869)] [[Code](https://github.com/insuhan/hyper-attn)]
    arXiv, 2023
    
- ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer,  [[Paper](https://aclanthology.org/2022.acl-long.170/)]
    ACL, 2022
    
- Reformer: The Efficient Transformer,  [[Paper](https://openreview.net/forum?id=rkgNKkHtvB)] [[Code](https://github.com/lucidrains/reformer-pytorch)]
    ICLR, 2022
    
- Sparse Sinkhorn Attention,  [[Paper](https://arxiv.org/abs/2002.11296)]
    ICML, 2020
    
- Fast Transformers with Clustered Attention,  [[Paper](https://arxiv.org/pdf/2007.04825.pdf)] [[Code](https://github.com/idiap/fast-transformers)]
    NeurIPS, 2020
    
- Efficient Content-Based Sparse Attention with Routing Transformers,  [[Paper](https://arxiv.org/abs/2003.05997)] [[Code](https://github.com/google-research/google-research/tree/master/routing_transformer)]
    TACL, 2020
    

## Mixture of Experts
### MoE-based LLMs
- Mistral 7B,  [[Paper](https://arxiv.org/abs/2310.06825)] [[Code](https://github.com/mistralai/mistral-src)]
    arXiv, 2023
    
- PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,  [[Paper](https://arxiv.org/abs/2303.10845)]
    arXiv, 2023
    
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,  [[Paper](https://jmlr.org/papers/volume23/21-0998/21-0998.pdf)] [[Code](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)]
    JMLR, 2022
    
- Efficient Large Scale Language Modeling with Mixtures of Experts,  [[Paper](https://arxiv.org/abs/2112.10684)] [[Code](https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm)]
    EMNLP, 2022
    
- BASE Layers: Simplifying Training of Large, Sparse Models,  [[Paper](https://arxiv.org/abs/2103.16716)] [[Code](https://github.com/pytorch/fairseq/)]
    ICML, 2021
    
- GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,  [[Paper](https://arxiv.org/abs/2006.16668)]
    ICLR, 2021
    

### Algorithm-Level MoE Optimization
- Lifelong Language Pretraining with Distribution-Specialized Experts,  [[Paper](https://arxiv.org/abs/2305.12281)]
    ICML, 2023
    
- Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models,  [[Paper](https://arxiv.org/abs/2305.14705)]
    arXiv, 2023
    
- Mixture-of-Experts with Expert Choice Routing,  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf)]
    NeurIPS, 2022
    
- StableMoE: Stable Routing Strategy for Mixture of Experts,  [[Paper](https://arxiv.org/pdf/2204.08396.pdf)] [[Code](https://github.com/Hunter-DDM/stablemoe)]
    ACL, 2022
    
- On the Representation Collapse of Sparse Mixture of Experts,  [[Paper](https://arxiv.org/abs/2204.09179)]
    NeurIPS, 2022
    

### Long Context LLMs: Extrapolation and Interpolation
- Scaling Laws of RoPE-based Extrapolation,  [[Paper](https://arxiv.org/abs/2310.05209)]
    arXiv, 2023
    
- A Length-Extrapolatable Transformer,  [[Paper](https://aclanthology.org/2023.acl-long.816/)] [[Code](https://aka.ms/LeX-Transformer)]
    ACL, 2023
    
- Extending Context Window of Large Language Models via Positional Interpolation,  [[Paper](https://arxiv.org/abs/2306.15595)]
    arXiv, 2023
    
- NTK Interpolation,  [[Reddit post](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)]
    Blog, 2023
    
- YaRN: Efficient Context Window Extension of Large Language Models,  [[Paper](https://arxiv.org/abs/2309.00071)] [[Code](https://github.com/jquesnelle/yarn)]
    arXiv, 2023
    
- CLEX: Continuous Length Extrapolation for Large Language Models,  [[Paper](https://arxiv.org/abs/2310.16450)][[Code](https://github.com/DAMO-NLP-SG/CLEX)]
    arXiv, 2023
    
- PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,  [[Paper](https://paperswithcode.com/paper/pose-efficient-context-window-extension-of)][[Code](https://github.com/dwzhu-pku/pose)]
    arXiv, 2023
    
- Functional Interpolation for Relative Positions Improves Long Context Transformers,  [[Paper](https://arxiv.org/pdf/2310.04418.pdf)]
    arXiv, 2023
    
- Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation,  [[Paper](https://arxiv.org/pdf/2108.12409.pdf)] [[Code](https://github.com/ofirpress/attention_with_linear_biases)]
    ICLR, 2022
    
- Exploring Length Generalization in Large Language Models,  [[Paper](https://arxiv.org/abs/2207.04901)]
    NeurIPS, 2022
    
- The EOS Decision and Length Extrapolation,  [[Paper](https://arxiv.org/abs/2010.07174)] [[Code](https://github.com/bnewm0609/eos-decision)]
    EMNLP, 2020
    

### Long Context LLMs: Recurrent Structure
- Retentive Network: A Successor to Transformer for Large Language Models,  [[Paper](https://arxiv.org/abs/2307.08621)] [[Code](https://github.com/microsoft/torchscale)]
    arXiv, 2023
    
- Recurrent Memory Transformer,  [[Paper](https://arxiv.org/abs/2207.06881)] [[Code](https://github.com/booydar/LM-RMT)]
    NeurIPS, 2022
    
- Block-Recurrent Transformers,  [[Paper](https://arxiv.org/abs/2203.07852)] [[Code](https://github.com/google-research/meliad)]
    NeurIPS, 2022
    
- ∞-former: Infinite Memory Transformer,  [[Paper](https://arxiv.org/abs/2109.00301)] [[Code](https://github.com/deep-spin/infinite-former)]
    ACL, 2022
    
- Memformer: A Memory-Augmented Transformer for Sequence Modeling,  [[Paper]](https://arxiv.org/abs/2010.06891) [[Code](https://github.com/deep-spin/infinite-former)]
    AACL-Findings, 2020
    
- Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,  [[Paper](https://arxiv.org/abs/1901.02860)] [[Code](https://github.com/kimiyoung/transformer-xl)]
    ACL, 2019
    

### Long Context LLMs: Segmentation and Sliding Window
- LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning, [[Paper](https://arxiv.org/abs/2305.15265)]
    arXiv, 2024
    
- Extending Context Window of Large Language Models via Semantic Compression,  [[Paper](https://arxiv.org/abs/2312.09571)]
    arXiv, 2023
    
- Efficient Streaming Language Models with Attention Sinks,  [[Paper](https://arxiv.org/abs/2309.17453)] [[Code](https://github.com/mit-han-lab/streaming-llm)]
    arXiv, 2023
    
- Parallel Context Windows for Large Language Models,  [[Paper](https://arxiv.org/abs/2212.10947)] [[Code](https://github.com/ai21labs/parallel-context-windows)]
    ACL, 2023
    
- LongNet: Scaling Transformers to 1,000,000,000 Tokens,  [[Paper](https://arxiv.org/abs/2307.02486)] [[Code](https://github.com/microsoft/unilm/tree/master)]
    arXiv, 2023
    
- Efficient Long-Text Understanding with Short-Text Models,  [[Paper](https://arxiv.org/abs/2208.00748)] [[Code](https://github.com/Mivg/SLED)]
    TACL, 2023
    

### Memory-Retrieval Augmentation
- Landmark Attention: Random-Access Infinite Context Length for Transformers,  [[Paper](https://arxiv.org/abs/2305.16300)] [[Code](https://github.com/epfml/landmark-attention/)]
    arXiv, 2023
    
- Augmenting Language Models with Long-Term Memory,  [[Paper](https://arxiv.org/abs/2306.07174)]
    NeurIPS, 2023
    
- Unlimiformer: Long-Range Transformers with Unlimited Length Input,  [[Paper](https://arxiv.org/abs/2305.01625)] [[Code](https://github.com/abertsch72/unlimiformer)]
    NeurIPS, 2023
    
- Focused Transformer: Contrastive Training for Context Scaling,  [[Paper](https://arxiv.org/abs/2307.03170)] [[Code](https://github.com/CStanKonrad/long_llama)]
    NeurIPS, 2023
    
- Retrieval meets Long Context Large Language Models,  [[Paper](https://arxiv.org/abs/2310.03025)]
    arXiv, 2023
    
- Memorizing Transformers,  [[Paper](https://arxiv.org/abs/2203.08913)] [[Code](https://github.com/lucidrains/memorizing-transformers-pytorch)]
    ICLR, 2022
    

## Transformer Alternative Architecture
### State Space Models
- Sparse Modular Activation for Efficient Sequence Modeling,  [[Paper](https://arxiv.org/abs/2306.11197)] [[Code](https://github.com/renll/SeqBoat)]
    NeurIPS, 2023
    
- Mamba: Linear-Time Sequence Modeling with Selective State Spaces,  [[Paper](https://arxiv.org/abs/2312.00752)] [[Code](https://github.com/state-spaces/mamba)]
    arXiv, 2023
    
- Hungry Hungry Hippos: Towards Language Modeling with State Space Models,  [[Paper](https://arxiv.org/abs/2212.14052)] [[Code](https://github.com/HazyResearch/H3)]
    ICLR 2023
    
- Long Range Language Modeling via Gated State Spaces,  [[Paper](https://arxiv.org/abs/2206.13947)]
    ICLR, 2023
    
- Block-State Transformers,  [[Paper](https://arxiv.org/abs/2306.09539)]
    NeurIPS, 2023
    
- Efficiently Modeling Long Sequences with Structured State Spaces,  [[Paper](https://arxiv.org/abs/2111.00396)] [[Code](https://github.com/state-spaces/s4)]
    ICLR, 2022
    
- Diagonal State Spaces are as Effective as Structured State Spaces,  [[Paper](https://arxiv.org/abs/2203.14343)] [[Code](https://github.com/ag1988/dss)]
    NeurIPS, 2022
    

### Other Sequential Models
- PanGu-π: Enhancing Language Model Architectures via Nonlinearity Compensation,  [[Paper](https://arxiv.org/abs/2312.17276)]
    arXiv, 2023
    
- RWKV: Reinventing RNNs for the Transformer Era,  [[Paper](https://arxiv.org/abs/2305.13048)]
    EMNLP-Findings, 2023
    
- Hyena Hierarchy: Towards Larger Convolutional Language Models,  [[Paper](https://arxiv.org/abs/2302.10866)]
    arXiv, 2023
    
- MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,  [[Paper](https://arxiv.org/pdf/2305.07185.pdf)]
    arXiv, 2023
    

## Data-Centric Methods
### Data Selection for Efficient Pre-Training
- Data Selection for Language Models via Importance Resampling,  [[Paper](https://arxiv.org/abs/2302.03169)] [[Code](https://github.com/p-lambda/dsir)]
    NeurIPS, 2023
    
- NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework,  [[Paper](https://arxiv.org/pdf/2111.04130.pdf)] [[Code](https://github.com/yaoxingcheng/TLM)]
    ICML, 2022
    
- Span Selection Pre-training for Question Answering,  [[Paper](https://arxiv.org/abs/1909.04120)] [[Code](https://github.com/IBM/span-selection-pretraining)]
    ACL, 2020


### Data Selection for Efficient Fine-Tuning
- What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning,  [[Paper](https://arxiv.org/abs/2312.15685)] [[Code](https://github.com/hkust-nlp/deita)]
    arXiv, 2023
    
- One Shot Learning as Instruction Data Prospector for Large Language Models,  [[Paper](https://arxiv.org/abs/2312.10302)]
    arXiv, 2023
    
- MoDS: Model-oriented Data Selection for Instruction Tuning,  [[Paper](https://arxiv.org/abs/2311.15653)] [[Code](https://github.com/CASIA-LM/MoDS)]
    arXiv, 2023
    
- Instruction Mining: When Data Mining Meets Large Language Model Finetuning,  [[Paper](https://arxiv.org/abs/2307.06290)]
    arXiv, 2023
    
- Data-Efficient Finetuning Using Cross-Task Nearest Neighbors,  [[Paper](https://aclanthology.org/2023.findings-acl.576.pdf)] [[Code](https://github.com/allenai/data-efficient-finetuning)]
    ACL, 2023
    
- Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values,  [[Paper](https://arxiv.org/abs/2306.10165)] [[Code](https://github.com/stephanieschoch/ts-dshapley)]
    ACL SRW, 2023
    
- Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,  [[Paper](https://arxiv.org/abs/2305.09246)]
    arXiv, 2023
    
- AlpaGasus: Training A Better Alpaca with Fewer Data,  [[Paper](https://arxiv.org/abs/2307.08701)] [[Code](https://github.com/Lichang-Chen/AlpaGasus)]
    arXiv, 2023
    
- LIMA: Less Is More for Alignment,  [[Paper](https://arxiv.org/abs/2305.11206)]
    arXiv, 2023
    

## Prompt Engineering
### Demonstration Selection
- Unified Demonstration Retriever for In-Context Learning,  [[Paper](https://arxiv.org/abs/2305.04320)] [[Code](https://arxiv.org/abs/2305.04320)]
    ACL, 2023
    
- Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning,  [[Paper](https://arxiv.org/abs/2301.11916)] [[Code](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)]
    NeurIPS, 2023
    
- In-Context Learning with Iterative Demonstration Selection,  [[Paper](https://arxiv.org/abs/2310.09881)]
    arXiv, 2022
    
- Dr.ICL: Demonstration-Retrieved In-context Learning,  [[Paper](https://arxiv.org/abs/2305.14128)]
    arXiv, 2022
    
- Learning to Retrieve In-Context Examples for Large Language Models,  [[Paper](https://arxiv.org/abs/2307.07164)]
    arXiv, 2022
    
- Finding Supporting Examples for In-Context Learning,  [[Paper](https://arxiv.org/abs/2302.13539)]
    arXiv, 2022
    
- Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering,  [[Paper](https://aclanthology.org/2023.acl-long.79.pdf)] [[Code](https://github.com/Shark-NLP/self-adaptive-ICL)]
    ACL, 2023
    
- Selective Annotation Makes Language Models Better Few-Shot Learners,  [[Paper](https://arxiv.org/abs/2209.01975)] [[Code](https://github.com/xlang-ai/icl-selective-annotation)]
    ICLR, 2023
    
- What Makes Good In-Context Examples for GPT-3?  [[Paper](https://arxiv.org/abs/2101.06804)]
    DeeLIO, 2022
    
- Learning To Retrieve Prompts for In-Context Learning,  [[Paper](https://arxiv.org/abs/2112.08633)] [[Code](https://github.com/OhadRubin/EPR)]
    NAACL-HLT, 2022
    
- Active Example Selection for In-Context Learning,  [[Paper](https://aclanthology.org/2022.emnlp-main.622/)] [[Code](https://github.com/chicagohai/active-example-selection)]
    EMNLP, 2022
    
- Rethinking the Role of Demonstrations: What makes In-context Learning Work?  [[Paper](https://aclanthology.org/2022.emnlp-main.759.pdf)] [[Code](https://github.com/Alrope123/rethinking-demonstrations)]
    EMNLP, 2022
    

### Demonstration Ordering
- Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity,  [[Paper](https://arxiv.org/abs/2104.08786)]
    ACL, 2022
    

### Instruction Generation
- Large Language Models as Optimizers,  [[Paper](https://arxiv.org/abs/2309.03409)]
    arXiv, 2023
    
- Instruction Induction: From Few Examples to Natural Language Task Descriptions,  [[Paper](https://arxiv.org/abs/2205.10782)] [[Code](https://github.com/orhonovich/instruction-induction)]
    ACL, 2023
    
- Large Language Models Are Human-Level Prompt Engineers,  [[Paper](https://arxiv.org/abs/2211.01910)] [[Code](https://github.com/keirp/automatic_prompt_engineer)]
    ICLR, 2023
    
- TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design,  [[Paper](https://arxiv.org/abs/2309.05447)]
    arXiv, 2023
    
- Self-Instruct: Aligning Language Model with Self Generated Instructions,  [[Paper](https://doi.org/10.48550/arXiv.2212.10560)] [[Code](https://github.com/yizhongw/self-instruct)]
    ACL, 2023
    

### Multi-Step Reasoning
- Automatic Chain of Thought Prompting in Large Language Models,  [[Paper](https://arxiv.org/abs/2210.03493)] [[Code](https://github.com/amazon-science/auto-cot)]
    ICLR, 2023
    
- Measuring and Narrowing the Compositionality Gap in Language Models,  [[Paper](https://arxiv.org/abs/2210.03350)] [[Code](https://github.com/ofirpress/self-ask)]
    EMNLP, 2023
    
- ReAct: Synergizing Reasoning and Acting in Language Models,  [[Paper](https://arxiv.org/abs/2210.03629)] [[Code](https://github.com/ysymyth/ReAct)]
    ICLR, 2023
    
- Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,  [[Paper](https://arxiv.org/abs/2205.10625)]
    ICLR, 2023
    
- Graph of Thoughts: Solving Elaborate Problems with Large Language Models,  [[Paper](https://arxiv.org/abs/2308.09687)] [[Code](https://github.com/spcl/graph-of-thoughts)]
    arXiv, 2023
    
- Tree of Thoughts: Deliberate Problem Solving with Large Language Models,  [[Paper](https://arxiv.org/abs/2305.10601)] [[Code](https://github.com/princeton-nlp/tree-of-thought-llm)]
    NeurIPS, 2023
    
- Self-Consistency Improves Chain of Thought Reasoning in Language Models,  [[Paper](https://arxiv.org/abs/2203.11171)]
    ICLR, 2023
    
- Graph of Thoughts: Solving Elaborate Problems with Large Language Models,  [[Paper](https://arxiv.org/abs/2308.09687)] [[Code](https://github.com/spcl/graph-of-thoughts)]
    arXiv, 2023
    
- Contrastive Chain-of-Thought Prompting,  [[Paper](https://arxiv.org/pdf/2311.09277.pdf)] [[Code](https://github.com/DAMO-NLP-SG/contrastive-cot)]
    arXiv, 2023
    
- Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation,  [[Paper](https://arxiv.org/abs/2311.04254)]
    arXiv, 2023
    
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)]
    NeurIPS, 2022
    

### Parallel Generation
- Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding,  [[Paper](https://arxiv.org/abs/2307.15337)] [[Code](https://github.com/imagination-research/sot)]
    arXiv, 2023
    

### Prompt Compression
- Learning to Compress Prompts with Gist Tokens,  [[Paper](https://arxiv.org/abs/2304.08467)]
    arXiv, 2023
    
- Adapting Language Models to Compress Contexts,  [[Paper](https://arxiv.org/abs/2305.14788)] [[Code](https://github.com/princeton-nlp/AutoCompressors)]
    EMNLP, 2023
    
- In-context Autoencoder for Context Compression in a Large Language Model,  [[Paper](https://arxiv.org/abs/2307.06945)] [[Code](https://github.com/getao/icae)]
    arXiv, 2023
    
- LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression,  [[Paper](https://arxiv.org/abs/2310.06839)] [[Code](https://github.com/microsoft/LLMLingua)]
    arXiv, 2023
    
- Discrete Prompt Compression with Reinforcement Learning,  [[Paper](https://arxiv.org/abs/2308.08758)]
    arXiv, 2023
    
- Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models,  [[Paper](https://arxiv.org/abs/2310.02409)]
    arXiv, 2023
    

### Prompt Generation
- TempLM: Distilling Language Models into Template-Based Generators,  [[Paper](https://arxiv.org/abs/2205.11055)] [[Code](https://github.com/Tiiiger/templm)]
    arXiv, 2022
    
- PromptGen: Automatically Generate Prompts using Generative Models,  [[Paper](https://aclanthology.org/2022.findings-naacl.3/)]
    NAACL Findings, 2022
    
- AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,  [[Paper](https://aclanthology.org/2020.emnlp-main.346.pdf)] [[Code](https://ucinlp.github.io/autoprompt/)]
    EMNLP, 2020
    

## System-Level Efficiency Optimization and LLM Frameworks
### System-Level Pre-Training Efficiency Optimization
- CoLLiE: Collaborative Training of Large Language Models in an Efficient Way,  [[Paper](https://arxiv.org/abs/2312.00407)] [[Code](https://github.com/OpenLMLab/collie)]
    EMNLP, 2023
    
- An Efficient 2D Method for Training Super-Large Deep Learning Models,  [[Paper](https://ieeexplore.ieee.org/document/10177476)] [[Code](https://github.com/xuqifan897/Optimus)]
    IPDPS, 2023
    
- PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel,  [[Paper](https://dl.acm.org/doi/10.14778/3611540.3611569)]
    VLDB, 2023
    
- Bamboo: Making Preemptible Instances Resilient for Affordable Training,  [[Paper](https://www.usenix.org/system/files/nsdi23-thorpe.pdf)] [[Code](https://github.com/uclasystem/bamboo)]
    NSDI, 2023
    
- Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3600006.3613152)] [[Code](https://github.com/SymbioticLab/Oobleck)]
    SOSP, 2023
    
- Varuna: Scalable, Low-cost Training of Massive Deep Learning Models,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3492321.3519584)] [[Code](https://github.com/microsoft/varuna)]
    EuroSys, 2022
    
- Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization,  [[Paper](https://www.usenix.org/conference/osdi22/presentation/unger)] [[Code](https://github.com/flexflow/flexflow)]
    OSDI, 2022
    
- Tesseract: Parallelize the Tensor Parallelism Efficiently, , [[Paper](https://dl.acm.org/doi/10.1145/3545008.3545087)]
    ICPP, 2022
    
- Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning, , [[Paper](https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf)][[Code](https://github.com/alpa-projects/alpa)]
    OSDI, 2022
    
- Maximizing Parallelism in Distributed Training for Huge Neural Networks,  [[Paper](https://arxiv.org/abs/2105.14450)]
    arXiv, 2021
    
- Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,  [[Paper](https://dl.acm.org/doi/10.1145/3458817.3476209)] [[Code](https://github.com/nvidia/megatron-lm)]
    SC, 2021
    
- ZeRO-Infinity: breaking the GPU memory wall for extreme scale deep learning,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3458817.3476205)]
    SC, 2021
    
- ZeRO-Offload: Democratizing Billion-Scale Model Training,  [[Paper](https://www.usenix.org/system/files/atc21-ren-jie.pdf)] [[Code](https://www.deepspeed.ai/tutorials/zero-offload/)]
    USENIX ATC, 2021
    
- ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,  [[Paper](https://dl.acm.org/doi/10.5555/3433701.3433727)] [[Code](https://github.com/microsoft/DeepSpeed)]
    SC, 2020
    

### System-Level Inference Efficiency Optimization
- LLM in a flash: Efficient Large Language Model Inference with Limited Memory,  [[Paper](https://arxiv.org/abs/2312.11514)]
    arXiv, 2023
    
- SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision,  [[Paper](https://arxiv.org/abs/2209.09130)]
    EMNLP, 2023
    
- FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU,  [[Paper](https://arxiv.org/abs/2303.06865)] [[Code](https://github.com/FMInference/FlexGen)]
    ICML, 2023
    
- Flash-Decoding for Long-Context Inference,  [[Blog](https://pytorch.org/blog/flash-decoding/)]
    Blog, 2023
    
- FlashDecoding++: Faster Large Language Model Inference on GPUs,  [[Paper](https://arxiv.org/abs/2311.01282)]
    arXiv, 2023
    
- Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time,  [[Paper](https://openreview.net/pdf?id=wIPIhHd00i)]
    ICML, 2023
    
- Efficiently Scaling Transformer Inference,  [[Paper](https://proceedings.mlsys.org/paper_files/paper/2023/file/523f87e9d08e6071a3bbd150e6da40fb-Paper-mlsys2023.pdf)]
    MLSys, 2023
    
- S3: Increasing GPU Utilization during Generative Inference for Higher Throughput,  [[Paper](https://arxiv.org/abs/2306.06000)]
    arXiv, 2023
    
- DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale,  [[Paper](https://dl.acm.org/doi/abs/10.5555/3571885.3571946)]
    SC, 2022
    

### System-Level Serving Efficiency Optimization
- PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU,  [[Paper](https://arxiv.org/abs/2312.12456)] [[Code](https://github.com/SJTU-IPADS/PowerInfer)]
    arXiv, 2023
    
- S-LoRA: Serving Thousands of Concurrent LoRA Adapters,  [[Paper](https://arxiv.org/pdf/2311.03285)] [[Code](https://github.com/S-LoRA/S-LoRA)]
    arXiv, 2023
    
- Efficient Memory Management for Large Language Model Serving with PagedAttention,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3600006.3613165)] [[Code](https://github.com/vllm-project/vllm)]
    SOSP, 2023
    
- Orca: A Distributed Serving System for Transformer-Based Generative Models,  [[Paper](https://www.usenix.org/conference/osdi22/presentation/yu)]
    OSDI, 2022
    
- Fast Distributed Inference Serving for Large Language Models,  [[Paper](https://arxiv.org/abs/2305.05920)]
    arXiv, 2023
    
- Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models,  [[Paper](https://arxiv.org/abs/2307.02666)]
    arXiv, 2023
    
- SpotServe: Serving Generative Large Language Models on Preemptible Instances,  [[Paper](https://arxiv.org/abs/2311.15566)]
    arXiv, 2023
    
- TurboTransformers: an efficient GPU serving system for transformer models,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3437801.3441578)]
    PPoPP, 2021
    

## System-Level Efficient Architecture Optimization
### System-Level Attention Optimization
- FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,  [[Paper](https://arxiv.org/abs/2307.08691)] [[Code](https://github.com/Dao-AILab/flash-attention)]
    arXiv, 2023
    
- Efficient Memory Management for Large Language Model Serving with PagedAttention,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3600006.3613165)] [[Code](https://github.com/vllm-project/vllm)]
    SOSP, 2023
    
- FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,  [[Paper](https://arxiv.org/abs/2205.14135)] [[Code](https://github.com/Dao-AILab/flash-attention)]
    NeurIPS, 2022
    
- Accelerated Inference for Large Transformer Models Using NVIDIA Triton Inference Server,  [[Blog](https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/)]
    Nvidia Blog, 2022
    
- ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks,  [[Paper](https://dl.acm.org/doi/10.1109/ISCA52012.2021.00060)]
    ISCA, 2021
    
- A3: Accelerating Attention Mechanisms in Neural Networks with Approximation,  [[Paper](https://arxiv.org/abs/2002.10941)]
    HPCA, 2020
    

### System-Level MoE Optimization
- Tutel: Adaptive mixture-of-experts at scale,  [[Paper](https://arxiv.org/pdf/2206.03382.pdf)] [[Code](https://github.com/microsoft/tutel)]
    MLSys, 2023
    
- MegaBlocks: Efficient Sparse Training with Mixture-of-Experts,  [[Paper](https://arxiv.org/pdf/2211.15841.pdf)] [[Code](https://github.com/stanford-futuredata/megablocks)]
    MLSys, 2023
    
- SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization,  [[Paper](https://www.usenix.org/conference/atc23/presentation/zhai)]
    USENIX ATC, 2023
    
- MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism,  [[Paper](https://ieeexplore.ieee.org/document/10177396)] [[Code](https://github.com/whuzhangzheng/MPipeMoE)]
    IPDPS, 2023
    
- EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models,  [[Paper](https://arxiv.org/abs/2308.14352)]
    arXiv, 2022
    
- TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training,  [[Paper](https://arxiv.org/abs/2302.09915)] [[Code](https://github.com/Chen-Chang/TA-MoE)]
    NeurIPS, 2022
    
- DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,  [[Paper](https://arxiv.org/pdf/2201.05596.pdf)] [[Code](https://github.com/microsoft/DeepSpeed)]
    ICML, 2022
    
- FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models,  [[Paper](https://dl.acm.org/doi/abs/10.1145/3503221.3508418)] [[Code](https://github.com/thu-pacman/FasterMoE)]
    PPoPP, 2022
    
- FastMoE: A Fast Mixture-of-Expert Training System,  [[Paper](https://arxiv.org/abs/2103.13262)] [[Code](https://github.com/laekov/fastmoe)]
    arXiv, 2021