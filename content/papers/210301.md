---
date: 2021-03-01
author: "oppenheimer1223"
title: "[논문] A Simple Framework for Contrastive Learning of Visual Representations"
categories: "논문"
weight: 10
---

## 용어의 정의
1. Pretext task: represenation learning을 위해 수행되는 태스크  
2. Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는
문제를 푸는 것

## Motivation
모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?

## Related Work
Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음  
- Generative  
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음
- Discriminative
    1. supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함
    2. 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함
    3. 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등)

## Contribution
- representation learning에서 data augmentation에 대한 체계적인 고민이 없었음. 효과적인 contrastive learning을 위해, augmentation 방법에 대한 레시피를 제안
- rerpesentation (latent vector) 와 loss function 사이에 non-linear transformation을 끼워서 represenstation의 퀄리티를 높임
- nomalized 임베딩과 템퍼쳐 파라미터로 representation learning loss는 좀 더 나아질 수 있음
- contrastive learning은 더 큰 배치 사이즈와 더 오랫동안의 훈련에서 이점이 있음

## Experiments
- Data augmentation  
저자는 몇 가지 augmentation을 사용하였고, 이미지넷은 크기가 제각각이기 때문에 crop, resize를 항상 적용. single transformation은 좋은 representation을 만들지는 못함 → compose augmentation은 contrastive prediction task가 어렵지만, 좋은 representation을 얻음.  
특히 하나의 composed augmentation가 성능이 좋음 (croppping+ color distortion) cropping만 적용하면 이미지 패치들이 대부분 비슷한 color distribution을 공유하는 문제, 따라서 augmentation을 해도 히스토그램이 비슷해서 이것 만으로도 이미지 구별이 가능할 것. NN은 이 히스토그램만으로 predictive task를 풀 수 있음. 좀 더 의미 있는 feature를 배우기 위해 color distortion을 cropping과 함께 쓰는 것이 좋음
- Non linear projection head both rerpesentation and loss  
non linear projection이 linear projection보다 성능이 좋음 non linear projection이 없는 것보다 성능이 훨씬 좋음
- L1 normalized와 temp 파라미터는 적절한 스케일링이 없으면 좋은 성능을 이끌어낼 수 없음, 따라서 적절한 파라미터를 선택해야함
- 더 많은 트레이닝과 배치 사이즈는 contrastive learning의 효과를 크게 함.  
특히 배치 사이즈가 크면 더 많안 네거티브 샘플을 만들 수 있어서 좋음. SimCLR는 특히 메모리 뱅크와 같은 방식을 사용하지 않음, 샘플링한 데이터에서 각각 agumentation을 적용하고, 같은 샘플은 positive pair, 다른 샘플은 네거티브 샘플로 다룸. 따라서 배치 사이즈가 크면 네거티브 샘플의 경우가 많아서 성능을 이끌어낼 수 있음 특히 트레이닝 에포크가 길수록, 배치 사이즈에 따른 성능 차이는 줄어듬

## Conclusion
생략