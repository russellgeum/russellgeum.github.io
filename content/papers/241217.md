---
date: 2024-12-17
author: "oppenheimer1223"
title: "[논문] Keyword Transformer: A Self-Attention Model for Keyword Spotting"
categories: "논문"
weight: 10
---


## 1. Motivation  
Transformer 구조는 자연어 처리뿐만 아니라 이미지 처리와 음성 인식 등 다양한 도메인에서 성공적으로 사용되고 있다.  
하지만 키워드 스포팅 분야에서는 주로 Transformer가 기존의 CNN이나 RNN 같은 구조 위에 추가적으로 사용되어 왔다.  
이를 해결하기 위해, 이 논문은 키워드 스포팅에 Transformer를 직접 적용하는 모델인 Keyword Transformer(KWT)를 제안한다.  
KWT는 별도의 사전 학습이나 추가 데이터를 필요로 하지 않으면서 기존의 복잡한 혼합 구조보다 뛰어난 성능을 보이며, Google Speech Commands 데이터셋에서 최고 수준의 정확도를 달성했다.  


## 2. Related Work  
### Keyword Spotting  
- 키워드 스포팅은 음성 데이터에서 특정 단어를 감지하는 기술로, 주로 스마트 스피커나 모바일 디바이스에서 사용된다.  
- 기존에는 CNN, RNN, Hybrid-Tree 모델 등이 주로 사용되었으며, 멜 주파수 스펙트럼(MFCC)을 주요 입력 데이터로 활용했다.  
- 최근 연구에서는 Self-Attention 기법이 추가적으로 사용되었지만, Transformer 기반의 완전한 Self-Attention 모델은 도입되지 않았다.  

### Self-Attention and Vision Transformer  
- Vision Transformer(ViT)는 이미지 분류에서 Transformer가 뛰어난 성능을 발휘할 수 있음을 보여주었다.  
- ViT는 대규모 데이터셋에서의 사전 학습이 필요했다.  
- 이 논문은 ViT의 구조를 키워드 스포팅에 맞게 변형하고, 작은 데이터셋에서도 효과적으로 작동하도록 최적화했다.  


## 3. Proposed Method  
### KWT 아키텍처  
- **입력 처리**: 멜 스펙트럼을 시간 도메인에서 분할한 후, 각 패치를 임베딩하고 클래스 토큰과 위치 임베딩을 추가한다.  
- **Transformer 구조**:  
  - 12개의 Transformer 인코더 블록으로 구성.  
  - 각 블록은 Multi-Head Attention과 MLP로 이루어짐.  
  - PostNorm 구조를 사용하여 성능을 개선.  
- **지식 증류(Knowledge Distillation)**:  
  - 교사 모델로 MHAtt-RNN을 활용하여 학습 성능을 높임.  


## 4. Experiments  
### Google Speech Commands 데이터셋  
- **데이터셋**:  
  - Google Speech Commands V1과 V2 데이터셋에서 실험.  
  - V1: 12개 클래스, V2: 35개 클래스.  
- **결과**:  
  - V2-12: 98.56%  
  - V2-35: 97.69%  
  - KWT는 V2 데이터셋에서 최고 정확도를 기록하며, 기존 모델들을 뛰어넘음.  

### Ablation Study  
- **패치 크기**:  
  - 시간 도메인 패치 크기가 성능에 가장 큰 영향을 미침.  
- **Norm 방식**:  
  - PostNorm 방식이 PreNorm 대비 우수한 성능을 보임.  

### Latency Measurements  
- 모바일 환경에서의 추론 속도를 비교.  
- KWT는 기존 모델들과 유사한 수준의 지연 시간을 보이며 모바일 환경에서도 실용적임을 확인.  


## 5. Conclusion & Limitation  
### Conclusion  
- KWT는 Transformer를 키워드 스포팅에 직접 적용한 첫 사례로, 기존 모델보다 단순하면서도 뛰어난 성능을 보였다.  
- Transformer 연구가 키워드 스포팅 도메인에서도 유망한 결과를 보일 수 있음을 증명했다.  

### Limitation  
1. 소규모 데이터셋(V1)에서 Transformer의 성능이 기존 모델보다 낮은 경우가 있음.  
2. 실시간 추론에 적합하도록 추가적인 최적화가 필요함.  
3. 대규모 사전 학습 및 압축 기법의 적용 가능성을 탐구할 필요가 있음.  


## Related Works  
1. **MHAtt-RNN**: 멀티 헤드 Self-Attention 기반의 CNN-RNN 혼합 모델로, KWT의 성능 비교 모델로 사용됨.  
2. **ViT**: Transformer 구조가 이미지 처리에서 성공적으로 사용된 사례.  
3. **SpecAugment**: 데이터 증강 방법으로 KWT의 성능 향상에 활용됨.  


## Key References  
1. Vaswani et al., "Attention is All You Need" (Transformer의 기본 구조)  
2. Dosovitskiy et al., "An Image is Worth 16x16 Words" (ViT)  
3. Rybakov et al., "Streaming Keyword Spotting on Mobile Devices" (MHAtt-RNN)  
4. Park et al., "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition" (SpecAugment)  