---
date: 2021-02-15
author: "oppenheimer1223"
title: "[논문] Spatiotemporal Contrastive Video Representation Learning"
categories: "논문"
weight: 10
---


## Motivation
1. 비디오의 비지도 표현 학습을 위해, 시간-공간적 맥락에서 contrastive learning을 적용
2. 풍부한 표현 학습을 위해 효과적인 spatial-tempral augmentation 방법을 연구

## Related Work
생략

## Contribution
- Contrasitve learning
    
    임베딩 스페이스의 피처 벡터들을 쫙 나열한 다음에 유사한 피처들은
    거리가 가깝게끔 학습 (유사도가 낮은 것은 거리가 먼 것이므로 패널티를 주지 않음) 이를 통해서 같은 비디오의 tempral distant가 있는 두 비디오 클립의
    encoder는 attract하고, 다른 비디오는 repel하게끔 학습 (SimCLR 참고)
    
- Temporal sampling strategy, consistenc spatial augmentation
    1. 기존의 방법들은 시간에 불변한 피처를 학습해서, 비디오처럼
    자연스럽게 시간에 변화하는 성질과는 이치에 맞지 않음 -> 논문은 가정을 하나 세움
        
        같은 비디오의 두 클립은 시간 차이가 길수록, 구별이 잘 됨 → 유사도가
        낮음 → 그래서 이를 반영하여 비디오 클립의 새로운 샘플링 전략
        
        시간이 가까운 간격의 샘플링 확률이 크고, 시간이 먼 간격의 샘플링
        확률이 작게끔 샘플링 분포를 줌 (이 관계는 비례)
        
        높은 확률로 샘플링한 가까운 비디오는 유사도가 클 것 → 콘트라스티브
        패널티를 많이 줄 것 (패널티가 큼)
        
        → 나의 생각: 대체로 관계가 비례하다고 생각할 수 있으나, 오래전 뷰를 다시 본다던가, 다시 물체가 나타난다던가 하면 비례하지 않을수도?
        
    2. 기존의 이미지 기반 spatial augmentation을 비디오 프레임마다 줄 수
    있으나, randomness가 모션 정보를 이해하는데에 오히려 부정적
        
        이미지 기반 spatial augmentation 자체가 하나의 이미지에만 대응하는데,
        이미 비디오의 모션들도 사실상 이미지마가 randomness를 가지고 있는 것
        
        (비디오의 consecutive frame 자체가 randomness를 가지고 있는데, 굳이
        이미지 기반 randomness cropping이나 rotation를 또 줄 필요 있나?)
        
        → 논문은 시간 축에 일관된게 spatial augmentation을 적용 (모든
        프레임에 걸쳐서 randomness를 고정)

## Experiments
생략

## Conclusion
생략