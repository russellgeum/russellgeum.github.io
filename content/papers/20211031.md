---
date: 2021-10-31
author: "oppenheimer1223"
title: "[논문] SOFT: Softmax-free Transformer with Linear Complexity"
categories: "논문"
weight: 10
---


## 형식에 자유로운 간단 요약
- NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음
- 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음
- nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?)
- 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함
- 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다.
  1. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님
  2. 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체
- 가우시안 커널 with 셀프 어텐션은 대칭임
  1. 모든 행렬이 0 ~ 1 사이 범위에 있음
  2. 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분
다른 페어는 0에 가까움
- positive defiinite kernel이므로 gram matrix로 간주 가능 ->
  1.  선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견
-  이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미)
   1. 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용
-  Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것)
-  내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지? 컴플렉시티를 왜 줄일수 있는지?