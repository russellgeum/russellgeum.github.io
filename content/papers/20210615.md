---
date: 2021-06-15
author: "Oppenheimer"
title: "[논문] Long-Shot Temporal Contrastive Learning of Video Transformers"
categories: "논문"
weight: 10
---


## Motivation    
- 비디오 트랜스포머가 기존 CNN 기반 비디오 모델들에 비해 경쟁력있는 성능을 보임
- 그러나 많은 파라미터와 inductive bias 부족은 대규모 데이터셋으로 학습된 강력한 사전 모델을 요구함

## Related Works
생략

## Contribution
- TimSformer vs Swin Transformer  
  이 논문은 스윈 트랜스포머의 속성을 비디오 도메인으로 확장하였음
    
- Long-Shot Temporal Contrastive Learning  
  템포랄 도메인의 CL을 고안
    
  1. 레이블링이 되지 않은 비디오 B를 입력으로 받는다.
  2. 비디오 B를 랜덤하게 숏클립과 롱클립으로 샘플링해서 나눈다.
  3. 프레임의 수는 같으나, 시간 간격이 달라서 롱클립이 더 긴 범위의 비디오 표현을 담고 있다.
  4. 숏클립을 쿼리로, 롱클립을 키로 임베딩하여 InfoCE에 적용한다.
  5. 이제 롤을 바꿔서 반대의 경우로 InfoCE에 적용한다.

## Experiments
- 스트라이드 수를 고정했을때 정확도가 가장 좋다.
- 모든 CL 학습 방식에서, 롱 스트라이드가 숏 스트라이드보다 클 때 성능이 제일 좋다.
- Spacetemporal + Swin은 템포랄 정보까지 학습.  
  단순 프레임 레벨로 공간 정보만 학습한 비디오 스윈 트랜스포머보다 성능이 더 높음

## Conclusion
스윈 트랜스포머를 temporal axis로 확장하면 성능이 더 좋다.  
CL에 서로 다른 템포랄 정보의 숏클립, 롱클립은 효과가 있으나, 스트라이드 수 고정이 좋음