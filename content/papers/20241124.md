---
date: 2024-11-24
author: "5biwan"
title: "[논문] BEATS : Audio Pre-Training with Acoustic Tokenizercategories"
categories: "논문"
weight: 10
---

## 1. Motivation
최근 자기지도학습(SSL)은 언어, 비전, 음성에서 큰 성과를 보여주고 있지만,  
오디오 도메인에서는 여전히 복원 손실(reconstruction loss)이 주로 사용되고 있다.  
복원 손실은 저수준 시간-주파수 특징을 재현하는 데 초점이 맞춰져,  
고수준의 의미 정보를 제대로 반영하지 못하는 한계가 있다.  

BEATS는 연속적인 오디오 데이터를 이산적(discrete) 라벨로 변환해  
고수준의 의미적 정보를 학습하는 새로운 프레임워크를 제안한다.  
이를 통해 기존 방식보다 효율적이고 의미 중심적인 학습이 가능하도록 한다.

## 2. Related Work
오디오 사전 학습은 크게 지도 학습과 자기지도학습으로 나뉜다.

- **지도 학습**  
  ImageNet 같은 외부 데이터셋이나 AudioSet을 기반으로 한 학습이 주를 이뤘다.  
  예를 들어, CLAP(Elizalde et al., 2022)은 텍스트-오디오 쌍을 활용한 대조 학습으로  
  오디오 표현을 학습했다.  

  - CLAP (Elizalde et al., 2022): 텍스트-오디오 대조 학습을 통해 오디오 표현 학습.  
  - Audio-MAE (Xu et al., 2022): 복원 기반의 오디오 SSL 모델.  
  - AST (Gong et al., 2021): Transformer 기반의 오디오 분류 모델.  

- **자기지도학습**  
  자기지도학습에서는 복원 기반 학습이 주로 사용되었다.  
  Audio-MAE(Xu et al., 2022)와 같은 방법은 뛰어난 성능을 보였지만,  
  고수준 의미 정보를 학습하는 데 한계가 있었다.  

다른 도메인에서는 BERT(Devlin et al., 2019)와 같은  
마스킹된 라벨 예측(masked label prediction) 접근법이 성공적이었다.  
하지만 일반 오디오에서는 연속적인 데이터 특성과 큰 데이터 변이 때문에  
적용이 어려운 상황이었다.

## 3. Proposed Method
BEATS는 반복적 오디오 사전 학습 프레임워크를 제안한다.  

### 1. **음향 토크나이저(Acoustic Tokenizer)**  
- **초기(Random-Projection Tokenizer)**  
  랜덤 투영과 코드북(codebook)을 사용해  
  연속적인 오디오 데이터를 이산 라벨로 변환한다.  
- **반복(Self-Distilled Tokenizer)**  
  이전 SSL 모델로부터 지식을 증류해 점진적으로  
  의미 중심의 라벨을 생성한다.  

### 2. **오디오 SSL 모델(Audio SSL Model)**  
- **Masked Audio Modeling (MAM)**  
  입력 오디오 패치의 75%를 마스킹하고,  
  마스킹된 위치의 이산 라벨을 예측하도록 학습한다.  
- Transformer 기반 구조를 사용하며, 다양한 태스크에 적용 가능하다.  

### 3. **반복 학습(Iterative Training)**  
- 음향 토크나이저와 SSL 모델을 번갈아 학습하며,  
  상호 보완적으로 성능을 높인다.  

## 4. Experiments
BEATS는 AudioSet-2M, ESC-50, Speech Commands V2 등의 데이터셋에서 테스트되었다.  

- ESC-50에서 98.1%의 정확도를 기록하며 기존 SOTA 모델보다 뛰어난 성능을 보였다.  
- AudioSet-2M에서 48.6%의 mAP(mean Average Precision)를 달성하며,  
  기존 대규모 모델보다 더 적은 데이터로 높은 성능을 기록했다.  
- BEATS의 반복 학습은 고수준 의미 정보를 점진적으로 개선하며,  
  토크나이저와 SSL 모델 간 상호 보완 효과를 입증했다.  

### 비교된 주요 모델  
- **Audio-MAE** (Xu et al., 2022): 복원 기반의 오디오 SSL 모델.  
- **CLAP** (Elizalde et al., 2022): 대조 학습을 통한 오디오-텍스트 멀티모달 학습.  
- **AST** (Gong et al., 2021): Transformer 기반의 오디오 분류 모델.  

## 5. Conclusion & Limitation
BEATS는 이산 라벨 예측 기반의 반복적 사전 학습 프레임워크를 제안하며,  
기존의 복원 손실 방식보다 더 높은 성능과 효율성을 보였다.  
특히, ESC-50과 AudioSet에서 SOTA를 기록하며  
고수준 의미 표현 학습의 가능성을 입증했다.  

하지만 BEATS는 주로 오디오 도메인에 초점이 맞춰져 있으며,  
향후 비전 및 언어와의 멀티모달 통합이 과제로 남아 있다.  
또한, 더 큰 모델과 데이터로 확장해 오디오 분류 성능을  
한층 더 높이는 것이 목표다.  

### 관련 논문  
- **Xu et al., 2022**: Masked autoencoders for audio (Audio-MAE)  
- **Elizalde et al., 2022**: Learning audio concepts with text supervision (CLAP)  
- **Gong et al., 2021**: Audio Spectrogram Transformer (AST)