---
date: 2021-04-20
author: "oppenheimer1223"
title: "[논문] Incorporating Convolution Design into Visual Transformers"
categories: "논문"
weight: 10
---


## Motivation
트랜스포머는 대규모 데이터셋이 있을떄 CNN 모델에 필적하는 성능을 보임.  
CNN의 로칼리티, 인덕티브 바이어스를 적극 활용하는 디자인의 트랜스포머 모델을 고안할 수 있을까?

## Related Works
- ViT는 대규모 이미지 데이터셋을 이용해서 CNN에 필적하는 성능을 보임  
    → 그러나 대규모 데이터는 컴퓨터 리소스의 요구가 크고, 훈련이 오래 걸림
- DeiT는 잘 학습된 대규모 CNN 모델을 티처로 두고 KD를 통해, 비전 트랜스포머 모델을 학습시키려 고함  
    → 이 역시 대규모 CNN 모델을 미리 준비해야한다는 단점
- 트랜스포머 태생이 인덕티브 바이어스를 반영하는 것이 어렵고, 불충분한 데이터로부터의 일반화 능력이 부족함  
    → CNN의 특성은 weight sharing을 통한 locality, translation invariant  
    → ViT의 로우 이미지 자체를 패치로 쓰는 것은 컨볼루션을 통한 로우 레벨 피처를 사용하지 않음  
    → 셀프 어텐션의 경우, long-range 디펜더시를 활용하나, CNN의 로칼리티를 오히려 무시하는 것 같음
  
## Contribution
- I2T 레이어  
이미지 자체를 패치로 자르지 않고, 컨브 레이어를 통하여 로우 레벨 피처를 추출하는 토큰화  
레즈넷 50과 같이 섞은 하이브리드 ViT가 있지만, I2T 레이어는 매우 shallow함, 즉 가벼움
    
- LEFFN
글로벌하게 코릴레이션을 보는 어텐션 모듈은 바꾸지 않음, 대신에 위치 정보를 반영하지 못하는 FFN을 LEFFN으로 교체함  
  1. 어텐션을 통과한 피처를 패치 토큰과 클래스 토큰으로 분리
  2. 패치 토큰의 사이즈가 N x C인데 리니어 레이어를 통해서 N x (e X C)로
  바꾸어 채널 차원을 늘림
  3. 이를 이미지 모양으로 바꿈 (Spatial Restore)
  4. Depth-wise 컨볼루션으로 representation correlation을 높임
  5. 다시 flatten하여 리이너 레이어 포워드
   
- LCA  
CNN에서 receptive field의 영역이 레이어가 깊어질수록 넓어지듯이, 트랜스포머에서는 attention distance가 레이어가 깊어질수록 넓어진다. 이렇게 트랜스포머 레이어 마다의 표현 정보를 aggressive할 수 있는 어텐션 모듈을 제안
  
## Experiments
여러가지 CNN 주요 모델과 이전 ViT 계열 비전 트래스포머 모델들과의 성능 지표 비교 → 이미지넷 분류  
I2T와 LEFFN, LCA의 ablation 실험 → 수렴 속도에 대한 언급

## Conclusion
1. CNN의 인덕티브 바이어스를 TF에 적용하기 위해 첫 아이디어를 제시한 논문
2. 어텐션 모듈은 attention + FFN인데, FFN에 적용하지 않고, attention 자체가 컨볼루션으로 이루어질 수 있을까?
3. 거기다가 FFN까지 완벽히 컨볼루션으로 이루어질 수 있을까?
4. 학습 수렴이 빠르다는 주장에서, CNN 백본과의 비교도 있었으면 좋았을 것 같음