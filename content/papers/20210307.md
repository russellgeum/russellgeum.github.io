---
date: 2021-03-07
author: "Oppenheimer"
title: "[논문] Representation Learning with Convtrastive Predictive Coding"
categories: "논문"
weight: 10
---


## Motivation
- Contrastive Learning은 latent space으로부터 downstream task에
유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러
태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면
더 좋다.
- 이 논문의 중요한 직관은 signal의 서로 다른 부분
사이에서 공유되는 정보를 인코딩하여 representation learning을 하는
것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다.
그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을
생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지
이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이
아니다.

## Related Works
셍략 
  
## Contribution
- Original signal X와 C에 대해 Mutual Information
    
    $$
      I(x;c)=\sum_{x,c}p(x, c)log{p(x,c) \over p(x)p(c)} = \sum_{x,c}p(x,
    c)log{p(x|c)p(c) \over p(x)p(c)} = \sum_{x,c}p(x, c)log{p(x|c) \over
    p(x)}
    $$
    
    상호 정보는 두 확률 변수 간의 정보량 평균을 나타낸다. 엔트로피에서
    정보량에 들어가는 확률은 c→x에 대한 조건부 확률이고 p(x, c)는 joint
    probability이다. 베이지안 규칙에 따라 이 식을 위와 같이 쓸 수 있다.
    마지막 식의 정보량에 해당하는 p(x|c)/p(x)가 density ratio인데, 상호
    정보는 이 density ratio에 비례하는 관계를 가진다.
    
- Contrastive Predictive Coding
    
    Signal의 각 time segmenet마다 해당하는 신호들을 encoder에 넣어서
    latent vector z를 출력한다. 그리고 t 이하의 모든 time segment의 latent
    vector들을 모아서 autoregressive 모델에 입력한다. 이것이 context latent
    representation이다. 이전 섹션에서 p(x|c) 형태 모델은 x_t+k를 직접적으로
    예측하지 못한다고 주장하였다. 대신에 mutual information의 density
    ratio를 모델링한다.
    
    $$
      f_k(x_{t+k},c_t) \propto  {p(x_{t+k}|c_t) \over
    p(x_{t+k})}  \rightarrow f_k(x_{t+k},c_t)=exp(z^T_{t+k}W_kc_t)
    $$
    
    - 왼쪽 식은 어떤 함수 f가 density ratio에 비례한다고 가정한다.
    - 그 식은 z와 z의 dim에 맞게 linear transformation하는 행렬 W와
    context represenation c의 내적으로 이루전 bilinear form이다.
    - 내적은 similarity를 측정하는 measure이다. 이러한 Wc는 이전
    컨텍스트로부터 예측한 predictve z이고, z^T는 현재 time signal을
    encoding한 latent vector이다.
    - 만약 예측과 인코딩한 정보가 완벽하게 일치하면 density ratio는 크고
    당연히 뻔한 정보이므로 상호 간 정보량은 크다. 반면에 서로 전혀 관련이
    없는 정보라면 내적이 orthogonal하고 desinty ratio가 1이고 예측을 못해
    놀라움이 크기에 정보량이 log 0, 즉 정보량잉 매우 크다.
- InforNCE Loss and Mutual Information Estimation
    
    $$
      L_N=-\mathbb{E} {f_k(x_{t+k},c_t) \over \sum f_k(x_{t+k},c_t)}
    $$
    
    따라서 위의 모델링 함수 f를 mutual information에 넣으면 다음과 같이
    되고 NCE 형태로 만들어서 이 Loss를 학습하도록 한다.
  
## Experiments
생략 

## Conclusion
그러면 얼마나 negative sample pair가 필요할까? sample N에서
positive sample은 1이고 negative sample은 N-1이다. 이때 같은 positive
pair면 exp 값이 매우 커져서 정보량이 뻔하고, negative pair면 exp값이
작아서 놀람의 척도인 정보량이 매우 큰데, 얼마나 negative sample이 있어야
정보량을 잘 측정할수 있을까? 사실 N-1의 수는 상호 정보량의 하한에 중요한
결정을 한다.