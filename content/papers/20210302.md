---
date: 2021-03-02
author: "Oppenheimer"
title: "[논문] Big Self-Supervised Models are Strong Semi-Supervised Learners (SimCLR v2)"
categories: "논문"
weight: 10
---

## 용어의 정리
- task-agnostic: 태스크에 구애받지 않는
- fine-tuning 할 때 사용하는 태스크는 최종 태스크 (specific task)
   
## Motivation  
레이블이 없는 방대한 데이터를 잘 활용하면서, 몇 가지 레이블로만 학습 효율을 높이는 방법론 중 하나는 비지도 학습 기반의 사전 훈련과 fine-tuning이다. 즉, 레이블이 없는 방대한 데이터를 통한 비지도 학습으로 좋은 representation을 얻은 후, 이를 통해 적은 레이블의 데이터만으로 fine-tuning을 하는 것 이러한 방법론을 컴퓨터 비전에서는 어떻게 할 수 있을지에 대한 연구이다.
    
## Related Work
이미 자연어 처리에서는 지배적인 방법이다. (큰 모델을 학습하고, 작은 모델에 teacher-student 방식으로 fine-tuning을 할 수 있는 것) 자연어 처리에서는 레이블링 되지 않은 데이터를 간접적으로 활용하지만, 컴퓨터 비전에서는 비지도 학습 기반의 방법을 직접 모델에 사용하였다. 이러한 접근은 unlabeled data를 task-specific에서 사용하는 것이 문제이다. → 좀 더 제네럴한 모델을 가져보고 싶다. 즉 task-agnostic한 방법이 없을까?
    
## Contribution
기존의 SimCLR 방법에서 다음의 과정을 덧붙임  
일단 SimCLR는 잘 알려진 비전 모델에 Linear head 등을 통하여 contrastive learning prediction task를 수행한 후, 그 모델에서 다시 specific task를 수행

- Unlabeled 데이터를 사용해서 (task-agnostic), 제네럴한 visual
representation을 얻는다. (USL을 통해, “Pretraninig”)
- 다시 적은 label data를 이용해서 teacher network를 fine-tuning
- 마지막으로 teacher network를 이용해서, Unlabeled, Labeled 데이터를
모두 이용하여 distillation 과정을 거쳐 student model을 학습

SimCLR 기반의 방법을 사용하지만, 달라진 점은 다음과 같다.  
- **ResNet 모델을 엄청 키웠다. ResNet-50은 4배, ResNet-152는 3배의 덩치로 키웠다.**  
- **Projection head라 부르는 non-linear network의 capacity를 늘렸다.**  
- **MoCo에서 영감을 받아, negative example을 최대한 늘리기 위한 메모리 네트워크를 추가 → 하지만 이는 1% 정도의 성능 향상**  

Distillation 과정에서는 student network를 학습하기 위해, fine-tuning된 teacher network를 사용한다. (위와 동일) 이때 teacher network의 출력 벡터를 이용하여 Distillation loss를 정의한다. (이해 완료) 즉 teacher network의 출력 벡터를 타겟으로 두고, student network의 출력 벡터를 예측으로 두어서, 이 둘 간의 CE를 계산
    

## Experiments
- 모델을 크기를 배로 키웠을 때, 단지 1%의 라벨링 데이터만으로 파인 튜닝을 해도 29%의 성능 향상
- Projection head를 구성하는 리니어 레이어 갯수를 늘리고, 학습 후에 projection head를 버리는 것이 아니라, 첫 번째 linear head까지 포함  
    (이는 base encoder 구조에 linear layer 하나를 추가한 것)  
    위와 같이 1% 라벨링 데이터만으로 fine-tuning을 해도, 14%의 놀라운 성능 향상
  1. Projection head가 많으면 few-labeled 데이터에서 좋은 성능
  2. 첫 번째 head를 fine-tuning에 포함하면 성능이 가장 크게 향상
- 티처 네트워크와 스튜던트 네트워크 간의 크기 차이에서
distillation의 성능 향상 여지 차이가 있음 (당연)
- 10%의 적은 레이블링 데이터로 fine-tuning하고, distillation까지
하면 지도 학습에 준하는 성능  

## Conclusion
컴퓨터 비전 분야에서도 대규모 업스트림 학습으로 좋은 representation을
얻고, 이를 통해 다운스트림 태스크를 잘 수행하자는 연구. 보다 일반적인 visual representation을 얻고 (더 큰 모델을 통해서),
이를 compact 모델에 distillation하는 KD 관점이 흥미로움  
- 어떻게 하면 더 좋은 distillation이 가능할까? 이 과정의 좋은
파이프라인과 레시피를 얻을 수 있을까?
- 왜 더 큰 모델일수록 학습력이 좋아질까?