---
date: 2021-06-30
author: "oppenheimer1223"
title: "[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes"
categories: "논문"
weight: 10
---

## Motivation
너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
    
## Related Work
- Novel View Synthesis  
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS)
- Novel Time Synthesis  
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음
- Space-Time synthesis  
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음
    
## Contribution
NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인

- Temporal Photometric Consistency
    - NeRF의 scene rendering에서 프레임 간 scene flow로 보정 텀을 추가
    - t 프레임에서 t+1 프레임으로 향하는 scene flow가 있다면, SFt_t+1을
    통해 t → t+1 렌더링을 하고, t+1의 이미지를 와핑하여 t 프레임으로 변환 이 합성 이미지 t 프레임과
    타겟 이미지 t 사이의 로스
    
- Scene Flow Prior
    - Scene flow는 t → t+1 플로우와 t+1 → t 플로우가 (오클루션 같은 이슈가
    없다면) 일치해야함. 이 프라이어를 regularization으로 줌
    
- Data-driven Prior
    - 옵티컬 플로우 pretrained model과 뎁스 추정 pretrained model 출력을
    슈퍼 비전으로 추어서- 플로우 맵과 뎁스 맵을 regularization
    - 다만 pretrained 출력은 노이지해서 계속 쓰지는 않고 초반에 init 용도로만 쓰고 이후론 0으로 decay
    
- Integrating static scene representation
    - 대부분의 scene은 움직이지 않음 → static scene 만을 위한 모델을 별개로
    추가
    - weight 출력을 하는데, weight를 static 오브젝트에 1을 할당, static
    오브젝트에 0을 할당
    - static NeRF에서 얻은 c와 dynamic NeRF에서 얻은 ci를 통해 다음의 식을
    조합
    - static model로 얻은 c, sigma에는 v를 곱하고, dynamic model로 얻은 sigma, c에는 1-v를 곱해서 레이 포인트가 움직이는 장면인지 아닌지 판단하게 만듬
    - sigma * c = v * sigma * c + (1-v) * sigma_dy * c_dy
  
## Experiments
생략
        
## Conclusion
- 훈련 시간이 오래 걸림
- 빨리 움직이는 고주파수 디테일을 살리지는 못함
- 훈련 하는 동안 한 번도 보지 못한 seen은 extrapolate를 하지 못함
- 시간 싱크가 맞추어진 멀티 프레임으로부터 NVS를 시간 정보까지 배우는 NVS를 하는 태스크
- 지오메트릭한 정보는 사용하지 않음 (오로지 Scene flow를 추정해서 와핑하는 것 외)
- Static model은 SSL Depth로 더 쉽게 할 수 있지 않을까?