---
date: 2023-10-03
author: "oppenheimer1223"
title: "[논문] ICCV 2023 관심 논문 리스트업"
categories: "논문"
weight: 10
---


## ICCV 2023
[ICCV 2023 Link](https://iccv2023.thecvf.com/)


### Papers
ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.  
(일부 특이한 연구도 포함)  


### Neural Radiance Fields
1. [NeRF-MS: Neural Radiance Fields with Multi-Sequence](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_NeRF-MS_Neural_Radiance_Fields_with_Multi-Sequence_ICCV_2023_paper.pdf)  
Peihao Li et al.

2. [Re-ReND: Real-time Rendering of NeRFs across Devices](https://openaccess.thecvf.com/content/ICCV2023/papers/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.pdf)  
Sara Rojas et al.

3. [CLNeRF: Continual Learning Meets NeRF](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_CLNeRF_Continual_Learning_Meets_NeRF_ICCV_2023_paper.pdf)  
Zhipeng Cai, Matthias Muller

4. [Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction](https://arxiv.org/abs/2304.06714)  
Hansheng Chen et al.

5. [SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields](https://arxiv.org/abs/2212.02501)  
Anh-Quan Cao, Raoul de Charette

6. [NerfAcc: Efficient Sampling Accelerates NeRFs](https://arxiv.org/abs/2305.04966)  
Ruilong Li et al.

7. [FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models](https://arxiv.org/abs/2303.12786)  
Jianglong Ye, Naiyan Wang, Xiaolong Wang

8. [ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering](https://arxiv.org/abs/2305.02103)  
Andrea Ramazzina et al.

9. [MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos](https://arxiv.org/abs/2212.13056)  
Fengrui Tian, Shaoyi Du, Yueqi Duan


### Multimodal && VQA
1. [DIME-FM : DIstilling Multimodal and Efficient Foundation Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_DIME-FM__DIstilling_Multimodal_and_Efficient_Foundation_Models_ICCV_2023_paper.pdf)  
Ximeng Sun et al.

2. [VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_VQA-GNN_Reasoning_with_Multimodal_Knowledge_via_Graph_Neural_Networks_for_ICCV_2023_paper.pdf)  
Yanan Wang et al.

3. [Localizing Moments in Long Video Via Multimodal Guidance](https://arxiv.org/abs/2302.13372)  
Wayner Barrios et al.

4. [Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf)  
Zi Qian et al.

5. [Discovering Spatio-Temporal Rationales for Video Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Discovering_Spatio-Temporal_Rationales_for_Video_Question_Answering_ICCV_2023_paper.pdf)  
Yicong Li et al.

6. [Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ko_Open-vocabulary_Video_Question_Answering_A_New_Benchmark_for_Evaluating_the_ICCV_2023_paper.pdf)  
Dogwab Ko et al.

7. [Variational Causal Inference Network for Explanatory Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.pdf)  
Dizhan Xue et al.

8. [Toward Unsupervised Realistic Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Toward_Unsupervised_Realistic_Visual_Question_Answering_ICCV_2023_paper.pdf)  
Yuwei Zhang et al.

9. [VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_VQA-GNN_Reasoning_with_Multimodal_Knowledge_via_Graph_Neural_Networks_for_ICCV_2023_paper.pdf)  
Yanan Wang et al. 


### Quantization
1. [Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers](https://arxiv.org/abs/2308.10814)  
Natalia Frumkin, Dibakar Gope, Diana Marculescu  

2. [EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization](https://arxiv.org/abs/2307.10554)  
Peijie Dong et al.

3. [DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization](https://arxiv.org/abs/2208.09708)  
Xinlin Li et al.

4. [ResQ: Residual Quantization for Video Perception](https://arxiv.org/abs/2308.09511)  
Davide Abati et al.

5. [I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.pdf15734)  
Zhikai Li, Qingyi Gu  

6. [RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf)    
Zhikai Li et al.

7. [Unified Data-Free Compression: Pruning and Quantization without Fine-TuningTeachers](https://arxiv.org/abs/2308.07209)  
Shipeng Bai et al.

8. [A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance](https://arxiv.org/abs/2308.13504)  
Ian Colbert et al.


### Distillation
1. [TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance](https://arxiv.org/abs/2309.12314)  
Kan Wu et al.

2. [DOT: A Distillation-Oriented Trainer](https://arxiv.org/abs/2307.08436)  
Borui Zhao et al.

3. [From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels](https://arxiv.org/abs/2303.13005)  
Zhendong Yang et al.

4. [Cumulative Spatial Knowledge Distillation for Vision Transformers](https://arxiv.org/abs/2307.08500)  
Borui Zhao et al.

5. [Multi-Label Knowledge Distillation](https://arxiv.org/abs/2308.06453)  
Penghui Yang et al.



### Etc
1. [Dataset Quantization](https://arxiv.org/abs/2308.10524)  
Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, Jiashi Feng

2. [DREAM: Efficient Dataset Distillation by Representative Matching](https://arxiv.org/abs/2302.14416)  
Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, Yang You

3. [DataDAM: Efficient Dataset Distillation with Attention Matching](https://openaccess.thecvf.com/content/ICCV2023/papers/Sajedi_DataDAM_Efficient_Dataset_Distillation_with_Attention_Matching_ICCV_2023_paper.pdf)  
Ahmad Sajedi et al.