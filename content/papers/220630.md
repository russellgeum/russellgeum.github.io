---
date: 2022-06-30
author: "oppenheimer1223"
title: "[논문] Rethinking the Augmentation Module in Contrastive Learning"
categories: "논문"
weight: 10
---


## Motivation
- CL은 DA에 강력하게 의존하는 방법이다.
- 인위적인 DA는 다음과 같은 단점이 있다.
    - 데이터 증강의 휴리스틱한 조합은 특정적인 표현 불변성을 가져다 준다.
    - 강력한 데이터 증강은 너무 많은 불변성을 가지고 있어서 오히려 fine-grained한 다운스트림 태스크에 적합하지 않다.
- 따라서 이 논문은 어디서? 무엇을? 이란 질문으로 DA를 하는 방법론을 소개한다.
    
## Related Work
생략

## Contribution
- 다양한 augmentation module 조합을 사용한다.
- 샴 구조는 깊이에 따라 여러 스테이지로 활용한다. 각 스테이지의 피처를 CL에 활용한다.
- Hierarchical augmentation invariance
    
    위에서 언급했던 데이터 증강에 따른 의존성을 해결하기 위해서는 인코더의 각각 다른 뎁스에 서로 다른 불변성을 적용해서 탈피한다. → 인코더가 계층마다 여러 불변성을 학습하게 함. 논문에서는 데이터 증강 기법을 2개에서 5개까지 조합한 4개의 타입으로 각 이미지를 변환. 이 변환된 이미지를 레즈넷에 포워드하여 서로 다른 불변성을 가진 피처를 통해 Contrastive Loss를 학습
    
- Feature expansion with augmentation embeddings

## Experiments
- 각 CL 베이스라인에 이 방법을 붙여서 실험했을 경우, 성능이 상당히 좋았음

## Conclusion
- 이 논문이 제안하는 방법은 근본적인 augmentation 불변성이 인코더 전체에 고루 분산되도록 하는 것
- 일부 중요하지 않은 불변성을 더 깊은 계층 → 계층적으로 불변성을 확대하는 학습으로 제한
- Augmentation의 휴리스틱한 조합은 인코더에 전반적인 불변성을 enforce하는 효과가 있기에, 다양한 불변성을 인코더에 널리 학습되게 하고, 레이어가 깊을수록 특히 불변성을 확대하는 것으로 학습
- 이러한 네트워크가 어떤 효과가 있을까?