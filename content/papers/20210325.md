---
date: 2021-03-25
author: "Oppenheimer"
title: "[논문] CvT, Introducing Convolution to Vision Transformer"
categories: "논문"
weight: 10
---


## Motivation
트랜스포머를 적용한 비전 모델은 더 적은 데이터로 학습하고 비슷한 사이즈의 ResNet보다 성능이 낮음. 그 이유는 비전 태스크에서 CNN이 가지는 장점을 ViT는 활용할 수 없음. 이미지는 pixel간 local correlation이 있고 CNN은 이걸 잘 잡아내는데, ViT는 이 능력이 부족함. 이러한 CNN의 local correlation에는 shift, scale, distortion invariance가 있음

## Related Works
생략
  
## Contribution
가장 큰 핵심은 트랜스포머의 MLP를 컨볼루션으로 대체한 것
- Convolutional Token Embedding Layer  
    이전의 CvT 출력을 입력으로 받아서, 새로운 토큰을 만드는 함수 f를 정의. 이 f는 s x s 컨볼루션으로 구성, 출력 사이즈는 H x W x X. 이를 flatten하면 HW x C이고 layer normalized를 적용하고 다음 트랜스포머 블록으로 입력, 이러한 과정은 두 장점이 있음.
    
    - 토큰 시퀀스의 길이를 줄일 수 있음, 그리고 피처 디멘젼에서 토큰을 늘릴 수 있음 (무슨 말일까?)
    - 이는 토큰에게 보다 복잡한 비주얼 표현을 배울 수 있게 하고, CNN 처럼 larger spartial footprint를 토큰에 줄 수 있음

- Convolutional Projection for MHSA  
    MHSA의 MLP를 Depth wise SepConv로 적용한 것. token map을 2D로 reshape하고, 이를 잘라서(?) MHSA에 넣음, 그리고 Depth-Wise Sep Conv로 Q, K, V를 계산 이를 통해서 여러 멀티 헤드 어텐션을 계산
    
- Free Positional Encoding  
  기존의 트랜스포머는 이미지 패치 간의 위치 정보를 파악할 수 없음 (로컬한 정보를 모름) 그러나 CvT처럼 트랜스포머가 local spatial relationship을 가지게 되면, 포지셔널 인코딩을 할 필요가 없음.
  
## Experiments
생략

## Conclusion
생략