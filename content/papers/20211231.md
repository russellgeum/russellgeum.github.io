---
date: 2021-12-31
author: "Oppenheimer"
title: "[논문] Masked Autoencoders Are Scalable Vision Learners"
categories: "논문"
weight: 10
---


## Motivation
1. 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이
복원할 수 있을까?
1. 비대칭 형태의 인코더 - 디코더
    
    인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이
    latent vector를 가지고 원래의 이미지를 복원
    
    인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성

## Related Works
1. 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
2. 마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만,
비전에서 오토인코딩으로의 진전 X

    **저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가**?

3. 자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다.

    몇 가지 단어를 마스크하고 학습할 때, 모델은 꽤 세련된 문장 구사력을
    가진다.

4. 반면에 이미지는 풍부한 공간적 중복성을 가진 시그널이다.

    → mask 패치도 인근 visible 패치의 정보와 관련해서 복원이 가능하다.

    → 어떻게? 높은 수준의 파츠, 오브젝트, 장면의 이해를 이용해서

## Contribution
 1. 높은 비율로 이미지를 마킹하는 것은 공간적 중복성을 낮춘다 → 패치간 연관성을 떨어트린다. → 로우 레벨 이미지 처리를 넘어 어려운 SSL
 태스크를 만든다.
 1. 인코더는 입력 이미지를 latent representation으로 임베딩하고, 디코더는 latent representation을 타겟으로 복원한다.
 2. 로스 함수도 간단하다. 가장 마지막 레이어는 타겟 이미지의 픽셀 채널과 동일한 출력 채널을 가지고 타겟 이미지와의 MSE 로스를 준다.
        
## Experiments
1. 마스크 비율이 70~80에서 ImageNet-1K validation acc가 높다. 이것이 무슨 의미를 가질까?
2. 마스크를 어떤 형태로 주냐에 따라 실험 결과가 다르다. (random block vs grid)
  
블록 마스크는 랜덤 마스크보다 태스크가 어렵다. 복원 이미지 또한 블러링 되어 있다.

그리드 마스크는 랜덤 마스크보다 태스크가 쉽다. 로스 역시 그렇게 나왔다. 복원 이미지는 굉장히 샤프하다.

**랜덤 마스크가 spatial rebundency를 더 잘 포착한다고 할 수 있다. 왜 그럴까?**

## Conclusion
1. 저자의 추측  
    이미지와 언어는 서로 다른 성질의 신호이다. 언어는 사전에 상당히 의미론적 정보를 담고 있으나, 이미지는 그렇지 않다 의미론적 분해 없이 단지 빛에 의해 기록된 신호이다. 오브젝트를 제거하는 대신에, 의미론적 분할을 형성하지 않을 가능성이 크게끔 랜덤으로 패치를 제거한다. 마찬가지로 MAE 로스 역시 의미론적이 아닌 픽셀 레벨로 재구성을 하는 로스이다. 두 가지의 제약 조건에도 불구하고, 실험 결과는 이러한 모델과 로스가 전체적으로 재구성을 하는 방법을 익혔음을 말한다. 저자는 이러한 기능이 MAE 내부에서 발생하는 풍부한 표현력에서 기인한다고 가설을 세운다.
  
2. 나의 생각  
    랜덤 마스킹을 해도 시맨틱한 정보를 모델이 유추할 수 있는 것은 ViT의 성질이 아닐까? 트랜스포머는 롱 레인지 디펜더시를 볼 수 있기 때문에 서로 멀리 떨어져 있는 패치간의 코릴레이션을 잘 볼 수 있다. mask ratio가 낮을 때, 오히려 성능이 낮은 이유가 트랜스포머와 관련이 있을까? → 잘 모르겠다. 직관으로는 mask ratio가 낮으면 더 성능이 좋아야할 것같은데 랜덤 마스킹이 트랜스포머의 롱 레인지 디펜더시를 부각시킬 수 있는 하나의 수단으로 볼 수 있나?