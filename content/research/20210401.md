---
date: 2021-04-01
author: "Oppenheimer"
title: "[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning"
categories: "논문"
weight: 10
---

## Motivation
비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.

## Contiribtuion 
그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)
- **논문의 가설**: 비디오의 timespan에 따라, 비주얼적 내용들이 시간적 지속성을 가진다는 것 (사실 당연함) → 비디오의 액션 등
- **논문의 목적**: 같은 비디오에서 서로 다른 클립의 표현적으로 유사성을 띄도록 가이던스를 주는 연구  

이미지 augmentation → 프레임의 natural augmentation, 즉 비디오
1. 시간-공간에 걸쳐서 비주얼 표현의 지속성을 강화하여 비디오 태스크에서의 성능을 좋게 할 수 있을까?
2. 같은 비디오의 클립들은 얼마나 떨어져 있어야 할까? 그리고 클립의
3. 시간적 길이는 얼마나 되어야 할까?
crops, augmentation을 통해 서로 다른 view에서도 invariant feature를 학습하는 것을 temporal domain으로 확장

### Persistent Temporal Feature Learning
클립의 사이즈는 전체 시간 간격 T를 가지고, 각 프레임 사이의 스트라이드는 tau → 인코더가 가지는 클립의 해상도는 T x tau 배치 사이즈 B이고, 한 배치마다 rho의 클립을 가지면 rB의 클립이 한 비디오에서 추출한 것. 논문의 프레임워크는 언레이블링된 비디오의 서로 다른 x 클립을 입력으로 넣음, 이때 인코더는 파라미터 theta의 ConvNet (ResNet50, SlowFast Net, 3D
ResNet-50) **→ ResNet-18 or 인코더 부분을 아예 트랜스포머로 교체 하는 것도
고려 가능**

이 섹션에서 연구된 학습 방법론은 키 샘플 셋 {k+}와 쿼리 샘플 q의 유사성을 최대화하는 방향으로 학습한다. key 셋은 q가 연산된 것과 동일한 비디오의 다른 클립들이 인코딩된
버전이다. (구체적으로 어떤 모양?)

- SimCLR
    미니배치에서 다른 클립의 임베딩을 negative sample로 사용
    
- SwAV
    SimCLR의 negative sample을 사용하지 않는 버전
    
- MoCo
    MoCo는 q와 동일한 비디오 클립에서 positive embedding k+를 계산,
    queue에서 취한 negative embedding k-는 이전 iteration에서 저장된 clip
    embedding이다.
    
- BYOL
    MoCo의 negative sample을 사용하지 않는 버전. 대신에 추가로 MLP head를 인코딩의 임베딩 이후에 붙임
    → positive sample의 consine similarity를 maximize하는 문제를 negative
    sample의 consine similarity를 minimize하는 문제로 바꿈
    
- Implementation specifices  
    클립의 수가 2보다 큰 경우, 모든 클립 각각에 대해서 q를 계산하고
    나머지 클립들로 positive sample을 계산 → Lq의 서브 로스로 사용.  
    그리고 symmetric loss는 모든 Lq 서브로에 걸쳐서 평균을 취하는 것,
    이때 MoCo, BYOL은 두 개의 인코더로 각각 처리  
    반면에 SimCLR, SwAV는 전체 손실이 모든 클립에서 병렬이므로 메모리 소비량이 사용된 클립 수에 비례하게 증가
        
## Experiments
- Sptiotemporal vs Only spatial temporal leraning
- Temporal augmentation  
    클립의 수가 2, 3, 4일때 큰 성능 향상이 있음
    
- More clips are beneficial    
    1. Epoch가 길수록 consistent gain이 크다.
    2. 어느정도 epoch 이상이면 saturation이 명확히 보인다.

- Negatives do no help but momentum encoders do  
    contrastive/non-contrastive 방법의 차이가 명확히 보이지는 않는다. 따라서 space-time persistence를 학습하는 것이 비디오 표현 학습 핵심이다.
    
- Timespan between positives  
    클립의 수가 2 이상인 모든 실험은 전체 비디오에서 global temporal sampling을 하는 것이다. 이는 클립들이 temporal location에 제약이 없이 샘플링될 수 있음을 말한다.
    
- 실험 결과 정리
    1. t = 0인 같은 비디오에서의 positive samping 성능이 좀 낮고, t =
    10까지 늘려도 성능이 상당히 robust하다. long-temporal correspondence obejctive가 성능에 크게 영향을 주지 않는다는 것 (그렇다고 가속하지는 않음)
        
    2. 훨씬 긴 비디오가 global sampling의 이점을 제공함. positive sampling의 time window를 제한하는 것은 이점이 없으며, 오히려 objective가 극도로 느리게 변하는 feature를 학습하는 것이라 생각할 수 있음 (60초에 걸쳐서 변하는 그런)
        
    3. 0.5초 ~ 10분 사이로 구성된 데이터 lG-Uncurated-1M에서는 평균
    35.3초의 데이터인데, 여기서는 36까지 temporal distance를 두고 학습시켰을때 성능 향상에
        이점이 있었고, 그 이후로는 성능이 오히려 낮아졌다.
    
- Augmentations  
    spatial cropping, temporal cropping, raiometric color가 주요한
    augmentation 방법  
    1. MoCo, BYOL이 SimCLR, SwAV보다 성능이 좋다.  
    2. augmentation 중에서도 temporal persistency가 가장 영향이 크다.  

## Conclusion
1. temporal distance가 positive sample을 통한 학습에 매우 중요
2. 생각보다 contrastive learning의 방법이 momentum encoder보다는 영향을 크게 주지 않는듯
3. 당연하지만 epoch, backbone, video augmentation이 성능에 매우 중요

## 레퍼런스
[1] Momentum contrast for unsupervised visual representation learning (MoCo)  
[2] A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)  
[3] Improved Baselines with Momentum Contrastive Learning (MoCo v2)  
[4] Bootstrap Your Own Latent： A New Approach to Self-Supervised Learning (BYOL)  
[5] Unsupervised Learning of Visual Features by ContrastingCluster Assignments (SwAB)  
[6] Learning temporally persistent hierarchical representations  
[7] Unsupervised Learning of Spatiotemporally Coherent Metrics  
[8] Deep Learning from Temporal Coherence in Video  
[9] Unsupervised Visual Represenation Learning  