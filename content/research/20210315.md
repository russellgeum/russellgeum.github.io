---
date: 2021-03-15
author: "Oppenheimer"
title: "[논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows"
categories: "논문"
weight: 10
---


## Motivation
이 논문은 일반적인 컴퓨터 비전에서의 트랜스포머 백본을 제공하기
위함이다. 자연어처리에서의 트랜스포머가 비전으로 옮겨올 때, 두
도메인에서의 차이가 있었다.

하나는 비주얼 객체의 다양한 바리에이션이고 단어와 비교해서 이미지의
높은 해상도가 문제이다. → 이미지나 이미지 패치에 직접 트랜스포머를
적용하면 계산량이 쿼드라틱하게 증가한다.

## Related Works
작년 10월, 구글의 ViT는 비전 태스크에 컴퓨터 비전 분야는 CNN가 지배적임. AlexNet부터 더 크고, 다양하고,
정교한 기술들로 CNN backbone들이 발전함. 한편 자연어 분야는 트랜스포머가 지배적 → 트랜스포머는 데이터의 long
range dependency를 잘 반영함 (언어의 특징). 이 논문은 컴퓨터 비전을 위한 트랜스포머를 좀 더 실용적으로 쓸 수 있는
아이디어를 제공

자연어처리에서 단어와는 다르게 비주얼 요소들은 스케일이 매우 다양할 수 있음 → 이는 OD 태스크에서 어텐션에게 문제가 됨. 이는 다양한 스케일을 반영하지 못하는 고정된 토큰 길이가 비전 응용에는 적합하지 않음. 또 다른 문제는 텍스트와 비교해서 고해상도 이미지인데, 픽셀 레벨로 예측을 해야하는 세그멘테이션 같은데서 트랜스포머는 다루기가 힘듬. 왜냐하면 연산 비용이 크기 때문.

**Swin-Transformer는 작은 사이즈의 패치에서 시작해서,
트랜스포머 레이어가 깊어질수록 점진적으로 이웃 패치들과
병합한다.**

**슬라이딩 윈도우와는 달리, 쉬프티드 윈도우로 연산하면 실제
세계의 레이턴시에 효율적이고, 하드웨어의 메모리 접근이
용이하다.**

**쉬프티드 윈도우는 이전 레이어에서의 윈도우 정보와 연결되는
맥락이 있다. → 모델 성능 개선에 좋음**
  
## Contribution
- **모델의 구조**
  - RGB 이미지를 겹치지 않게 패치로 나눈다. 논문에서는 4x4 크기 패치로 나눔, 따라서 패치의 차원은 4x4x3. 리니어 임베딩의 차원은 임의로 정해준다.
        
  - 각 패치의 토큰에 조금 수정한 셀프 어텐션 계산을 한다. 트랜스포머 블록은 전체 토큰의 수, (H/4, W/4)를 유지한다.
  - 하이어라키한 표현을 만들기 위해서, 패치의 머징을 통해 토큰의 수를 줄인다. (레이어가 깊어질수록)
        
    첫 번째 패치를 머징하는 레이어는 피처들을 2 x 2 그룹으로 머징한다. 그리고 4C로 임베딩한다. 이때 토큰의 수는 4배 줄어든다. 이 과정을 반복한다. 해상도 스케일은 /4 /8/ 16/ 32
  - **Swin-Transformer-block**
        
    2개의 MLP, W-MSA, SW-MSA로 구성, 그리고 각 연산 뒤에는 레이어노름이 따라오고 비선형성을 위해 GELU를 부여
        
- **Shited Window based Self-Attention**
    
    표준적인 트랜스포머는 하나의 패치가 다른 모든 패치와의 relation을 계산하도록 한다. → 이러한 전역 계산은 쿼드라틱한 복잡도를 증가, 당연히 이는 비전
    태스크에 적절하지 않음
    
    - Self-attention in non-overlapped windows
    - **Shifted window partitioning in succesive
    blocks**
        
        윈도우 기반 셀프 어텐션은 윈도우 간 연결이 부족하다. 이는 모델링 파워에 제약을 건다. 겹치지 않는 윈도우의 효율적인 계산을 유지하면서 윈도우 간 연결을 도입하기 위해, shifted window partitioning 접근을 제안한다. 이는 연속된 Swin Transformer 사이에서 두 개의 분할된 구성 사이를 대체한다. (정확히 무슨 말인지는 이해하지 못하였음). 이 모듈은 MHSA 형태에 기반하지만, 이전 레이어에서 겹치지 않고 이웃하는 window 끼리의 connection을 준다. → 성능이 우수함
        
    - Relative position bias
        
        상대 위치가 절대 위치나 포지셔널 인코딩이 없는 것보다 성능이 더 좋다
  
## Experiments
생략 

## Conclusion
생략