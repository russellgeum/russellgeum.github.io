---
date: 2021-11-01
author: "Oppenheimer"
title: "[논문] Decoupled Contrastive Learning"
categories: "논문"
weight: 10
---


## Motivation
CL의 로스에서 각 편미분에 대해 커플링되는 텀이 있음 → 이는 학습 효율성에 관여

## Related Works
- CL은 학습에서 많은 양의 네거티브가 필요하다 → 큰 배치 사이즈를 요구로 함 → 이는 어쩔수없이 컴퓨터 리소스가 많이 필요
- 따라서 배치 사이즈에 민감하다.
  
## Contribution
- Infoloss를 각각 편미분 하였을때 공통된 텀이 포함되는 것을 보임
- 이 텀은 P/N 간 커플링 되는 것을 의미하고 학습 효율성에 영향을 줄 수 있음  
  예를 들어 N이 가깝고 P도 가까우면, N의 grad 또한 감소. 반면에 N이 멀고 P도 멀면 P를 땡겨야 하는데 P 역시 N과 커플링되어 P를 땡기질 못함
- Infoloss에서 분모의 positive pair를 제거하고, 분자의 positive pair를 분리하여 식을 약간 변형
- 이때 positive pair에 weighted function을 주어서 직관적으로 positive pair 샘플끼리 거리가 멀면 학습해야할 것이 더 많음을 알 수 있다.
  
## Experiments
- 베이스라인은 SimCLR이고 베이스라인 대비, DCL를 적용한 학습이 적은 배치사이즈, 적은 큐 사이즈에서도 성능이 월등이 우위
- 이미지넷, STL의 실험에서 결과가 모두 좋다고 리포트
  
## Conclusion
- 왜 커플링 텀을 제거하는 것이 적은 배치사이즈에서도 학습 효율성을 좋게 만들까?
- DCL은 모멘텀 엔코더, 라지 배치사이즈, 롱 에포크를 요구하지 않는다.
