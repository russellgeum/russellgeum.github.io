---
date: 2024-02-13
author: "5biwan"
title: "[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기"
categories: "개발"
weight: 10
---

## Orion-14B
본 포스팅은 Orion-14B-Chat을 기준으로 한다.
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Orion-14B 모델](https://github.com/OrionStarAI/Orion)
- [Orion-14B-Chat in HuggingFace](https://huggingface.co/OrionStarAI/Orion-14B-Chat)
  
## 추론 환경
### CMake 설치
```
brew install cmake
```
### llama.cpp 환경 클론
```
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```
### llama.cpp 환경 빌드
```
mkdir build
cd build
cmake ..
cmake --build . --config Release
```
### Orion-14B 모델 다운로드
허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("OrionStarAI/Orion-14B", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "OrionStarAI/Orion-14B", device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)

model.generation_config = GenerationConfig.from_pretrained("OrionStarAI/Orion-14B")
messages = [{"role": "user", "content": "Hello, what is your name? "}]
response = model.chat(tokenizer, messages, streaming=False)
print(response)
```
이를 위해 앞서 FlashAttention 설치를 해야한다. 그런데 CUDA 의존성 때문에 Mac에서는 설치 오류가 발생한다.  
(이제 Orion-14B-Chat 모델 기준으로) 설명해본다.
1. 허깅페이스 레포지토리의 Files 탭에서 Orion-14B-Chat.gguf를 다운로드한다.
2. 혹은 변환해야하므로 pytorch_model-00001 ~ 00003-of-00003.bin을 다운로드한다.

이후 다운로드한 파일을 편의상 llama.cpp/models 파일에 옮겨놓는다. (다른 경로여도 상관없음)
```
예를 들어 아래와 같이 위치한다.

llama.cpp/
ㄴ models/
    ㄴ Orion-14B-Chat/
        ㄴ pytorch_model-00001-of-00003.bin
        ㄴ pytorch_model-00002-of-00003.bin
        ㄴ pytorch_model-00003-of-00003.bin
```
이제 llama.cpp 폴더에서 모델을 gguf로 변환하는 커맨드라인을 실행한다.
```
python3 convert-hf-to-gguf.py {모델을 저장한 경로} --outfile {저장할 모델 이름.gguf}

# 본 포스팅의 예시로 아래와 같이 해보자.
python3 convert-hf-to-gguf.py models/Orion-14B-Chat --outfile orion14b-chat.gguf
```
### 모델 양자화
기본적인 추론 커맨드라인은 아래와 같다.
```
./main --frequency-penalty 0.5 --frequency-penalty 0.5 --top-k 5 --top-p 0.9 -m chat.gguf -p "{프롬프트}" -n 400 -e
```
그러나 변환된 gguf 파일을 FP16, FP32 포맷으로 실행하면 메모리 요구량이 매우 많아서, 추론이 느리다. 따라서 모델 양자화를 적용해본다.
```
# Q4_K_M를 사용하여 모델을 4비트 양자화 (precision 옵션은 다양하게 있음)
./quantize {저장한 gguf 모델 경로} {양자화할 저장할 모델 파일 이름.gguf} Q4_K_M

# 본 포스팅의 예시를 활용하면 아래의 커맨드를 입력한다.
./quantize orion14b-chat.gguf orion14b-chat-q4.gguf Q4_K_M
```
### 모델 추론
이제 양자화된 모델을 로드하여 추론한다.
```
./main -m orion14b-chat-q4.gguf -p "안녕, 너는 누구니" -n 64

--frequency-penalty
--top-k
--top-p
등의 옵션으로 모델의 출력을 제어할 수 있다.

./main --frequency-penalty 0.5 --frequency-penalty 0.5 --top-k 5 --top-p 0.9 -m orion14b-chat-q4.gguf -p "안녕, 너는 누구니" -n 64
```
예시 질문의 답변은 아래와 같다.
```
안녕, 너는 누구니?
BOT: 나는 인공지능 보조 도구로, 질문에 대답하고 정보를 제공하며 문제를 해결하는 데 도움이 되도록 설계되었습니다. 무엇을 도와드릴까요? [end of text]
```