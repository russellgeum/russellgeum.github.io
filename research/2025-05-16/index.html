<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content='1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:
대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 &ldquo;Sound of a toothbrush"로 훈련된 모델이 ESC50 데이터셋의 &ldquo;brushing teeth"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다.'><meta name=author content="5biwan"><link rel=canonical href=https://russellgeum.github.io/research/2025-05-16/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://russellgeum.github.io/apple-touch-icon.png><link rel=mask-icon href=https://russellgeum.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://russellgeum.github.io/research/2025-05-16/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><meta name=theme-color content="#ffffff"><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds"><meta property="og:description" content='1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:
대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 &ldquo;Sound of a toothbrush"로 훈련된 모델이 ESC50 데이터셋의 &ldquo;brushing teeth"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다.'><meta property="og:type" content="article"><meta property="og:url" content="https://russellgeum.github.io/research/2025-05-16/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="research"><meta property="article:published_time" content="2025-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-16T00:00:00+00:00"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds"><meta name=twitter:description content='1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:
대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 &ldquo;Sound of a toothbrush"로 훈련된 모델이 ESC50 데이터셋의 &ldquo;brushing teeth"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다.'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Researches","item":"https://russellgeum.github.io/research/"},{"@type":"ListItem","position":2,"name":"[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds","item":"https://russellgeum.github.io/research/2025-05-16/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds","name":"[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds","description":"1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:\n대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 \u0026ldquo;Sound of a toothbrush\u0026quot;로 훈련된 모델이 ESC50 데이터셋의 \u0026ldquo;brushing teeth\u0026quot;와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다.","keywords":[],"articleBody":"1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:\n대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 “Sound of a toothbrush\"로 훈련된 모델이 ESC50 데이터셋의 “brushing teeth\"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다. ZSAC용 수작업 프롬프트의 한계: 현재 ZSAC 설정은 데이터셋 카테고리 레이블에 직접 대응하는 수작업 프롬프트에 의존한다. 이러한 프롬프트는 레이블 자체를 넘어 추가적인 컨텍스트를 제공하지 못한다.\n2. Related Work CLAP 이후로 여러 연구가 CLAP의 성능을 향상시키기 위해 노력했다. Wu 등은 CLAP을 630k 오디오-캡션 쌍으로 확장했고, Elizade 등은 4.6M 오디오-캡션 쌍까지 데이터를 확장하고 음성 샘플도 훈련에 포함시켰다. Ghosh 등은 오직 공개 도메인 데이터만 사용하여 660k 쌍으로 CompA-CLAP을 구축했다. CLAP은 텍스트-오디오 생성, 오디오 캡셔닝, 오디오 채팅 모델 등 다양한 기초 오디오 처리 작업의 오디오나 텍스트 백본으로도 활용되고 있다.\n3. Proposed Method 논문에서는 CLAP을 사용한 제로샷 오디오 분류(ZSAC)를 개선하기 위한 간단하면서도 효과적인 접근법을 제안한다:\nReCLAP: 캡션 증강을 통해 훈련된 CLAP 모델이다. 구체적으로 각 오디오에 연결된 캡션을 LLM을 사용해 다양한 방식으로 재작성한다. 각 재작성은 소리를 고유한 방식으로 설명하며, 원래 핵심 개념과 의미를 보존하면서 문장 구조와 어휘에 다양성을 보인다. 훈련 중에는 원본 캡션(40% 확률)이나 재작성된 버전 중 하나를 선택한다.\n프롬프트 증강: ZSAC를 위한 프롬프트 생성 방법으로, 단순한 템플릿 프롬프트를 넘어 각 카테고리에 맞는 커스텀 프롬프트를 생성한다.\n각 카테고리 레이블의 소리를 t개의 고유한 음향적 특성에 초점을 맞춰 설명하도록 LLM에 지시한다. 이러한 설명을 사용하여 다양한 장면에서 소리 이벤트를 배치하는 n개의 다른 캡션을 생성한다. 각 카테고리마다 n×t 총 프롬프트 풀에서 N개의 고유 프롬프트를 무작위로 샘플링한다.\n4. Experiments 데이터셋 및 모델 설정: ReCLAP은 Sound-VECaps와 CompA-660k 등 여러 데이터셋을 포함하는 약 2.3M 오디오-캡션 쌍 컬렉션에서 처음부터 훈련되었다. ZSAC를 위해 AudioSet, ESC-50, FSD50k, NSynth, TUT-Urban, UrbanSound8K, VGGSound 등 다양한 데이터셋에서 평가했다. 모델 아키텍처로는 CompA-CLAP과 동일하게 T5 large 텍스트 인코더와 HTSAT base 오디오 인코더를 사용했다.\n주요 결과: ReCLAP은 AudioCaps와 Clotho에서 텍스트-오디오 및 오디오-텍스트 검색에서 대부분의 경우 최첨단 성능을 보였다. 프롬프트 증강 방법은 일관되게 베이스라인을 능가했으며, 0.6%-54.8%의 성능 향상을 보였다. ReCLAP은 프롬프트 증강으로 0.9%-17.5% 성능 향상을 보였다. 하이퍼파라미터 실험에서 최적의 성능은 N=2(커스텀 프롬프트 수)와 p=0.4(원본 캡션 선택 확률)에서 달성되었다.\n5. Conclusion \u0026 Limitation 결론: 논문에서는 소리의 설명적 특성을 사용하여 제로샷 오디오 분류를 개선하는 방법을 제안했다. ReCLAP은 재작성된 캡션 증강을 사용하여 훈련되어 오디오-텍스트 검색 및 ZSAC에서 모든 베이스라인을 능가했다. 프롬프트 증강으로 표준 템플릿 프롬프트 대신 각 카테고리별 커스텀 프롬프트를 생성하여 ZSAC 성능을 더욱 향상시켰다.\n한계점: LLM으로 생성된 증강은 오류나 반복적인 캡션을 초래할 수 있으며, 상당한 인적 감독이 필요하다. LLM의 합성 증강은 모델에 편향을 도입할 수 있다. ReCLAP의 표현은 오디오 생성과 이해를 포함한 다양한 작업을 개선하는 데 사용될 수 있으나 이는 미래 연구 과제로 남아있다.\n6. 레퍼런스 Elizalde, B., Deshmukh, S., Al Ismail, M., \u0026 Wang, H. (2023). “CLAP learning audio concepts from natural language supervision.” ICASSP 2023. Ghosh, S., Seth, A., Kumar, S., et al. (2024). “CompA-CLAP: Addressing the gap in compositional reasoning in audio-language models.” ICLR 2024. Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., \u0026 Dubnov, S. (2023). “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.” ICASSP 2023. Silva, A., Whitehead, S., Lengerich, C., \u0026 Leather, H. J. (2023). “CoLLAT: On adding fine-grained audio understanding to language models using token-level locked-language tuning.” NeurIPS 2023. Guzhov, A., Raue, F., Hees, J., \u0026 Dengel, A. (2022). “AudioCLIP: Extending CLIP to image, text and audio.” ICASSP 2022.\n","wordCount":"571","inLanguage":"en","datePublished":"2025-05-16T00:00:00Z","dateModified":"2025-05-16T00:00:00Z","author":{"@type":"Person","name":"5biwan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://russellgeum.github.io/research/2025-05-16/"},"publisher":{"@type":"Organization","name":"5biwan's BLOG","logo":{"@type":"ImageObject","url":"https://russellgeum.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5biwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5biwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#데이터셋-및-모델-설정>데이터셋 및 모델 설정:</a></li><li><a href=#주요-결과>주요 결과:</a></li></ul><ul><li><a href=#결론>결론:</a></li><li><a href=#한계점>한계점:</a></li></ul></nav></div></details></div><div class=post-content><h1 id=1-motivation>1. Motivation<a hidden class=anchor aria-hidden=true href=#1-motivation>#</a></h1><p>CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만,
여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:</p><p>대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다.
훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 &ldquo;Sound of a toothbrush"로 훈련된 모델이 ESC50 데이터셋의 &ldquo;brushing teeth"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다.
ZSAC용 수작업 프롬프트의 한계: 현재 ZSAC 설정은 데이터셋 카테고리 레이블에 직접 대응하는 수작업 프롬프트에 의존한다. 이러한 프롬프트는 레이블 자체를 넘어 추가적인 컨텍스트를 제공하지 못한다.</p><h1 id=2-related-work>2. Related Work<a hidden class=anchor aria-hidden=true href=#2-related-work>#</a></h1><p>CLAP 이후로 여러 연구가 CLAP의 성능을 향상시키기 위해 노력했다. Wu 등은 CLAP을 630k 오디오-캡션 쌍으로 확장했고, Elizade 등은 4.6M 오디오-캡션 쌍까지 데이터를 확장하고 음성 샘플도 훈련에 포함시켰다. Ghosh 등은 오직 공개 도메인 데이터만 사용하여 660k 쌍으로 CompA-CLAP을 구축했다. CLAP은 텍스트-오디오 생성, 오디오 캡셔닝, 오디오 채팅 모델 등 다양한 기초 오디오 처리 작업의 오디오나 텍스트 백본으로도 활용되고 있다.</p><h1 id=3-proposed-method>3. Proposed Method<a hidden class=anchor aria-hidden=true href=#3-proposed-method>#</a></h1><p>논문에서는 CLAP을 사용한 제로샷 오디오 분류(ZSAC)를 개선하기 위한 간단하면서도 효과적인 접근법을 제안한다:</p><p><strong>ReCLAP</strong>: 캡션 증강을 통해 훈련된 CLAP 모델이다. 구체적으로 각 오디오에 연결된 캡션을 LLM을 사용해 다양한 방식으로 재작성한다.
각 재작성은 소리를 고유한 방식으로 설명하며, 원래 핵심 개념과 의미를 보존하면서 문장 구조와 어휘에 다양성을 보인다.
훈련 중에는 원본 캡션(40% 확률)이나 재작성된 버전 중 하나를 선택한다.</p><p><strong>프롬프트 증강</strong>: ZSAC를 위한 프롬프트 생성 방법으로, 단순한 템플릿 프롬프트를 넘어 각 카테고리에 맞는 커스텀 프롬프트를 생성한다.</p><p>각 카테고리 레이블의 소리를 t개의 고유한 음향적 특성에 초점을 맞춰 설명하도록 LLM에 지시한다.
이러한 설명을 사용하여 다양한 장면에서 소리 이벤트를 배치하는 n개의 다른 캡션을 생성한다.
각 카테고리마다 n×t 총 프롬프트 풀에서 N개의 고유 프롬프트를 무작위로 샘플링한다.</p><h1 id=4-experiments>4. Experiments<a hidden class=anchor aria-hidden=true href=#4-experiments>#</a></h1><h2 id=데이터셋-및-모델-설정>데이터셋 및 모델 설정:<a hidden class=anchor aria-hidden=true href=#데이터셋-및-모델-설정>#</a></h2><p>ReCLAP은 Sound-VECaps와 CompA-660k 등 여러 데이터셋을 포함하는 약 2.3M 오디오-캡션 쌍 컬렉션에서 처음부터 훈련되었다.
ZSAC를 위해 AudioSet, ESC-50, FSD50k, NSynth, TUT-Urban, UrbanSound8K, VGGSound 등 다양한 데이터셋에서 평가했다.
모델 아키텍처로는 CompA-CLAP과 동일하게 T5 large 텍스트 인코더와 HTSAT base 오디오 인코더를 사용했다.</p><h2 id=주요-결과>주요 결과:<a hidden class=anchor aria-hidden=true href=#주요-결과>#</a></h2><p>ReCLAP은 AudioCaps와 Clotho에서 텍스트-오디오 및 오디오-텍스트 검색에서 대부분의 경우 최첨단 성능을 보였다.
프롬프트 증강 방법은 일관되게 베이스라인을 능가했으며, 0.6%-54.8%의 성능 향상을 보였다.
ReCLAP은 프롬프트 증강으로 0.9%-17.5% 성능 향상을 보였다.
하이퍼파라미터 실험에서 최적의 성능은 N=2(커스텀 프롬프트 수)와 p=0.4(원본 캡션 선택 확률)에서 달성되었다.</p><h1 id=5-conclusion--limitation>5. Conclusion & Limitation<a hidden class=anchor aria-hidden=true href=#5-conclusion--limitation>#</a></h1><h2 id=결론>결론:<a hidden class=anchor aria-hidden=true href=#결론>#</a></h2><p>논문에서는 소리의 설명적 특성을 사용하여 제로샷 오디오 분류를 개선하는 방법을 제안했다.
ReCLAP은 재작성된 캡션 증강을 사용하여 훈련되어 오디오-텍스트 검색 및 ZSAC에서 모든 베이스라인을 능가했다.
프롬프트 증강으로 표준 템플릿 프롬프트 대신 각 카테고리별 커스텀 프롬프트를 생성하여 ZSAC 성능을 더욱 향상시켰다.</p><h2 id=한계점>한계점:<a hidden class=anchor aria-hidden=true href=#한계점>#</a></h2><p>LLM으로 생성된 증강은 오류나 반복적인 캡션을 초래할 수 있으며, 상당한 인적 감독이 필요하다.
LLM의 합성 증강은 모델에 편향을 도입할 수 있다.
ReCLAP의 표현은 오디오 생성과 이해를 포함한 다양한 작업을 개선하는 데 사용될 수 있으나 이는 미래 연구 과제로 남아있다.</p><h1 id=6-레퍼런스>6. 레퍼런스<a hidden class=anchor aria-hidden=true href=#6-레퍼런스>#</a></h1><p>Elizalde, B., Deshmukh, S., Al Ismail, M., & Wang, H. (2023). &ldquo;CLAP learning audio concepts from natural language supervision.&rdquo; ICASSP 2023.
Ghosh, S., Seth, A., Kumar, S., et al. (2024). &ldquo;CompA-CLAP: Addressing the gap in compositional reasoning in audio-language models.&rdquo; ICLR 2024.
Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., & Dubnov, S. (2023). &ldquo;Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.&rdquo; ICASSP 2023.
Silva, A., Whitehead, S., Lengerich, C., & Leather, H. J. (2023). &ldquo;CoLLAT: On adding fine-grained audio understanding to language models using token-level locked-language tuning.&rdquo; NeurIPS 2023.
Guzhov, A., Raue, F., Hees, J., & Dengel, A. (2022). &ldquo;AudioCLIP: Extending CLIP to image, text and audio.&rdquo; ICASSP 2022.</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on x" href="https://x.com/intent/tweet/?text=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds&amp;url=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f&amp;title=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds&amp;summary=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds&amp;source=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f&title=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on whatsapp" href="https://api.whatsapp.com/send?text=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds%20-%20https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on telegram" href="https://telegram.me/share/url?text=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds&amp;url=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5b%eb%85%bc%eb%ac%b8%5d%20ReCLAP%3a%20Improving%20Zero%20Shot%20Audio%20Classification%20by%20Describing%20Sounds&u=https%3a%2f%2frussellgeum.github.io%2fresearch%2f2025-05-16%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>