<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Oppenheimer&#39;s BLOG</title>
    <link>https://russellgeum.github.io/</link>
    <description>Recent content on Oppenheimer&#39;s BLOG</description>
    <image>
      <title>Oppenheimer&#39;s BLOG</title>
      <url>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 04 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://russellgeum.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[논문] Survey: Large Multimodal Models</title>
      <link>https://russellgeum.github.io/posts/2024-02-04/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2024-02-04/</guid>
      <description>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에는 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장</description>
    </item>
    <item>
      <title>[논문] Survey: Efficient Large Language Models</title>
      <link>https://russellgeum.github.io/posts/2024-01-07/</link>
      <pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2024-01-07/</guid>
      <description>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023</description>
    </item>
    <item>
      <title>[생각] 2023년 회고</title>
      <link>https://russellgeum.github.io/posts/2023-12-11/</link>
      <pubDate>Mon, 11 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-12-11/</guid>
      <description>1. 졸업과 취업 졸업 (1분기) 석사 디펜스를 끝내고, 완전히 학교를 떠났다. 포항과 서울을 2주마다 오가곤 했으니, 타지에서 지내는 외로움이 컸다. 그렇지만 개개인 퍼포먼스가 훌륭한 연구실 친구들은 나에게 큰 자산이다. 연구적으로 디스커션도 많이하고, 같은 업계에 있으니 서로 동향을 알기 좋은 사람들이다.
일전에도 말한바 있지만, 학위로 얻은 것은 다음과 같이 정리한다.
내가 무엇을 공부를 하든 스스로 커리큘럼을 설계하고, 학습할 수 있는 능력 체계적, 논리적으로 고민하고 그 결과를 계층적 구조로 작문하는 능력 나는 연구가 잘 풀린 사람은 아니다.</description>
    </item>
    <item>
      <title>[생각] 재능있는 척 하지 않기</title>
      <link>https://russellgeum.github.io/posts/2023-11-12/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-11-12/</guid>
      <description>재능있는 척 하지 않기 재능 있는 척 하지 않기
이 글은 브랜치의 &amp;ldquo;향로&amp;quot;님의 글을 보고 나에게 대입하여 재정리하였다.
요약하자면 작가는 개발을 잘 하지 못했던 시절에,
따로 공부했다는 사실을 주변에 알리고 싶지 않아했다.
왜냐하면 못한 성과에 대해서 재능이 없는 것이 아니라,
노력이 부족했다는 면피성 명분을 만들수 있기 때문이었다.
나의 이야기 올 여름~가을 나는 회사에서 (개인적으로 스스로가 너무 절망적이었던)
퍼포먼스가 너무 좋지 못한 태스크를 수행중이었다.
나는 &amp;ldquo;내가 잘 모른다. 어렵다. 그러니 나를 도와주었으면 좋겠다.</description>
    </item>
    <item>
      <title>[논문] ICCV 2023 관심 분야 논문</title>
      <link>https://russellgeum.github.io/posts/2023-10-03/</link>
      <pubDate>Tue, 03 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-10-03/</guid>
      <description>ICCV 2023 ICCV 2023 Link
Papers ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.
(일부 특이한 연구도 포함)
Neural Radiance Fields NeRF-MS: Neural Radiance Fields with Multi-Sequence
Peihao Li et al.
Re-ReND: Real-time Rendering of NeRFs across Devices
Sara Rojas et al.
CLNeRF: Continual Learning Meets NeRF
Zhipeng Cai, Matthias Muller
Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
Hansheng Chen et al.
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models Compression</title>
      <link>https://russellgeum.github.io/posts/2023-08-29/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-29/</guid>
      <description>대규모 언어 모델의 경량화 동향 Abstract LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬 LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함 Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌 압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함 Introduction &amp;amp; Method 대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다.</description>
    </item>
    <item>
      <title>[논문] Survey: Large Language Models</title>
      <link>https://russellgeum.github.io/posts/2023-08-28/</link>
      <pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-28/</guid>
      <description>LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models</description>
    </item>
    <item>
      <title>[생각] 나의 문제점</title>
      <link>https://russellgeum.github.io/posts/2023-08-12/</link>
      <pubDate>Sat, 12 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-08-12/</guid>
      <description>나의 문제점 0. 왜 이 글을 쓰는가? 짧고, 긴 시간 동안 나하고 같이 지낸 소중한 사람들로부터 몇 가지 피드백과 조언을 얻을 때가 있다. 그러한 조언 중 공통된 문장을 정리해서 내가 어떤 단점이 있는지 생각하고 이를 개선할 방법을 고민한다.
1. 나는 다른 사람에게 관심이 없다. 나는 다른 사람에게 관심이 없다. 이것은 타고나는 성향일 가능성이 크다. 갑자기 남에게 관심을 많이 가지라고 하는 것은 고문에 가깝다. 그렇지만 다른 사람과의 지속적인 커뮤니케이션을 위한다면, &amp;ldquo;관심있는 척&amp;rdquo; 이라도 해본다.</description>
    </item>
    <item>
      <title>[독서] 최재천의 공부</title>
      <link>https://russellgeum.github.io/posts/2023-07-29/</link>
      <pubDate>Sat, 29 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-29/</guid>
      <description>어떻게 배우며 살 것인가? 이 책에 담긴 최재천 교수와 안희경 저널리스트 간의 대화록은 두 가지 관점으로 이야기가 흐른다. 하나는 &amp;ldquo;개인 차원에서의 학습&amp;rdquo; 이고, 둘째는 &amp;ldquo;사회 차원에서의 교육&amp;rdquo; 이다. 사회 차원에서의 교육도 겉으로는 &amp;ldquo;어떻게 가르칠까?&amp;rdquo; 의 미래적 고민을 담지만, 그렇다고 명확한 교수법을 딱 가이드라인을 세울 수는 없다. 하지만 어느정도 적절한 방법은 있을 수 있다. 그 중 저자가 추구하는 방법은 여러 사람의 생각과 경험이 한데 섞여, 어울려 학습하는 현장을 만드는 것이다.
하지만 올바른 교육법도 교육 받는 주체가 &amp;ldquo;바른 학습&amp;rdquo; 이 가능하다는 가정에서, 어떻게 그것을 지속할지의 해답에 불과하다고 생각한다.</description>
    </item>
    <item>
      <title>[논문] ICML 2023 추론 최적화 관련 리스트</title>
      <link>https://russellgeum.github.io/posts/2023-07-25/</link>
      <pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-25/</guid>
      <description>ICML 2023 Papers ICML 2023이 열리고 있다.
Distillation, Quantization, HW-aware Deep Learning 위주로 트래킹 중이다.
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models
Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan
DIVISION: Memory Efficient Training via Dual Activation Precision
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu
Fast Private Kernel Density Estimation via Locality Sensitive Quantization
Tal Wagner, Yonatan Naamad, Nina Mishra</description>
    </item>
    <item>
      <title>[개발] Hugo 테마에서 마크다운 텍스트 양쪽 정렬</title>
      <link>https://russellgeum.github.io/posts/2023-07-24/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-24/</guid>
      <description>Hugo 텍스트 양쪽 정렬 기본적으로 마크다운 문법은 텍스트 양쪽 정렬을 지원하지 않는다.
다만 .scss 파일에 몇 줄 코드 추가로 강제 양쪽 정렬을 할 수 있다.
먼저 아래의 경로로 들어가자.
&amp;lt;blog folder&amp;gt;/assets/themes/_main.scss &amp;lt;blog folder&amp;gt;/assets/themes/_markdown.scss 그리고 아래의 코드를 추가하여 저장한다.
// 글 양쪽 정렬 p { text-align: justify; word-break: break-all; } 다시 리빌드를 하면 텍스트 양쪽 정렬이 된 것을 확인할 수 있다.</description>
    </item>
    <item>
      <title>[논문] Pruning vs Quantization: Which is Better?</title>
      <link>https://russellgeum.github.io/posts/2023-07-23/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-23/</guid>
      <description>Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motiviation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다.</description>
    </item>
    <item>
      <title>[생각] 나의 자아상</title>
      <link>https://russellgeum.github.io/posts/2023-07-21/</link>
      <pubDate>Fri, 21 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-21/</guid>
      <description>의미박탈자와 의미부여자 얼마 전, 모임의 어느 전문가 분에게 짤막한 자아상 분석을 받았다.
몇 가지 질문에 대하여 나는 답을 내려야 했고, 앞으로 고민할 것이 생겼다.
나는 주변을 통제하려는 성향이 있다. -&amp;gt; 왜? 내가 자아상을 엄격하게 바라보기도 한다. 무슨 의미일까? 내가 자아상을 유연하게 바라보기도 한다. 무슨 의미일까? 몇 가지 개념을 들엇다. 그 중 기억에 남는 것이 곧 제목이다.
&amp;ldquo;의미박탈자라 함은 내 인생에 득이 되지않고 거리를 둬야하는&amp;rdquo;
&amp;ldquo;의미부여자라 함는 내 인생을 더 발전시켜 빛이 나도록 해주는&amp;rdquo;</description>
    </item>
    <item>
      <title>[논문] SqueezeLLM: Dense-and-Sparse Quantization</title>
      <link>https://russellgeum.github.io/posts/2023-07-19/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-19/</guid>
      <description>Paper Link Sehoon Kim et al (UC Berkeley)
Introduction This paper proposes a Psuedo-PTQ method considering the weight distribution of LLM and outliers.
Motiviation Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. Deploying LLMs for inference has been a significant challenge due to their unprecedented resource requirements. AThis has forced existing deployment frameworks to use multi-GPU inference pipelines, or to use smaller and less performant models.</description>
    </item>
    <item>
      <title>[회고] 2023년 상반기 회고</title>
      <link>https://russellgeum.github.io/posts/2023-07-14/</link>
      <pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2023-07-14/</guid>
      <description>1. 작년 졸업을 준비할 쯤, 큰 부상을 당해 팔 수술을 하였다. 팔목에 영구 후유가 생겼는데, 지금도 키보드를 오래 잡으면 조금 시큰하다. 어쨋든 작년에는 취업을 못할 것 같아서 반년 쉬고 준비를 다시 해야할 것 같았다. (회사 지원을 폭 넓게 많이 못했다. 꼭 하고 싶은 분야의 회사로만 지원을 할 수 밖에 없었다.) 그래도 운이 좋게 원하던 회사에 합격하였고, 어느덧 재직 딱 반년 차 주니어이다.
2. (큰 이변이 없는 한) 다시 학교로 돌아가지는 않을 것 같다.</description>
    </item>
    <item>
      <title>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</title>
      <link>https://russellgeum.github.io/posts/2021-06-30/</link>
      <pubDate>Wed, 30 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-06-30/</guid>
      <description>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인</description>
    </item>
    <item>
      <title>[논문] When Does Contrastive Visual Representation Learning Work</title>
      <link>https://russellgeum.github.io/posts/2021-05-31/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-05-31/</guid>
      <description>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task </description>
    </item>
    <item>
      <title>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
      <link>https://russellgeum.github.io/posts/2021-04-01/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-04-01/</guid>
      <description>Motiviation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)</description>
    </item>
    <item>
      <title>[논문] ViViT, A Video Vision Transformer</title>
      <link>https://russellgeum.github.io/posts/2021-03-05/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-05/</guid>
      <description>Motiviation 비디오에서 temporal token을 받아, 트랜스포머에서 처리하는 방법론을 제안
ViT에서 영감을 받아, 트랜스포머가 시퀀셜한 데이터를 처리하는 것을 비디오에 적용해보는 것은 자연스러움
Contribution 트랜스포머만으로 비디오 데이터를 처리하는 프레임워크를 제안 공간 차원과 시간 차원으로 분해해서 연산하는 효율적인 방법론 regularization과 빠른 학습을 위해 어떻게 Pre-trianed 모델을 가져다 썻는지 보여줌 비디오 임베딩 ViT에서 했던 방법을 사용해서 비디오 클립을 유니폼 샘플링 후, 샘플링 프레임마다 tokenizing 다른 하나는 토큰 차원을 temporal로 확장해서 사용 세 가지 구조 모델 1</description>
    </item>
    <item>
      <title>[논문] A Simple Framework for Contrastive Learning of Visual Representations</title>
      <link>https://russellgeum.github.io/posts/2021-03-01/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/posts/2021-03-01/</guid>
      <description>용어의 정의 Pretext task: represenation learning을 위해 수행되는 태스크 Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는 문제를 푸는 것 Motivation 모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?
Related Work Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음
Generative
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음 Discriminative supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등) Contribution representation learning에서 data augmentation에 대한 체계적인 고민이 없었음</description>
    </item>
  </channel>
</rss>
