<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.124.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>5biwan's BLOG</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="5biwan's BLOG"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="5biwan's BLOG"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"5biwan's BLOG","url":"https://russellgeum.github.io/","description":"ExampleSite description","thumbnailUrl":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["https://github.com/russellgeum"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Multi-view Optimization of Local Feature Geometry</h2></header><div class=entry-content><p>Motivation 기존의 로컬 피처 디텍션은 싱글 이미지에서 이루어짐 → 에러가 누적되고 다운스트림 태스크에 악영향
Related Works 이전 논문들은 전통적인 방법이든 CNN 기반 방법이든, 싱글 뷰 이미지에서 로컬 피처 디텍션이 이루어졌다. 피처 매칭 단계에서 멀티 뷰를 고려하는 논문은 있지만, 저자가 아는 한, 더 정확한 키포인트 디텍팅을 위해 멀티뷰를 활용하는 사례는 없었다. Contribution 키포인트를 구성하는 그래프의 모든 엣지에 대해서 멀티뷰 refinement를 수행한다 이전 연구와 비슷하게 샴-네트워크와 코릴레이션 방법을 선택 파이널 플로우는 CNN, FCN을 통해 예측되어진다....</p></div><a class=entry-link aria-label="post link to [논문] Multi-view Optimization of Local Feature Geometry" href=https://russellgeum.github.io/posts/research/2021-04-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Stand-Alone Self-Attention in Vision Models</h2></header><div class=entry-content><p>Motivation 컴퓨터 비전에서 셀프 어텐션은 피처 스케일이 충분히 작아야 가능함 → 충분히 큰 피처맵에서도 셀프 어텐션 계산이 가능할까? 그리고 글로벌 어텐션은 계산량이 너무 많음 CNN이 없이 완전히 홀로 설 수 있는 셀프 어텐션 기반의 비전 모델을 제안 Related Work 이전에는 channel-wise, spatial-wise 등의 셀프 어텐션이 등장하였고, 적은 오버 헤드로 CNN 레이어 사이에 셀프 어텐션을 끼울 수 있었음 그러나 글로벌 어텐션 특성 상, 이미지 혹은 피처맵이 충분히 다운 샘플링 되어야 함 Contribution 모든 영역에서 어텐션을 계산하지 않음 → CNN의 로컬리티를 보증하면서도 어텐션을 계산할 수 있는 구조를 제안, 계산량을 줄일 수 있음 중심 픽셀을 쿼리로 두고, 그 주변 픽셀의 로컬 영역을 키와 밸류로 두어서 어텐션을 계산 Convolution STEM은 엣지 등의 정보를 파악하는 매우 중요한 요소...</p></div><a class=entry-link aria-label="post link to [논문] Stand-Alone Self-Attention in Vision Models" href=https://russellgeum.github.io/posts/research/2021-04-23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Incorporating Convolution Design into Visual Transformers</h2></header><div class=entry-content><p>Motivation 트랜스포머는 대규모 데이터셋이 있을떄 CNN 모델에 필적하는 성능을 보임.
CNN의 로칼리티, 인덕티브 바이어스를 적극 활용하는 디자인의 트랜스포머 모델을 고안할 수 있을까?
Related Works ViT는 대규모 이미지 데이터셋을 이용해서 CNN에 필적하는 성능을 보임
→ 그러나 대규모 데이터는 컴퓨터 리소스의 요구가 크고, 훈련이 오래 걸림 DeiT는 잘 학습된 대규모 CNN 모델을 티처로 두고 KD를 통해, 비전 트랜스포머 모델을 학습시키려 고함
→ 이 역시 대규모 CNN 모델을 미리 준비해야한다는 단점 트랜스포머 태생이 인덕티브 바이어스를 반영하는 것이 어렵고, 불충분한 데이터로부터의 일반화 능력이 부족함...</p></div><a class=entry-link aria-label="post link to [논문] Incorporating Convolution Design into Visual Transformers" href=https://russellgeum.github.io/posts/research/2021-04-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Skip-Convolutions for Efficient Video Processing</h2></header><div class=entry-content><p>Motivation 비디오는 정지된 이미지의 연속일수도 있고, 변화하는 이미지의 연속일수도 있다.
우리는 세상을 비디오로 인지 → 즉, 변화를 인지 → 변화를 느낀다는 건, 프레임간 차이 (residual)이 누적되면서 어느 임계를 넘어가서 알아채는 것. 이러한 동기로 몇 가지 연구들이 있다. (뉴로모픽, 이벤트 카메라, SNN 등등) 그러나 아직까지가 주류가 아님.
Related Works 기존의 비디오 처리는 픽셀 레벨의 dense prediction을 요구하는 경우가 많음 → 모든 프레임을 모델에 넣어서 연산
프레임 수가 증가할수록 연산량 오버헤드가 리니어하게 증가 → 심지어 새로운 변화가 없어도 계산을 해야만 함...</p></div><a class=entry-link aria-label="post link to [논문] Skip-Convolutions for Efficient Video Processing" href=https://russellgeum.github.io/posts/research/2021-04-02/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</h2></header><div class=entry-content><p>Motivation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)...</p></div><a class=entry-link aria-label="post link to [논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning" href=https://russellgeum.github.io/posts/research/2021-04-01/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples</h2></header><div class=entry-content><p>Motivation MoCo 구조를 비디오 도메인으로 확장
Related Works 생략
Contribution Propose temporallly adversarial learning to improve the feature representation of the encoder
ConvLSTM을 통해 프레임 마스크를 출력 → Discriminator(encoder)를 통해
쿼리 피처와 프레임 피처를 출력 → 프레임이 같으면 0, 마스킹된 것은 차이가 최대
마스킹 프레임의 피처를 잘 배울 수 있도록 이 차이가 최대가 되도록 학습
Propose temporal decay to reduce the effect from historical keys in the memory queyes during contrastive learning...</p></div><a class=entry-link aria-label="post link to [논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples" href=https://russellgeum.github.io/posts/research/2021-03-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] CvT, Introducing Convolution to Vision Transformer</h2></header><div class=entry-content><p>Motivation 트랜스포머를 적용한 비전 모델은 더 적은 데이터로 학습하고 비슷한 사이즈의 ResNet보다 성능이 낮음. 그 이유는 비전 태스크에서 CNN이 가지는 장점을 ViT는 활용할 수 없음. 이미지는 pixel간 local correlation이 있고 CNN은 이걸 잘 잡아내는데, ViT는 이 능력이 부족함. 이러한 CNN의 local correlation에는 shift, scale, distortion invariance가 있음
Related Works 생략
Contribution 가장 큰 핵심은 트랜스포머의 MLP를 컨볼루션으로 대체한 것
Convolutional Token Embedding Layer
이전의 CvT 출력을 입력으로 받아서, 새로운 토큰을 만드는 함수 f를 정의....</p></div><a class=entry-link aria-label="post link to [논문] CvT, Introducing Convolution to Vision Transformer" href=https://russellgeum.github.io/posts/research/2021-03-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows</h2></header><div class=entry-content><p>Motivation 이 논문은 일반적인 컴퓨터 비전에서의 트랜스포머 백본을 제공하기 위함이다. 자연어처리에서의 트랜스포머가 비전으로 옮겨올 때, 두 도메인에서의 차이가 있었다.
하나는 비주얼 객체의 다양한 바리에이션이고 단어와 비교해서 이미지의 높은 해상도가 문제이다. → 이미지나 이미지 패치에 직접 트랜스포머를 적용하면 계산량이 쿼드라틱하게 증가한다.
Related Works 작년 10월, 구글의 ViT는 비전 태스크에 컴퓨터 비전 분야는 CNN가 지배적임. AlexNet부터 더 크고, 다양하고, 정교한 기술들로 CNN backbone들이 발전함. 한편 자연어 분야는 트랜스포머가 지배적 → 트랜스포머는 데이터의 long range dependency를 잘 반영함 (언어의 특징)....</p></div><a class=entry-link aria-label="post link to [논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows" href=https://russellgeum.github.io/posts/research/2021-03-15/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] RAFT, Recurrent All Pairs Feild Transforms for Optical Flow</h2></header><div class=entry-content><p>Motivation 옵티컬 플로우 문제의 정의: 비디오 프레임에서 픽셀 레벨로 모션을 추정하는 것이다.
옵티컬 플로우는 Occlusion, motion blur, textureless surfaces 등에서 어렵다.
옵티컬 플로우 최적화 문제는 두 개의 항으로 구성되어 있다.
Data term Regularization term 이 둘에는 trade-off가 있다. Visual similarity에 기여하는 data가 문제인가? Prior를 강하게 부과하는 regularization이 문제인가?
Related Works Supervised Optical Estimation에서 가장 중요한 모델은 FlowNet, FlowNet 2.0
옵티컬 플로우 연구의 중요한 기술은 Pyramid, Warping, Cost Volume
Pyramid
뎁스, 플로우 Large motion을 다룰려면 큰 이미지에서는 픽셀간 거리가 멀다....</p></div><a class=entry-link aria-label="post link to [논문] RAFT, Recurrent All Pairs Feild Transforms for Optical Flow" href=https://russellgeum.github.io/posts/research/2021-03-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Representation Learning with Convtrastive Predictive Coding</h2></header><div class=entry-content><p>Motivation Contrastive Learning은 latent space으로부터 downstream task에 유용하게 쓰일 정보를 최대한 뽑아낸다. Contrastive Learning은 여러 태스크에서 좋은 성능을 보일 수 있다. 특히 Predictive Coding과 함께히면 더 좋다. 이 논문의 중요한 직관은 signal의 서로 다른 부분 사이에서 공유되는 정보를 인코딩하여 representation learning을 하는 것이다. 고차원 데이터를 예측할 때, MSE나 CE같은 로스는 적절하지 못하다. 그리고 강력한 조건부적인 생성 모델이 필요한데, 데이터의 모든 디테일을 생성해야하는 특성 상, 계산량 오버헤드가 너무 커서 부당이 된다. 여러가지 이유로 x, c 사이의 p(x|c) 방식의 모델링은 상호간 정보를 알기에는 최적이 아니다....</p></div><a class=entry-link aria-label="post link to [논문] Representation Learning with Convtrastive Predictive Coding" href=https://russellgeum.github.io/posts/research/2021-03-07/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/page/5/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/page/7/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>