<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.124.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Oppenheimer's BLOG</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Oppenheimer's BLOG"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Oppenheimer's BLOG"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Oppenheimer's BLOG","url":"https://russellgeum.github.io/","description":"ExampleSite description","thumbnailUrl":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["https://github.com/russellgeum"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations</h2></header><div class=entry-content><p>Motivation ViT, MLP 믹서가 어떤 경우에 레즈넷의 성능을 능가할 수 있을까? 의 고찰
ViT, MLP 믹서는 라지 스케일 트레이닝이나, 강한 데이터 arguments를 주어야 했음 모델이 인덕티브 바이어스를 포괄하기 힘들기 때문 그런데 이러한 기법 없이 레즈넷 보다 성능을 올리는 방법을 고민 Related Works 생략
Contribution ViT와 MLP 믹서의 그래디언트 필드는 매우 날카로운 로컬 미니마에 수렴한다는 것을 보여준다. (이는 레즈넷보다 몇 배 더 큼) 이러한 필드는 백프롭때 그래디언트가 누적되고, 초기 임베딩 레이어가 굉장히 큰 헤시안 행렬의 고유값을 가지면서 문제가 될 수 있음 네트워크들은 상대적으로 작은 훈련 에러를 가지고, 특히 MLP 믹서는 ViT보다 오버피팅 가능성이 있다....</p></div><a class=entry-link aria-label="post link to [논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations" href=https://russellgeum.github.io/posts/review/2021-06-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers</h2></header><div class=entry-content><p>Motivation 비디오에서 시간 차원은 공간 차원과 같은 방법으로 처리되었음.
비디오에서 물리적 위치가 t 프레임에 사영된 것과 t+k 프레임에 사영된 지점은 서로 무관할 수 있기 때문임.
Temporal correspondence는 이러한 다이나믹 장면을 학습하기 용이하게 설계되어야 함
Related Works RAFT의 옵티컬 플로우 추정은 두 프레임의 temporal corrrespondence 문제이다. RAFT의 correlation volume은 본질적으로 attention map와 같다. Allan Jabri의 연구에서는 비디오 temporal correspondnce 문제를 contrastive random walk 문제로 정의하고 해결 위와는 다르게 trajectory attention을 통해 temporal correspondece 문제를 해결 Contribution Trajectory Attention...</p></div><a class=entry-link aria-label="post link to [논문] Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers" href=https://russellgeum.github.io/posts/review/2021-06-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Long-Shot Temporal Contrastive Learning of Video Transformers</h2></header><div class=entry-content><p>Motivation 비디오 트랜스포머가 기존 CNN 기반 비디오 모델들에 비해 경쟁력있는 성능을 보임 그러나 많은 파라미터와 inductive bias 부족은 대규모 데이터셋으로 학습된 강력한 사전 모델을 요구함 Related Works 생략
Contribution TimSformer vs Swin Transformer
이 논문은 스윈 트랜스포머의 속성을 비디오 도메인으로 확장하였음
Long-Shot Temporal Contrastive Learning
템포랄 도메인의 CL을 고안
레이블링이 되지 않은 비디오 B를 입력으로 받는다. 비디오 B를 랜덤하게 숏클립과 롱클립으로 샘플링해서 나눈다. 프레임의 수는 같으나, 시간 간격이 달라서 롱클립이 더 긴 범위의 비디오 표현을 담고 있다....</p></div><a class=entry-link aria-label="post link to [논문] Long-Shot Temporal Contrastive Learning of Video Transformers" href=https://russellgeum.github.io/posts/review/2021-06-15/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문]Self-Supervised Learning of Compressed Video Representation</h2></header><div class=entry-content><p>Motivation 효율적으로 비디오 표현 학습을 하는 방법을 고민
Related Work 이전 연구들은 비디오 프레임을 프로세싱하기 전에 JPEG 같은 형태로 디코딩하여 저장하고 representation 학습을 하였음. 이것은 스토리지를 많이 요구하고, 대규모 트레이닝에 비효율적임. Decoded frame없이 학습할 수 있었지만, supervised 기반이었지, self-supervised는 관심이 덜했음
Contribution 압측된 비디오 포맷에서 직접 비디오 표현 학습을 한다. 압축된 비디오는 두 가지 고유한 특성이 있음, 일단 GOP란? MPEG 포맷을 위해 영상 프레임의 덩어리를 가리킴
GOP(Group Of Picture)
왜 압축된 비디오가 유리할까?...</p></div><a class=entry-link aria-label="post link to [논문]Self-Supervised Learning of Compressed Video Representation" href=https://russellgeum.github.io/posts/review/2021-06-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Does Contrastive Visual Representation Learning Work</h2></header><div class=entry-content><p>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task</p></div><a class=entry-link aria-label="post link to [논문] When Does Contrastive Visual Representation Learning Work" href=https://russellgeum.github.io/posts/review/2021-05-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal</h2></header><div class=entry-content><p>Motivation Video Instance Segmentation 문제는 동시에 classify, segment, track을 하는 것이다. 이 태스크는 프레임 레벨 VIS보다 성능이 좋다. 그러나 리얼 타임이 아니다. VisTR이 이 문제를 해결하려 했으나, 훈련 시간이 길었다. 그리고 hand-crafted data association이 많이 필요해서 비효율적이다.
Related Works 프레임 레벨 VIS tracking by segmentation 방법 복잡한 data association 알고리즘이 필요 temporal context를 추출하는게 한계가 있음 object occlusion을 핸들링하지 못함 클립 레벨 VIS clip by clip으로 segmentation and tracking 프레임 레벨 VIS보다 long range temporal context를 추출 가능 그러나 실시간성이 부족해서 속도가 느림 Contribution EfficientVIS...</p></div><a class=entry-link aria-label="post link to [논문] Efficient Vide Instance Segmentation via Tracklet Query and Proposal" href=https://russellgeum.github.io/posts/review/2021-05-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation using Space-Time Memory Networks</h2></header><div class=entry-content><p>Motivation 준지도 학습에서 문제에 따라, 중간단계 예측과 함께 사용 가능한 정보는 풍부하다. 기존의 방법에서는 이러한 아이디어를 활용할 수 없었다. 논문은 메모리 네트워크를 통해, 가능한 모든 소수로부터 관련된 정보를 읽어 학습하고자 한다. 과거 프레임과 마스크가 외부 메모리를 형성하고, 현재 프레임이 쿼리로서 메모리 속 마스크 정보를 사용하여 세그멘테이션을 수행함 구체적으로 쿼리와 메모리는 피처 스페이스에서 매칭이 된다. (모든 space-time 픽셀 지점에서) Related Works 이전 프레임에서 형상을 추출하고 전파하는 방식 외관 변화에 더 잘 대처하나, 오클루션이나 에러 드리프트의 러버스트가 낮을 수 있다....</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation using Space-Time Memory Networks" href=https://russellgeum.github.io/posts/review/2021-05-12/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Multi-view Optimization of Local Feature Geometry</h2></header><div class=entry-content><p>Motivation 기존의 로컬 피처 디텍션은 싱글 이미지에서 이루어짐 → 에러가 누적되고 다운스트림 태스크에 악영향
Related Works 이전 논문들은 전통적인 방법이든 CNN 기반 방법이든, 싱글 뷰 이미지에서 로컬 피처 디텍션이 이루어졌다. 피처 매칭 단계에서 멀티 뷰를 고려하는 논문은 있지만, 저자가 아는 한, 더 정확한 키포인트 디텍팅을 위해 멀티뷰를 활용하는 사례는 없었다. Contribution 키포인트를 구성하는 그래프의 모든 엣지에 대해서 멀티뷰 refinement를 수행한다 이전 연구와 비슷하게 샴-네트워크와 코릴레이션 방법을 선택 파이널 플로우는 CNN, FCN을 통해 예측되어진다....</p></div><a class=entry-link aria-label="post link to [논문] Multi-view Optimization of Local Feature Geometry" href=https://russellgeum.github.io/posts/review/2021-04-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Stand-Alone Self-Attention in Vision Models</h2></header><div class=entry-content><p>Motivation 컴퓨터 비전에서 셀프 어텐션은 피처 스케일이 충분히 작아야 가능함 → 충분히 큰 피처맵에서도 셀프 어텐션 계산이 가능할까? 그리고 글로벌 어텐션은 계산량이 너무 많음 CNN이 없이 완전히 홀로 설 수 있는 셀프 어텐션 기반의 비전 모델을 제안 Related Work 이전에는 channel-wise, spatial-wise 등의 셀프 어텐션이 등장하였고, 적은 오버 헤드로 CNN 레이어 사이에 셀프 어텐션을 끼울 수 있었음 그러나 글로벌 어텐션 특성 상, 이미지 혹은 피처맵이 충분히 다운 샘플링 되어야 함 Contribution 모든 영역에서 어텐션을 계산하지 않음 → CNN의 로컬리티를 보증하면서도 어텐션을 계산할 수 있는 구조를 제안, 계산량을 줄일 수 있음 중심 픽셀을 쿼리로 두고, 그 주변 픽셀의 로컬 영역을 키와 밸류로 두어서 어텐션을 계산 Convolution STEM은 엣지 등의 정보를 파악하는 매우 중요한 요소...</p></div><a class=entry-link aria-label="post link to [논문] Stand-Alone Self-Attention in Vision Models" href=https://russellgeum.github.io/posts/review/2021-04-23/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Incorporating Convolution Design into Visual Transformers</h2></header><div class=entry-content><p>Motivation 트랜스포머는 대규모 데이터셋이 있을떄 CNN 모델에 필적하는 성능을 보임.
CNN의 로칼리티, 인덕티브 바이어스를 적극 활용하는 디자인의 트랜스포머 모델을 고안할 수 있을까?
Related Works ViT는 대규모 이미지 데이터셋을 이용해서 CNN에 필적하는 성능을 보임
→ 그러나 대규모 데이터는 컴퓨터 리소스의 요구가 크고, 훈련이 오래 걸림 DeiT는 잘 학습된 대규모 CNN 모델을 티처로 두고 KD를 통해, 비전 트랜스포머 모델을 학습시키려 고함
→ 이 역시 대규모 CNN 모델을 미리 준비해야한다는 단점 트랜스포머 태생이 인덕티브 바이어스를 반영하는 것이 어렵고, 불충분한 데이터로부터의 일반화 능력이 부족함...</p></div><a class=entry-link aria-label="post link to [논문] Incorporating Convolution Design into Visual Transformers" href=https://russellgeum.github.io/posts/review/2021-04-20/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/page/3/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/page/5/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>