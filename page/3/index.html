<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.124.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>5biwan's BLOG</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="5biwan's BLOG"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="5biwan's BLOG"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"5biwan's BLOG","url":"https://russellgeum.github.io/","description":"ExampleSite description","thumbnailUrl":"https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["https://github.com/russellgeum"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] SOFT: Softmax-free Transformer with Linear Complexity</h2></header><div class=entry-content><p>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -> 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?...</p></div><a class=entry-link aria-label="post link to [논문] SOFT: Softmax-free Transformer with Linear Complexity" href=https://russellgeum.github.io/research/2021-10-31/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Non Deep Networks</h2></header><div class=entry-content><p>Motivation DNN의 깊이가 깊어지면 단점이 많음 → 레이턴시가 길어지기 때문에 빠른 반응을 필요로 하는 애플리케이션이 부적합 어떻게 하면 얕은 깊이의 DNN으로도 충분한 성능을 낼 수 있을까? → 해답은 패러렐한 뉴럴넷 구성으로 성능을 낼 수 있다. Related Works 생략
Contribution 구체적으로 ~10 레이어, ~12 레이어까지 적절함을 말한다.
VGG 스타일의 블록을 사용한다. (구체적으로 Rep-VGG을 빌리지만, 목적에 맞게 조금 수정)
제한된 네트워크 깊이로 receptive field가 좁다. 이를 해결하기 위해, Squeeze-Exicitation 레이어에 기반한 SSE 레이어를 추가하였다....</p></div><a class=entry-link aria-label="post link to [논문] Non Deep Networks" href=https://russellgeum.github.io/research/2021-10-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation with Compressed Video</h2></header><div class=entry-content><p>Motivation 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
Related Works 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 Contribution 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함...</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation with Compressed Video" href=https://russellgeum.github.io/research/2021-08-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Contextual Transformer Networks for Visual Recognition</h2></header><div class=entry-content><p>Motivation 비전 태스크에서 셀프 어텐션의 계산이, 즉 공간적인 위치에서 Q, K가 서로 independent하게 계산이 되어지는 것이 단점 → context가 필요
Related Works CNN의 receptive field를 넓히는 것 → context를 잘 보긴 하지만, long range dependecy를 보지 못함 ViT, long range dependency를 보기는 하지만, independent한 Q, K의 interaction을 계산 Contribution 기존의 conventional self-attention은 서로 다른 위치간의 interaction을 잘 계산. 그러나 모든 pairwise Q-K relation은 independent함 → 풍부한 context를 보지 못함, 따라서 Conetxt Transformer 구조를 제안....</p></div><a class=entry-link aria-label="post link to [논문] Contextual Transformer Networks for Visual Recognition" href=https://russellgeum.github.io/research/2021-07-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</h2></header><div class=entry-content><p>Motivation 너프의 스태틱 가정을 깨고 space-time 형태의 다이나믹 비디오에서 NVS를 하고자 함
Related Work Novel View Synthesis
NeRF는 static scene임 (멈춰 있는 한 장면에서 MVS로 찍은 카메라 가지고 NVS) Novel Time Synthesis
Temporal synthesis는 가능했지만, Space synthesis는 하지 않음 Space-Time synthesis
Static 장면을 다루거나, 복잡한 기하적 관계를 풀지 못함 필요에 따라 사람의 라벨링이 요구되는 경우도 있음 Contribution NeRF와는 달리, 다이나믹 장면은 temporal domain을 포함한다. 따라서 비디오 프레임의 i도 포지션으로 입력하면 i → i+1, i-1의 scene flow [f, f’]가 출력을 하게끔 MLP 모델 디자인...</p></div><a class=entry-link aria-label="post link to [논문] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes" href=https://russellgeum.github.io/research/2021-06-30/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations</h2></header><div class=entry-content><p>Motivation ViT, MLP 믹서가 어떤 경우에 레즈넷의 성능을 능가할 수 있을까? 의 고찰
ViT, MLP 믹서는 라지 스케일 트레이닝이나, 강한 데이터 arguments를 주어야 했음 모델이 인덕티브 바이어스를 포괄하기 힘들기 때문 그런데 이러한 기법 없이 레즈넷 보다 성능을 올리는 방법을 고민 Related Works 생략
Contribution ViT와 MLP 믹서의 그래디언트 필드는 매우 날카로운 로컬 미니마에 수렴한다는 것을 보여준다. (이는 레즈넷보다 몇 배 더 큼) 이러한 필드는 백프롭때 그래디언트가 누적되고, 초기 임베딩 레이어가 굉장히 큰 헤시안 행렬의 고유값을 가지면서 문제가 될 수 있음 네트워크들은 상대적으로 작은 훈련 에러를 가지고, 특히 MLP 믹서는 ViT보다 오버피팅 가능성이 있다....</p></div><a class=entry-link aria-label="post link to [논문] When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations" href=https://russellgeum.github.io/research/2021-06-25/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers</h2></header><div class=entry-content><p>Motivation 비디오에서 시간 차원은 공간 차원과 같은 방법으로 처리되었음.
비디오에서 물리적 위치가 t 프레임에 사영된 것과 t+k 프레임에 사영된 지점은 서로 무관할 수 있기 때문임.
Temporal correspondence는 이러한 다이나믹 장면을 학습하기 용이하게 설계되어야 함
Related Works RAFT의 옵티컬 플로우 추정은 두 프레임의 temporal corrrespondence 문제이다. RAFT의 correlation volume은 본질적으로 attention map와 같다. Allan Jabri의 연구에서는 비디오 temporal correspondnce 문제를 contrastive random walk 문제로 정의하고 해결 위와는 다르게 trajectory attention을 통해 temporal correspondece 문제를 해결 Contribution Trajectory Attention...</p></div><a class=entry-link aria-label="post link to [논문] Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers" href=https://russellgeum.github.io/research/2021-06-20/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] Long-Shot Temporal Contrastive Learning of Video Transformers</h2></header><div class=entry-content><p>Motivation 비디오 트랜스포머가 기존 CNN 기반 비디오 모델들에 비해 경쟁력있는 성능을 보임 그러나 많은 파라미터와 inductive bias 부족은 대규모 데이터셋으로 학습된 강력한 사전 모델을 요구함 Related Works 생략
Contribution TimSformer vs Swin Transformer
이 논문은 스윈 트랜스포머의 속성을 비디오 도메인으로 확장하였음
Long-Shot Temporal Contrastive Learning
템포랄 도메인의 CL을 고안
레이블링이 되지 않은 비디오 B를 입력으로 받는다. 비디오 B를 랜덤하게 숏클립과 롱클립으로 샘플링해서 나눈다. 프레임의 수는 같으나, 시간 간격이 달라서 롱클립이 더 긴 범위의 비디오 표현을 담고 있다....</p></div><a class=entry-link aria-label="post link to [논문] Long-Shot Temporal Contrastive Learning of Video Transformers" href=https://russellgeum.github.io/research/2021-06-15/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문]Self-Supervised Learning of Compressed Video Representation</h2></header><div class=entry-content><p>Motivation 효율적으로 비디오 표현 학습을 하는 방법을 고민
Related Work 이전 연구들은 비디오 프레임을 프로세싱하기 전에 JPEG 같은 형태로 디코딩하여 저장하고 representation 학습을 하였음. 이것은 스토리지를 많이 요구하고, 대규모 트레이닝에 비효율적임. Decoded frame없이 학습할 수 있었지만, supervised 기반이었지, self-supervised는 관심이 덜했음
Contribution 압측된 비디오 포맷에서 직접 비디오 표현 학습을 한다. 압축된 비디오는 두 가지 고유한 특성이 있음, 일단 GOP란? MPEG 포맷을 위해 영상 프레임의 덩어리를 가리킴
GOP(Group Of Picture)
왜 압축된 비디오가 유리할까?...</p></div><a class=entry-link aria-label="post link to [논문]Self-Supervised Learning of Compressed Video Representation" href=https://russellgeum.github.io/research/2021-06-10/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[논문] When Does Contrastive Visual Representation Learning Work</h2></header><div class=entry-content><p>Conclusion Contrastive Learning이 언제 유효하고, 또 언제 성능이 안 좋은지에 대해서 4가지 관점으로 고민
데이터 양, 데이터 도메인, 데이터 품질, 태스크 세분화
50만 장을 넘는 데이터 이점은 그리 많지 않음 다른 도메인으로부터 pretraining image를 추가하는 것은 general representation을 이끌어내지 않음 corrupted pretraining image → disparate impact on supervised pretraining CL lags far behind SL on fine-grained visual task</p></div><a class=entry-link aria-label="post link to [논문] When Does Contrastive Visual Representation Learning Work" href=https://russellgeum.github.io/research/2021-05-31/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/page/2/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/page/4/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>