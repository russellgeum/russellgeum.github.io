<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Technicals on 5biwan&#39;s BLOG</title>
    <link>https://russellgeum.github.io/technical/</link>
    <description>Recent content in Technicals on 5biwan&#39;s BLOG</description>
    <image>
      <title>5biwan&#39;s BLOG</title>
      <url>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 01 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://russellgeum.github.io/technical/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[개발] Mac OS에서 LibTorch 설치 및 빌드</title>
      <link>https://russellgeum.github.io/technical/2024-10-01/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-10-01/</guid>
      <description>LibTorch LibTorch는 C++ 인터페이스를 제공하는 PyTorch의 라이브러리이다. 이를 설치하면 PyTorch의 모든 기능을 사용할 수 있다.
파이썬 기반의 모델 서빙 말고, C++에서 활용 가능한 멀티스레드와 같은 기능을 사용하려면 LibTorch를 설치해야 한다.
이 작업은 CMakeLists.txt 파일을 통해 빌드하는 방식으로 진행한다. 따라서 CMake를 먼저 설치해야 한다.
CMake 설치는 인터넷에 많이 나오므로, 그 내용은 생략한다.
LibTorch 설치 먼저 PyTorch 공식 홈페이지에서 설치 가이드를 참고해 설치한다.
Locally하게 다운받아도 되지만, 터미널에서 아래 명령을 실행해도 된다. (2.4.1 버전 기준)</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (8) - 공유 메모리</title>
      <link>https://russellgeum.github.io/technical/2024-08-25/</link>
      <pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-08-25/</guid>
      <description>공유 메모리 공유 메모리 사용 방법은 크게 세 가지 케이스로 구분한다.
L1 캐시: 자주 사용되는 데이터를 직접 분류, 관리하기 어려운 경우 사용자 관리 캐시 1: 개발자가 커널 내 알고리즘의 데이터 접근 패턴을 파악 후, 직접 제어 사용자 관리 캐시 2: 자주 사용하는 데이터의 전역 메모리 접근을 줄이기 위함 스레드 간 공유 메모리와 L1 캐시 활용 방법 공유 메모리 (Shared Memory) 역할 및 특징: 공유 메모리는 각 블록 내 모든 스레드가 접근할 수 있는 고속 메모리 공간이다.</description>
    </item>
    <item>
      <title>[개발] 리눅스 개발을 위한 몇 가지 환경 구축</title>
      <link>https://russellgeum.github.io/technical/2024-08-22/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-08-22/</guid>
      <description>개요 리눅스 개발을 하다보면, 엔비디아 드라이버, 도커, CMake 등 환경을 잡을 일이 있다. GPU가 있는 환경에서 경험상 가장 유용한 방법은
NVIDIA 드라이버 설치 NVIDIA 컨테이너 툴킷 설치 Docker Hub에서 이미지 다운로드 CMake 등 여러 빌드 도구 및 패키지 설치 이 과정이면 NVIDIA GPU 환경에서 작업마다 패키지 의존성을 피하여 독립된 환경을 구축할 수 있다.
NVIDIA 드라이버 설치 GPU를 활용한다면, 엔비디아 드라이버는 필수이다. 아래 명령어로 적절한 NVIDIA 드라이버 설치 유무를 확인한다.
nvidia-smi GPU 정보가 제대로 뜨지 않는다면, 설치해야 한다.</description>
    </item>
    <item>
      <title>[기술] LLM 경량화를 위한 가이드</title>
      <link>https://russellgeum.github.io/technical/2024-08-13/</link>
      <pubDate>Tue, 13 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-08-13/</guid>
      <description>LLM Quantization 이 글은 &amp;ldquo;Maarten Grootendorst&amp;quot;의 허락을 받고 Visual Guide To Quantization 글을 간결하게 설명하였다.
대형 언어 모델(LLM)은 상용 하드웨어에서 실행하기에는 매우 크다. 이러한 모델은 수십억 개의 파라미터를 보유하며, 일반적으로 추론 속도를 높이기 위해 많은 메모리 용량을 가진 GPU가 필요하다. 따라서 점점 더 많은 연구가 이러한 모델을 더 작게 만드는 것에 초점을 맞추고 있다. 이는 개선된 학습, 어댑터 등의 방법을 통해 이루어집니다. 이 분야에서 주요한 기법 중 하나는 양자화(quantization)라고 부른다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (7) - CUDA 기반 행렬 곱셈</title>
      <link>https://russellgeum.github.io/technical/2024-07-25/</link>
      <pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-07-25/</guid>
      <description>CUDA 기반 행렬 곱셈 행렬 연산은 CUDA 연산에 가장 어울리는 문제이다. 따라서 행렬 곱셈을 GPU에서 수행하는 방법을 이해한다.
스레드 레이아웃 설정 대규모 행렬 곱을 위한 CUDA 프로그램을 위해 스레드 레이아웃을 먼저 결정해야 한다. 어떻게 레이아웃 기준을 잡아야 할까? 두 가지 경우를 생각할 수 있다.
데이터를 읽는 행렬 A, B 기준 결과가 저장되는 행렬 C 기준 C = AB 이 행렬 연산에서 C의 (row, col) 값 계산을 위해서는 A(row, k) B (k, col) 원소들을 불러와야 한다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (6) - CUDA 실행 모델</title>
      <link>https://russellgeum.github.io/technical/2024-07-16/</link>
      <pubDate>Tue, 16 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-07-16/</guid>
      <description>GPU 아키텍처 SM: 스트리밍 멀티프로세서 하나의 GPU는 SM이라는 물리적 구조를 여러 개 포함한다. SM은 여러 CUDA 코어를 가진 연산 장치다. Fermi 아키텍처는 하나의 SM에 32개의 CUDA 코어를 가지고 있다. SM에는 CUDA 코어말고 레지스터, 공유 메모리, L1 캐시 등이 포함된다.
CUDA 코어 CUDA 코어는 GPU의 가장 기본이 되는 프로세싱 유닛이다. 코어 안에는 FP 연산기, INT 연산기 등이 있으며, CUDA 프로그램의 동작 단위가 스레드이므로, 스레드 1개에 CUDA 코어 1개가 할당된다.
CUDA 스레드 계층과 GPU 하드웨어 간단한 요약 1 스레드 = 1 코어 32개 스레드가 모여 1개의 워프이다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (5) - 스레드 레이아웃</title>
      <link>https://russellgeum.github.io/technical/2024-07-03/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-07-03/</guid>
      <description>스레드 레이아웃 스레드 레이아웃 결정 앞서 CUDA 커널의 레이아웃은 그리드와 블록의 형태로 결정한다고 하였다.
구체적으로 다음의 과정을 따른다.
블록 형태 결정 (즉, 스레드를 어떻게 배치할껀지 결정) 데이터의 크기 및 블록 형태에 따라 그리드 형태 결정 블록 형태는 커널의 알고리즘 특성과 GPU 환경을 고려하여야 한다. 이때 레지스터, 공유 메모리 크기 등도 고려해야 할 요소이다.
큰 벡터의 합을 연산하는 CUDA 커널 (2) 벡터 차원이 1,024보다 크면 블록을 여러 개 지정해야 한다. 하나의 블록이었다면, 각 스레드가 벡터의 첫 번째 원소부터 담당하여 연산한다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (4) - CUDA 연산 구조</title>
      <link>https://russellgeum.github.io/technical/2024-06-29/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-06-29/</guid>
      <description>CUDA 스레드 계층 스레드 CUDA 스레드 계층에서 가장 작은 단위는 스레드이다. 따라서 CUDA 연산을 수행하거나, 코어를 사용하는 기본 단위이다. 커널 호출 시, CUDA 커널 코드는 모든 스레드에 공유된다. 각 스레드는 커널을 독립적으로 실행한다.
워프 CUDA 스레드 계층의 두 번째 계층은 워프이다. 워프는 32개 스레드를 하나로 묶은 단위이다. 중요한 점은 워프는 디바이스에서 하나의 제어 장치에 의해 제어된다. GPU의 SIMT 구조에서 멀티 스레드 단위가 바로 워프이다. 이 말은 1개의 명령어에 의해 32개 스레드가 동시에 움직이는 것을 의미한다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (3) - CPU와 GPU의 벡터 합 연산</title>
      <link>https://russellgeum.github.io/technical/2024-06-28/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-06-28/</guid>
      <description>벡터 합을 구하는 호스트 프로그램 #include &amp;lt;stdio.h&amp;gt; #inlcude &amp;lt;stdlib.h&amp;gt; #include &amp;lt;string.h&amp;gt; #define NUM_DATA 1024 int main(void) { int* a, * b, * c; int memSize = sizeof(int) * NUM_DATA a = new int[NUM_DATA]; memset(a, 0, memSize); b = new int[NUM_DATA]; memset(b, 0, memSize); c = new int[NUM_DATA]; memset(c, 0, memSize); for (int i = 0; i &amp;lt; NUM_DATA; i++) { a[i] = rand() % 10; b[i] = rand() % 10; } for (int i = 0; i &amp;lt; NUM_DATA; i++) { c[i] = a[i] + b[i]; } delete[] a; delete[] b; delete[] c; } 벡터 합을 구하는 디바이스 프로그램 #include &amp;#34;cuda_runtime.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (2) - CPU와 GPU 통신</title>
      <link>https://russellgeum.github.io/technical/2024-06-26/</link>
      <pubDate>Wed, 26 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-06-26/</guid>
      <description>호스트와 디바이스 호스트 호스트는 일반적으로 CPU를 의미한다. 따라서 호스트 코드는 CPU에서 실행되는 코드를 의미한다. 또한 호스트 메모리는 CPU가 사용하는 시스템 메모리이다. (DRAM)
디바이스 디바이스는 일반적으로 GPU를 의미한다. 따라서 디바이스 코드는 GPU에서 실행되는 코드를 의미한다. 또한 디바이스 코드는 GPU가 사용하는 GPU 메모리이다.
CUDA 프로그램 CUDA 프로그램은 호스트 코드와 디바이스 코드로 구성된다. 프로그램 실행 시 처음 호출되는 코드는 CPU에서 프로세스를 할당하기 때문에, 호스트 코드가 통상 같이 있다. CUDA 프로그램에서 호스트 코드는 gcc와 같은 컴파일러로, 디바이스 코드는 NVCC 컴파일러로 컴파일한다.</description>
    </item>
    <item>
      <title>[기술] GPU와 CUDA (1) - GPU의 연산 개념</title>
      <link>https://russellgeum.github.io/technical/2024-04-06/</link>
      <pubDate>Sat, 06 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-04-06/</guid>
      <description>GPU에 관하여 GPU는 방대한 수학 연산을 가속하기 위해 설계된 전자 회로이다. GPU는 CPU에 비해 수천 개의 작은 코어(모델 및 사용 목적에 따라 다름)를 가지고 있기 때문에 GPU 아키텍처는 병렬 처리에 최적화되어 있다. GPU는 여러 작업을 동시에 처리할 수 있으며 그래픽 및 수학적 워크로드에서 더 빠르다.
GPU vs CPU GPU vs CPU 기본적인 GPU 구조 Flynn&amp;rsquo;s Taxanomy 플린의 분류법은 스탠포드 대학교의 마이클 J. 플린이 컴퓨터 아키텍처를 분류한 것이다. 플린의 분류법의 기본 개념은 간단하다.</description>
    </item>
    <item>
      <title>[개발] Mac에서 llama.cpp를 사용하여 Orion-14B-Chat을 추론하기</title>
      <link>https://russellgeum.github.io/technical/2024-02-13/</link>
      <pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://russellgeum.github.io/technical/2024-02-13/</guid>
      <description>Orion-14B 본 포스팅은 Orion-14B-Chat을 기준으로 한다.
llama.cpp Orion-14B 모델 Orion-14B-Chat in HuggingFace 추론 환경 CMake 설치 brew install cmake llama.cpp 환경 클론 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp llama.cpp 환경 빌드 mkdir build cd build cmake .. cmake --build . --config Release Orion-14B 모델 다운로드 허깅페이스의 Orion-14B 모델을 허깅페이스 API로 로컬에 다운로드하려면 아래의 코드를 실행해야 한다.
import torch from transformers import AutoModelForCausalLM, AutoTokenizer from transformers.generation.utils import GenerationConfig tokenizer = AutoTokenizer.from_pretrained(&amp;#34;OrionStarAI/Orion-14B&amp;#34;, use_fast=False, trust_remote_code=True) model = AutoModelForCausalLM.</description>
    </item>
  </channel>
</rss>
