<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>논문 | Oppenheimer's BLOG</title><meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="논문"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="논문"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>논문
<a href=/categories/%EB%85%BC%EB%AC%B8/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Rethinking the Augmentation Module in Contrastive Learning</h2></header><div class=entry-content><p>Motivation CL은 DA에 강력하게 의존하는 방법이다. 인위적인 DA는 다음과 같은 단점이 있다. 데이터 증강의 휴리스틱한 조합은 특정적인 표현 불변성을 가져다 준다. 강력한 데이터 증강은 너무 많은 불변성을 가지고 있어서 오히려 fine-grained한 다운스트림 태스크에 적합하지 않다. 따라서 이 논문은 어디서? 무엇을? 이란 질문으로 DA를 하는 방법론을 소개한다. Related Work 생략
Contribution 다양한 augmentation module 조합을 사용한다.
샴 구조는 깊이에 따라 여러 스테이지로 활용한다. 각 스테이지의 피처를 CL에 활용한다.
Hierarchical augmentation invariance...</p></div><a class=entry-link aria-label="post link to [논문] Rethinking the Augmentation Module in Contrastive Learning" href=https://russellgeum.github.io/posts/2022-06-30/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception</h2></header><div class=entry-content><p>Motivation CL이나 특정한 Pretext task는 비디오에서 중요하지 않은 배경에 집중하는 문제가 발생 비디오에는 모션이 있음 이 모션에 집중하기 위한 학습 방법을 제안해야함 Related Work Pretext task 방법
지오메트릭한 정보를 배우는 spatial learning clip order를 학습하는 temporal learning space-time 정보를 학습하는 spatiotemporal learning 그러나 이 방법의 단점은 비디오에 리더던시 정보가 많아서 불필요한 학습을 야기함
배경에 대하여 정적이거나 무관한 정보는 모델의 판단성을 저해할 수 있음
배경 때문에 모델의 비디오 이해도가 낮아질 수 있음...</p></div><a class=entry-link aria-label="post link to [논문] Self-Supervised Video Representation Leraning with Motion-Contrastive Perception" href=https://russellgeum.github.io/posts/2022-04-30/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Deep Video Prior for Consistency and Propagation</h2></header><div class=entry-content><p>Motivation 비디오 프레임간 시간 불일치성을 해결하기 위해 DVP를 implcit하게 DNN에 주는 방법을 제안 DVP가 무엇인가?
비디오를 사용한 멀티모달 태스크에서는 성능의 흔들림이 심함 → 이터레티브하게 중요도를 재할당하는 전략으로 해결 Related Work 이전 비디오 연구들은 구축된 대규모 비디오 데이터셋이 필요했음 옵티컬 플로우 같은 정보나, 단순 프레임 간 유사도를 비교하는 것만으로는 롱-텀 비디오에 적합하지 않음 이전 비디오 연구들은 멀티 모달 태스크에서 좋은 성능을 골고루 보이기 어려웠음 Contribution DVP가 무엇인가?
DVP는 비디오 처리에서 임플리싯하게 비디오 일관성을 주기 위해 사용되는 성질들을 일컬음...</p></div><a class=entry-link aria-label="post link to [논문] Deep Video Prior for Consistency and Propagation" href=https://russellgeum.github.io/posts/2022-01-31/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio</h2></header><div class=entry-content><p>Motivation Can we train a single transformer model capable of processing multiple modalities and datasets, whilst sharing almost all of its learnable parameters?
Despite recent advances across different domains and tasks, current state-ofthe-art methods train a separate model with different model parameters for each task at hand.
Co-training PolyViT on multiple modalities and tasks leads to a model that is even more parameter-efficient, and learns representations that generalize across multiple domains....</p></div><a class=entry-link aria-label="post link to [논문] PolyViT, Co-training Vision Transformers on Images, Videos and Audio" href=https://russellgeum.github.io/posts/2021-11-20/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Decoupled Contrastive Learning</h2></header><div class=entry-content><p>Motivation CL의 로스에서 각 편미분에 대해 커플링되는 텀이 있음 → 이는 학습 효율성에 관여
Related Works CL은 학습에서 많은 양의 네거티브가 필요하다 → 큰 배치 사이즈를 요구로 함 → 이는 어쩔수없이 컴퓨터 리소스가 많이 필요 따라서 배치 사이즈에 민감하다. Contribution Infoloss를 각각 편미분 하였을때 공통된 텀이 포함되는 것을 보임 이 텀은 P/N 간 커플링 되는 것을 의미하고 학습 효율성에 영향을 줄 수 있음
예를 들어 N이 가깝고 P도 가까우면, N의 grad 또한 감소....</p></div><a class=entry-link aria-label="post link to [논문] Decoupled Contrastive Learning" href=https://russellgeum.github.io/posts/2021-11-01/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Masked Autoencoders Are Scalable Vision Learners</h2></header><div class=entry-content><p>Motivation 입력 이미지의 패치를 랜덤으로 마스킹한 상태에서 오토인코더 모델이 복원할 수 있을까?
비대칭 형태의 인코더 - 디코더
인코더 입력은 마스크 패치를 제외하고 visible 패치를 입력, 디코더는 이 latent vector를 가지고 원래의 이미지를 복원
인코더는 표준적인 ViT이고 디코더는 트랜스포머 블록으로 구성
Related Works 마스크 오토인코더는 디노이징 오토인코더의 일반적 형태
마스킹 입력으로 표현력을 끌어올리는 방법은 버트에서 선행되었지만, 비전에서 오토인코딩으로의 진전 X
저자의 질문, 비전과 자연어 사이에서 무엇이 마스크된 오토인코딩을 만드는가?
자연어는 인간이 만들어낸 상당히 시맨틱하고 높은 정보 밀도의 신호이다....</p></div><a class=entry-link aria-label="post link to [논문] Masked Autoencoders Are Scalable Vision Learners" href=https://russellgeum.github.io/posts/2021-12-31/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] SOFT: Softmax-free Transformer with Linear Complexity</h2></header><div class=entry-content><p>형식에 자유로운 간단 요약 NLP에서 리니어리티한 어텐션 게산은 비주얼 태스크에서 이론적으로, 실험적으로 어울리지 않음 기존의 리니어리티 어텐션 계산 한게는 소프트맥스를 고집하는 것에 있음 nomalization scaled dot-product 연산이 아니라, 가우시안 커널을 사용함 (왜?) 가우시안 커널로 대체하면, 어텐션 매트릭스를 low rank decomposition 가능하게 함 어떻게 근사하는지는 걱정마라, 뉴턴-랩슨 방법을 통한 무어 펜로즈 연산이 근사의 신뢰성을 보장한다. softmax는 어텐션에서 사실상 선택의 영역, 아무도 의심하지 않았음 그러나 선형화에 어울리는 연산이 아님 셀프 어텐션의 소프트맥스를 가우시안 커널로 대체 가우시안 커널 with 셀프 어텐션은 대칭임 모든 행렬이 0 ~ 1 사이 범위에 있음 대각 값은 가장 큰 값 (자기 자신과의 차이가 0이므로 가장 큼), 대부분 다른 페어는 0에 가까움 positive defiinite kernel이므로 gram matrix로 간주 가능 -> 선형화 없이 가우시안 커널 기반 셀프 어텐션을 사용하면 트랜스포머가 수렴에 실패하는 것을 발견 이런 어려움 때문에 소프트맥스 어텐션이 대중적인지 (잘 되니까 사용한다의 의미) 수렴과 쿼드라틱 복잡도를 해결하기 위해, matrix decomposition을 사용 Nystrom method를 low rank decomposition 방법으로 사용 (이 방법은 gram matrix decomposition을 위한 것) 내가 모르는 부분 왜 low rank decomposition이 선형화에 필요한지?...</p></div><a class=entry-link aria-label="post link to [논문] SOFT: Softmax-free Transformer with Linear Complexity" href=https://russellgeum.github.io/posts/2021-10-31/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Non Deep Networks</h2></header><div class=entry-content><p>Motivation DNN의 깊이가 깊어지면 단점이 많음 → 레이턴시가 길어지기 때문에 빠른 반응을 필요로 하는 애플리케이션이 부적합 어떻게 하면 얕은 깊이의 DNN으로도 충분한 성능을 낼 수 있을까? → 해답은 패러렐한 뉴럴넷 구성으로 성능을 낼 수 있다. Related Works 생략
Contribution 구체적으로 ~10 레이어, ~12 레이어까지 적절함을 말한다.
VGG 스타일의 블록을 사용한다. (구체적으로 Rep-VGG을 빌리지만, 목적에 맞게 조금 수정)
제한된 네트워크 깊이로 receptive field가 좁다. 이를 해결하기 위해, Squeeze-Exicitation 레이어에 기반한 SSE 레이어를 추가하였다....</p></div><a class=entry-link aria-label="post link to [논문] Non Deep Networks" href=https://russellgeum.github.io/posts/2021-10-20/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation with Compressed Video</h2></header><div class=entry-content><p>개요 비디오 압축 코덱 정보만으로 세그멘테이션 추론을 어떻게 빨리 할 수 있을까?
이전 연구의 한계 기존 VOS 태스크들은 정확하지만 속도가 느림 효율적인 방법들이 제시되었으나, 정확도 간의 트레이드오프가 있음 옵티컬 플로우 기반은 비용이 너무 비쌈, 그리고 two-view 밖에 못 봄 제안 방법 키프레임에서 다른 프레임으로 bidirectional, multi-hop 방식으로 세그멘테이션 마스크를 전달하여 워핑하는 네트워크 디자인
소프트 프로파게이션 모듈
부정확하고 블록 단위의 모션 벡터를 입력으로 받아서, 노이즈를 없앤 후 정확한 와핑을 할 수 있게 함...</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation with Compressed Video" href=https://russellgeum.github.io/posts/2021-08-10/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Contextual Transformer Networks for Visual Recognition</h2></header><div class=entry-content><p>Motivation 비전 태스크에서 셀프 어텐션의 계산이, 즉 공간적인 위치에서 Q, K가 서로 independent하게 계산이 되어지는 것이 단점 → context가 필요
Related Works CNN의 receptive field를 넓히는 것 → context를 잘 보긴 하지만, long range dependecy를 보지 못함 ViT, long range dependency를 보기는 하지만, independent한 Q, K의 interaction을 계산 Contribution 기존의 conventional self-attention은 서로 다른 위치간의 interaction을 잘 계산. 그러나 모든 pairwise Q-K relation은 independent함 → 풍부한 context를 보지 못함, 따라서 Conetxt Transformer 구조를 제안....</p></div><a class=entry-link aria-label="post link to [논문] Contextual Transformer Networks for Visual Recognition" href=https://russellgeum.github.io/posts/2021-07-30/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/>«&nbsp;Prev&nbsp;</a>
<a class=next href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>