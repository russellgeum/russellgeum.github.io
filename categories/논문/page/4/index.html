<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>논문 | Obiwan's BLOG</title>
<meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="논문"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Obiwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="논문"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="Obiwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>Obiwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>논문
<a href=/categories/%EB%85%BC%EB%AC%B8/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Self-Supervised Multi-Frame Monocular Scene Flow</h2></header><div class=entry-content><p>Motivation 모노큘라 시퀀스에서 3D scene flow 추정 문제는 근본적으로 ill-posed 문제 → 현재의 정확도에는 한계가 있고, 효율성 / 리얼 타임에도 문제가 있음
(Hur의 이전 연구) 이전에 제시한 모델의 성능과 real-time 이슈를 더 끌어 올리기 위한 연구
Related Works 이전에는 이미지 2장으로 태스크를 수행함 → 그러나 multiple consecutive frame이 리얼 월드 시나리오에 더 알맞음
물론 Joint learning을 하면서 또 multi-frame을 활용하긴 했으나, 실행 시간이 느린 문제 → 더 빠르게 만들 필요가 있음...</p></div><a class=entry-link aria-label="post link to [논문] Self-Supervised Multi-Frame Monocular Scene Flow" href=https://russellgeum.github.io/posts/review/2021-05-29/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Video Object Segmentation using Space-Time Memory Networks</h2></header><div class=entry-content><p>Motivation 준지도 학습에서 문제에 따라, 중간단계 예측과 함께 사용 가능한 정보는 풍부하다. 기존의 방법에서는 이러한 아이디어를 활용할 수 없었다. 논문은 메모리 네트워크를 통해, 가능한 모든 소수로부터 관련된 정보를 읽어 학습하고자 한다. 과거 프레임과 마스크가 외부 메모리를 형성하고, 현재 프레임이 쿼리로서 메모리 속 마스크 정보를 사용하여 세그멘테이션을 수행함 구체적으로 쿼리와 메모리는 피처 스페이스에서 매칭이 된다. (모든 space-time 픽셀 지점에서) Related Works 이전 프레임에서 형상을 추출하고 전파하는 방식 외관 변화에 더 잘 대처하나, 오클루션이나 에러 드리프트의 러버스트가 낮을 수 있다....</p></div><a class=entry-link aria-label="post link to [논문] Video Object Segmentation using Space-Time Memory Networks" href=https://russellgeum.github.io/posts/review/2021-05-12/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Multi-view Optimization of Local Feature Geometry</h2></header><div class=entry-content><p>Motivation 기존의 로컬 피처 디텍션은 싱글 이미지에서 이루어짐 → 에러가 누적되고 다운스트림 태스크에 악영향
Related Works 이전 논문들은 전통적인 방법이든 CNN 기반 방법이든, 싱글 뷰 이미지에서 로컬 피처 디텍션이 이루어졌다. 피처 매칭 단계에서 멀티 뷰를 고려하는 논문은 있지만, 저자가 아는 한, 더 정확한 키포인트 디텍팅을 위해 멀티뷰를 활용하는 사례는 없었다. Contribution 키포인트를 구성하는 그래프의 모든 엣지에 대해서 멀티뷰 refinement를 수행한다 이전 연구와 비슷하게 샴-네트워크와 코릴레이션 방법을 선택 파이널 플로우는 CNN, FCN을 통해 예측되어진다....</p></div><a class=entry-link aria-label="post link to [논문] Multi-view Optimization of Local Feature Geometry" href=https://russellgeum.github.io/posts/review/2021-04-25/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Stand-Alone Self-Attention in Vision Models</h2></header><div class=entry-content><p>Motivation 컴퓨터 비전에서 셀프 어텐션은 피처 스케일이 충분히 작아야 가능함 → 충분히 큰 피처맵에서도 셀프 어텐션 계산이 가능할까? 그리고 글로벌 어텐션은 계산량이 너무 많음 CNN이 없이 완전히 홀로 설 수 있는 셀프 어텐션 기반의 비전 모델을 제안 Related Work 이전에는 channel-wise, spatial-wise 등의 셀프 어텐션이 등장하였고, 적은 오버 헤드로 CNN 레이어 사이에 셀프 어텐션을 끼울 수 있었음 그러나 글로벌 어텐션 특성 상, 이미지 혹은 피처맵이 충분히 다운 샘플링 되어야 함 Contribution 모든 영역에서 어텐션을 계산하지 않음 → CNN의 로컬리티를 보증하면서도 어텐션을 계산할 수 있는 구조를 제안, 계산량을 줄일 수 있음 중심 픽셀을 쿼리로 두고, 그 주변 픽셀의 로컬 영역을 키와 밸류로 두어서 어텐션을 계산 Convolution STEM은 엣지 등의 정보를 파악하는 매우 중요한 요소...</p></div><a class=entry-link aria-label="post link to [논문] Stand-Alone Self-Attention in Vision Models" href=https://russellgeum.github.io/posts/review/2021-04-23/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Incorporating Convolution Design into Visual Transformers</h2></header><div class=entry-content><p>Motivation 트랜스포머는 대규모 데이터셋이 있을떄 CNN 모델에 필적하는 성능을 보임.
CNN의 로칼리티, 인덕티브 바이어스를 적극 활용하는 디자인의 트랜스포머 모델을 고안할 수 있을까?
Related Works ViT는 대규모 이미지 데이터셋을 이용해서 CNN에 필적하는 성능을 보임
→ 그러나 대규모 데이터는 컴퓨터 리소스의 요구가 크고, 훈련이 오래 걸림 DeiT는 잘 학습된 대규모 CNN 모델을 티처로 두고 KD를 통해, 비전 트랜스포머 모델을 학습시키려 고함
→ 이 역시 대규모 CNN 모델을 미리 준비해야한다는 단점 트랜스포머 태생이 인덕티브 바이어스를 반영하는 것이 어렵고, 불충분한 데이터로부터의 일반화 능력이 부족함...</p></div><a class=entry-link aria-label="post link to [논문] Incorporating Convolution Design into Visual Transformers" href=https://russellgeum.github.io/posts/review/2021-04-20/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Skip-Convolutions for Efficient Video Processing</h2></header><div class=entry-content><p>Motivation 비디오는 정지된 이미지의 연속일수도 있고, 변화하는 이미지의 연속일수도 있다.
우리는 세상을 비디오로 인지 → 즉, 변화를 인지 → 변화를 느낀다는 건, 프레임간 차이 (residual)이 누적되면서 어느 임계를 넘어가서 알아채는 것. 이러한 동기로 몇 가지 연구들이 있다. (뉴로모픽, 이벤트 카메라, SNN 등등) 그러나 아직까지가 주류가 아님.
Related Works 기존의 비디오 처리는 픽셀 레벨의 dense prediction을 요구하는 경우가 많음 → 모든 프레임을 모델에 넣어서 연산
프레임 수가 증가할수록 연산량 오버헤드가 리니어하게 증가 → 심지어 새로운 변화가 없어도 계산을 해야만 함...</p></div><a class=entry-link aria-label="post link to [논문] Skip-Convolutions for Efficient Video Processing" href=https://russellgeum.github.io/posts/review/2021-04-02/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</h2></header><div class=entry-content><p>Motivation 비디오로부터 spatio-temporal 표현의 대규모 연구를 보여준다. 최근의 네 가지 이미지 기반 프레임워크에 대한 통합된 관점과 함께, 시공간적 방법, 즉 비디오 데이터로 일반화할 수 있는 간단한 목표를 제시. 중요한 이미지 비지도 표현 학습 논문은 data augmentation을 통해 같은 이미지의 서로 다른 뷰들에서 유사도가 높은 피처를 찾아내는 것이 목표이다.
Contiribtuion 그런데 비디오는 자연적인 augmentation을 줄 수 있다. 모션, deformation, occlusion, illumination 등이다. (나의 이해: 비디오의 각 프레임들이 어떤 이미지의 augmentation. 이런 것들이 이어져서 temporal consistency를 만듬)...</p></div><a class=entry-link aria-label="post link to [논문] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning" href=https://russellgeum.github.io/posts/review/2021-04-01/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples</h2></header><div class=entry-content><p>Motivation MoCo 구조를 비디오 도메인으로 확장
Related Works 생략
Contribution Propose temporallly adversarial learning to improve the feature representation of the encoder
ConvLSTM을 통해 프레임 마스크를 출력 → Discriminator(encoder)를 통해
쿼리 피처와 프레임 피처를 출력 → 프레임이 같으면 0, 마스킹된 것은 차이가 최대
마스킹 프레임의 피처를 잘 배울 수 있도록 이 차이가 최대가 되도록 학습
Propose temporal decay to reduce the effect from historical keys in the memory queyes during contrastive learning...</p></div><a class=entry-link aria-label="post link to [논문] VideoMoCo, Contrastive Video Representation Learning with Temporally Adversarial Examples" href=https://russellgeum.github.io/posts/review/2021-03-30/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] CvT, Introducing Convolution to Vision Transformer</h2></header><div class=entry-content><p>Motivation 트랜스포머를 적용한 비전 모델은 더 적은 데이터로 학습하고 비슷한 사이즈의 ResNet보다 성능이 낮음. 그 이유는 비전 태스크에서 CNN이 가지는 장점을 ViT는 활용할 수 없음. 이미지는 pixel간 local correlation이 있고 CNN은 이걸 잘 잡아내는데, ViT는 이 능력이 부족함. 이러한 CNN의 local correlation에는 shift, scale, distortion invariance가 있음
Related Works 생략
Contribution 가장 큰 핵심은 트랜스포머의 MLP를 컨볼루션으로 대체한 것
Convolutional Token Embedding Layer
이전의 CvT 출력을 입력으로 받아서, 새로운 토큰을 만드는 함수 f를 정의....</p></div><a class=entry-link aria-label="post link to [논문] CvT, Introducing Convolution to Vision Transformer" href=https://russellgeum.github.io/posts/review/2021-03-25/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows</h2></header><div class=entry-content><p>Motivation 이 논문은 일반적인 컴퓨터 비전에서의 트랜스포머 백본을 제공하기 위함이다. 자연어처리에서의 트랜스포머가 비전으로 옮겨올 때, 두 도메인에서의 차이가 있었다.
하나는 비주얼 객체의 다양한 바리에이션이고 단어와 비교해서 이미지의 높은 해상도가 문제이다. → 이미지나 이미지 패치에 직접 트랜스포머를 적용하면 계산량이 쿼드라틱하게 증가한다.
Related Works 작년 10월, 구글의 ViT는 비전 태스크에 컴퓨터 비전 분야는 CNN가 지배적임. AlexNet부터 더 크고, 다양하고, 정교한 기술들로 CNN backbone들이 발전함. 한편 자연어 분야는 트랜스포머가 지배적 → 트랜스포머는 데이터의 long range dependency를 잘 반영함 (언어의 특징)....</p></div><a class=entry-link aria-label="post link to [논문] Swin Transformer, Hierarchical Vision Transformer using Shitfed Windows" href=https://russellgeum.github.io/posts/review/2021-03-15/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/3/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/5/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>Obiwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>