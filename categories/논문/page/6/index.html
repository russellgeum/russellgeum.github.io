<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>논문 | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="논문"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="논문"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5iwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5iwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>논문
<a href=/categories/%EB%85%BC%EB%AC%B8/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] A Simple Framework for Contrastive Learning of Visual Representations</h2></header><div class=entry-content><p>용어의 정의 Pretext task: represenation learning을 위해 수행되는 태스크 Downstream task: pretext task로 얻은 파라미터를 동해 본격 풀고자 하는 문제를 푸는 것 Motivation 모델의 표현력을 극대로 끌어올리는 방법에 대한 연구, 특히 이를 효율적으로 할 수 있을까?
Related Work Visual representation learning의 non supervision 관점에서 두 가지 메인스트림이 있음
Generative
이 방식은 계산량이 많음, 그리고 representation learning이 꼭 필요하지는 않음 Discriminative supervised learning에서 사용된 방법과 비한 오브젝티브 펑션이 있고, 이를 통해 reprsentation을 학습함 그러나 unlabeld dataset으로부터 얻은 label과 input 사이에서 pretext task를 수행해야함 최근의 discriminative 방식은 contrastive learning에 근거한 방법이 많음 (CPC, CMC, CPC v2 등등) Contribution representation learning에서 data augmentation에 대한 체계적인 고민이 없었음....</p></div><a class=entry-link aria-label="post link to [논문] A Simple Framework for Contrastive Learning of Visual Representations" href=https://russellgeum.github.io/posts/research/2021-03-01/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer</h2></header><div class=entry-content><p>Motivation UniT는 비전에서부터 자연어까지 명백히 다른 도메인의 태스크들을 동시에 학습하는 모델이다. 모달리티 입력을 인코딩해서, 디코더를 통해 각 태스크에 맞는 예측을 진행한다. 각 태스크에 맞는 로스와 함께 엔드 투 엔드로 학습을 한다. 핵심은 이전 논문과는 다르게 이 모델은 태스크 스페시픽한 파인튜닝 없이도 모델의 파라미터를 공유한다. 그럼에도 불구하고 서로 다른 도메인 문제를 핸들링할 수 있다. 트랜스포머는 자연어나 비전에서의 다운스트림 태스크에 매우 큰 성능을 보여주고 있다. 최근에 비전 + 자연어 태스크에서 좋은 성능을 보여주었지만, 아직까지 트랜스포머를 통해 서로 다른 도메인 태스크를 연결하는 시도는 잘 없었다....</p></div><a class=entry-link aria-label="post link to [논문] Transformer is All You need, Multimodal Multitask Learning with a Unified Transformer" href=https://russellgeum.github.io/posts/research/2021-02-25/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] TCLR: Temporal Contrastive Learning for Video Representation</h2></header><div class=entry-content><p>Motivation 비디오 레프리젠테이션 러닝에서 쓰일만한 두 가지 콘트라스티브 러닝 프레임워크를 제안. 첫 번째 로스는 같은 비디오에서 겹치지 않는 클립 간의 콘트라스티브 러닝. 두 번째 로스는 피처의 시간적 다양성을 위해서 입력 클립의 피처맵에서 타임 스탬프 간을 구분하는 콘트라스티브 러닝. 좋은 표현 학습은 다운스트림 태스크의 성능을 좋게 만들 수 있음
Related Work 생략
Contribution Local-Local Temporal Contrastive Learning
같은 비디오에서 같은 local timestamp의 (augmentation를 먹이더라도) 비디오 클립은 서로 attract 같은 비디오에서 다른 local timestamp의 비디오 클립은 서로 repel...</p></div><a class=entry-link aria-label="post link to [논문] TCLR: Temporal Contrastive Learning for Video Representation" href=https://russellgeum.github.io/posts/research/2021-02-22/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Spatiotemporal Contrastive Video Representation Learning</h2></header><div class=entry-content><p>Motivation 비디오의 비지도 표현 학습을 위해, 시간-공간적 맥락에서 contrastive learning을 적용 풍부한 표현 학습을 위해 효과적인 spatial-tempral augmentation 방법을 연구 Related Work 생략
Contribution Contrasitve learning
임베딩 스페이스의 피처 벡터들을 쫙 나열한 다음에 유사한 피처들은 거리가 가깝게끔 학습 (유사도가 낮은 것은 거리가 먼 것이므로 패널티를 주지 않음) 이를 통해서 같은 비디오의 tempral distant가 있는 두 비디오 클립의 encoder는 attract하고, 다른 비디오는 repel하게끔 학습 (SimCLR 참고)
Temporal sampling strategy, consistenc spatial augmentation...</p></div><a class=entry-link aria-label="post link to [논문] Spatiotemporal Contrastive Video Representation Learning" href=https://russellgeum.github.io/posts/research/2021-02-15/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</h2></header><div class=entry-content><p>Motivation 기존 뎁스 추정은 correspondence estimation으로 풀었다. 그러나 이 과정에는 문제가 있음
Conventional 방법은 텍스쳐가 약하거나, non-Lambertian 표면에서 문제가 생김 딥러닝 기반은 뎁스 consistency가 일정하지 않고, photometric consistency에서 3D 정보를 제대로 반영하지 못하는 문제 이 논문은 NeRF의 힘을 빌려, 멀티 뷰 스테레오 뎁스 추정을 하고자 함
correspondence estimation과 corr view depth reprojection 최적화 대신에, 이 논문은 다이렉트로 부피를 최적화함 → 그런데 NeRF에서는 shape-radiance ambiguity 문제가 있음. 이를 해결하기 위해 뎁스 프라이어 기반의 NeRF 훈련 가이던스를 제안함 Related Work Conventional 방법은 textuless, non-lambertian 표면에서 문제가 발생 Learning based 방법은 depth consistency 일정하지 않고, photometric loss가 3D 정보를 반영하지 못함 Contribution Depth consistency를 해결하기 위한 제안 방법...</p></div><a class=entry-link aria-label="post link to [논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions" href=https://russellgeum.github.io/posts/research/2021-09-31/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/5/>«&nbsp;Prev&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>