<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>논문 | 5biwan's BLOG</title>
<meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://russellgeum.github.io/apple-touch-icon.png><link rel=mask-icon href=https://russellgeum.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><meta name=theme-color content="#ffffff"><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="논문"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="5biwan's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="논문"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="5biwan's BLOG (Alt + H)"><img src=https://russellgeum.github.io/icon.png alt aria-label=logo height=20>5biwan's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>논문
<a href=/categories/%EB%85%BC%EB%AC%B8/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds</h2></header><div class=entry-content><p>1. Motivation CLAP(Contrastive Language-Audio Pre-training) 모델은 제로샷 오디오 분류(ZSAC) 작업에서 우수한 성능을 보이지만, 여전히 표준 지도학습 방법보다 성능이 낮다. 이는 다음 세 가지 주요 이유 때문이다:
대규모 오디오-캡션 데이터셋 접근의 한계: CLAP은 CLIP과 달리 대규모 오픈소스 오디오-캡션 데이터셋으로 훈련되지 않았기 때문에 다양한 오디오와 언어 상호작용을 완전히 이해하는 능력이 제한된다. 훈련 카테고리 레이블 너머의 일반화 부족: CLAP은 훈련에 사용된 특정 카테고리 레이블을 넘어 일반화하는 데 어려움을 겪는다. 예를 들어, AudioSet에서 “Sound of a toothbrush"로 훈련된 모델이 ESC50 데이터셋의 “brushing teeth"와 같은 유사한 레이블에 정확히 일반화하지 못할 수 있다....</p></div><a class=entry-link aria-label="post link to [논문] ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds" href=https://russellgeum.github.io/posts/research/2025-05-16/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] A Multi-Resolution Front-End for End-to-End Speech Anti-Spoofing</h2></header><div class=entry-content><p>1. Motivation 기존 음성 신호 분류 작업에서 시간-주파수 해상도의 최적 선택은 성능에 중요한 영향을 미치지만,
어떤 해상도가 가장 적합한지는 명확하지 않다.
특히, 스푸핑 방지를 위한 음성 분류에서는 다양한 시간-주파수 스케일이 필요하다.
기존 연구는 고정된 해상도에서 작업하여 정보 손실 가능성이 있으며,
이는 분류 성능을 제한할 수 있다.
이 논문은 다중 해상도 기반의 전처리(front-end) 방식을 제안하여 이러한 문제를 해결하고자 한다.
2. Related Work 기존 연구에서는 다중 해상도 또는 다중 스케일 구조를 통해 분류 성능을 개선하려는 시도가 있었다....</p></div><a class=entry-link aria-label="post link to [논문] A Multi-Resolution Front-End for End-to-End Speech Anti-Spoofing" href=https://russellgeum.github.io/posts/research/2024-12-22/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] MATPC: Masked Latent Prediction and Classification for Self Supervised Audio Representation Learning</h2></header><div class=entry-content><p>MATPAC: Masked Latent Prediction and Classification for Self Supervised Audio Representation Learning 1. Motivation 최근 마스크 잠재 예측(masked latent prediction)에 기반한 자기지도 학습(SSL) 방법들이 입력 데이터를 강력한 표현으로 인코딩하는 데 효과적임이 입증되었다. 그러나 학습 과정에서 학습된 잠재 공간을 더 높은 수준의 정보를 추출하도록 변환하면 다운스트림 분류 작업에 더 적합할 수 있다. 이 논문은 두 가지 사전 작업(pretext task)을 결합하여 오디오 표현 학습의 성능을 향상시키는 새로운 방법론인 MATPAC(MAsked latenT Prediction And Classification)을 제안한다....</p></div><a class=entry-link aria-label="post link to [논문] MATPC: Masked Latent Prediction and Classification for Self Supervised Audio Representation Learning" href=https://russellgeum.github.io/posts/research/2025-04-15/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Sparse Binarization for Fast Keyword Spotting</h2></header><div class=entry-content><p>1. Motivation 음성 기반 디바이스와 애플리케이션의 증가로 키워드 스포팅(Keyword Spotting, KWS)은 실시간 음성 인식을 가능하게 하며, 엣지 디바이스에서의 프라이버시와 대역폭 효율성을 높인다.
엣지 디바이스는 메모리와 연산 속도가 제한되어 있어 KWS 모델의 경량화와 최적화가 필수적이다.
이 논문에서는 효율적이고 정확한 KWS를 위한 새로운 방법으로 Sparse Binarization을 기반으로 한 모델 SparkNet을 제안한다.
SparkNet은 기존 최첨단(SOTA) 모델 대비 4배 빠르면서도 더 높은 정확도를 제공하며, 소음 환경에서도 더 강력한 성능을 보여준다.
2. Related Work Keyword Spotting (KWS) KWS는 음성 데이터를 실시간으로 분석해 특정 단어를 탐지하는 기술이다....</p></div><a class=entry-link aria-label="post link to [논문] Sparse Binarization for Fast Keyword Spotting" href=https://russellgeum.github.io/posts/research/2024-12-18/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Keyword Transformer: A Self-Attention Model for Keyword Spotting</h2></header><div class=entry-content><p>1. Motivation Transformer 구조는 자연어 처리뿐만 아니라 이미지 처리와 음성 인식 등 다양한 도메인에서 성공적으로 사용되고 있다.
하지만 키워드 스포팅 분야에서는 주로 Transformer가 기존의 CNN이나 RNN 같은 구조 위에 추가적으로 사용되어 왔다.
이를 해결하기 위해, 이 논문은 키워드 스포팅에 Transformer를 직접 적용하는 모델인 Keyword Transformer(KWT)를 제안한다.
KWT는 별도의 사전 학습이나 추가 데이터를 필요로 하지 않으면서 기존의 복잡한 혼합 구조보다 뛰어난 성능을 보이며, Google Speech Commands 데이터셋에서 최고 수준의 정확도를 달성했다....</p></div><a class=entry-link aria-label="post link to [논문] Keyword Transformer: A Self-Attention Model for Keyword Spotting" href=https://russellgeum.github.io/posts/research/2024-12-17/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] BEATS : Audio Pre-Training with Acoustic Tokenizercategories</h2></header><div class=entry-content><p>1. Motivation 최근 자기지도학습(SSL)은 언어, 비전, 음성에서 큰 성과를 보여주고 있지만,
오디오 도메인에서는 여전히 복원 손실(reconstruction loss)이 주로 사용되고 있다.
복원 손실은 저수준 시간-주파수 특징을 재현하는 데 초점이 맞춰져,
고수준의 의미 정보를 제대로 반영하지 못하는 한계가 있다.
BEATS는 연속적인 오디오 데이터를 이산적(discrete) 라벨로 변환해
고수준의 의미적 정보를 학습하는 새로운 프레임워크를 제안한다.
이를 통해 기존 방식보다 효율적이고 의미 중심적인 학습이 가능하도록 한다.
2. Related Work 오디오 사전 학습은 크게 지도 학습과 자기지도학습으로 나뉜다....</p></div><a class=entry-link aria-label="post link to [논문] BEATS : Audio Pre-Training with Acoustic Tokenizercategories" href=https://russellgeum.github.io/posts/research/2024-11-24/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Filterbank Learning for Noise-Robust Small-Footprint Keyword Spotting</h2></header><div class=entry-content><p>Filterbank Learning for Noise-Robust Small-Footprint Keyword Spotting 1. Motivation 키워드 스포팅(KWS)은 보통 Log-Mel이나 MFCC 같은 수작업 특징을 사용함. 학습 가능한 필터뱅크가 기존 특징을 대체하려는 시도는 있었으나, 큰 성과는 없었음. 필터뱅크 채널 수를 줄이면 학습된 필터뱅크가 성능을 유지하면서도 에너지 소비를 크게 줄일 수 있다는 것을 주장함. 항상 켜져 있는 저자원 KWS 시스템에 특히 중요함. 2. Related Works SincNet: 학습 가능한 필터뱅크를 CNN 기반 KWS에 적용한 연구. 수작업 특징과의 직접 비교는 부족했음. 기존 연구에서는 Log-Mel과 MFCC가 여전히 더 우수하다고 결론지음....</p></div><a class=entry-link aria-label="post link to [논문] Filterbank Learning for Noise-Robust Small-Footprint Keyword Spotting" href=https://russellgeum.github.io/posts/research/2024-11-18/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Noise-Robust Keyword Spotting throught Self-Supervised Pretraikning</h2></header><div class=entry-content><p>Noise-Robust Keyword Spotting throught Self-Supervised Pretraikning 1. Motivation 현대의 음성 비서는 거의 모든 컴퓨터와 스마트 기기에서 사용 가능함 음성 비서는 ASR(자동 음성 인식) 모델을 사용하지만, 이는 계산 비용이 높아 작은 기기에서 실행하기 어려움 대신 키워드 스팟팅(KWS) 알고리즘을 통해 특정 키워드가 발화되었을 때 ASR을 활성화함 현재 최신 KWS 모델들은 지도학습 방식으로 학습되어 많은 양의 레이블된 데이터가 필요한 상황 레이블이 없는 데이터를 활용할 수 있는 자기지도학습의 활용이 필요한 상황 2. Related Works Data2Vec 프레임워크를 사용한 transformer 기반 KWS 모델의 사전학습이 성능 향상에 도움이 된다는 연구가 있었음 하지만 이전 연구는 깨끗한 오디오 입력만을 가정했고, 실제 환경의 노이즈는 고려하지 않았음 ASR 분야에서는 자기지도학습을 통한 노이즈 강건성 연구가 진행되어 왔음 KWS에서는 대부분 지도학습 기반의 multi-style training이나 adversarial training 방식으로 노이즈 강건성을 확보 3....</p></div><a class=entry-link aria-label="post link to [논문] Noise-Robust Keyword Spotting throught Self-Supervised Pretraikning" href=https://russellgeum.github.io/posts/research/2024-11-17/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Efficient Large Language Models</h2></header><div class=entry-content><p>Efficient Large Language Models Introduction 본 글은 Yizhang Jin et al “Efficient Multimodal Large Language Models” 서베이에 기반한다.
2023년 중후반부터 멀티모달 기반 대형 언어 모델(Multimodal Large Language Models, MLMMs)의 발전은 텍스트 기반을 넘어 시각적 이해 및 추론 작업에서 놀라운 성과를 보였다. 그러나 LLM과 마찬가지로 모델 크기가 매우 크고, 훈련 및 추론 비용이 높아 학계와 산업계에서 광범위한 응용을 제한시켰다. 이에 따라 로컬 장치, 엣지 컴퓨팅 등의 요구 사항을 충족하기 위해 효율적이고 경량화된 MLMM을 연구하는 시도가 많아졌다....</p></div><a class=entry-link aria-label="post link to [논문] Survey: Efficient Large Language Models" href=https://russellgeum.github.io/posts/research/2024-07-29/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Speculative Decoding</h2></header><div class=entry-content><p>개요 이 글은 스퀴즈비츠의 김태수님이 발표한 내용으로 두 논문을 정리하였다.
LLM에 토큰을 하나씩 생성할 때마다 굉장히 많은 weight를 불러와야 한다. 그래서 DRAM bandwidth가 문제가 된다. Autoregressive 방식이 GPU를 완전하 활용하지 못하는 문제가 발생한다. 이를 해결하기 위한 방법 중 하나로 Speculative Decoding이 있다. Speculative Decoding은 1개의 프롬프트를 1 배치로 처리하는 것이 아니라, 예측한 여러 토큰들을 동시에 재입력하여 병렬 처리하는 기술이다. 따라서 모델은 여러 입력 문장을 배치 단위로 처리한다.
Speculative Decoding 이 논문은 Draft, Verification을 단순하게 구현하여 최적의 토큰을 찾는다....</p></div><a class=entry-link aria-label="post link to [논문] Speculative Decoding" href=https://russellgeum.github.io/posts/research/2024-05-23/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://russellgeum.github.io/>5biwan's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>