<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>논문 | Oppenheimer's BLOG</title>
<meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0b9997834f48352dbb30268ded49b3e4c6c99fe4bf2c63e280332891535a5192.css integrity="sha256-C5mXg09INS27MCaN7Umz5MbJn+S/LGPigDMokVNaUZI=" rel="preload stylesheet" as=style><link rel=icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://russellgeum.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/index.xml><link rel=alternate hreflang=en href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="논문"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/"><meta property="og:image" content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Oppenheimer's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://russellgeum.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="논문"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://russellgeum.github.io/ accesskey=h title="Oppenheimer's BLOG (Alt + H)"><img src=https://russellgeum.github.io/apple-touch-icon.png alt aria-label=logo height=20>Oppenheimer's BLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://russellgeum.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://russellgeum.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>논문
<a href=/categories/%EB%85%BC%EB%AC%B8/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Layer Sharing for Parameter-Efficient Transformer</h2></header><div class=entry-content><p>개요 이 글은 Qualcomm의 심규홍 박사님이 발표해주신 자료를 토대로 작성한다.
대상: 트랜스포머를 이해하고 있는 개발자들을 위한 세미나
Motivation 트랜스포머 애플리케이션은 서버 베이스 모델에서는 활발하게 사용되고 있다. 이제 모바일 베이스로 들어갈려고 한다. 트랜스포머는 scaling-law를 따른다. 더 크고, 더 많이 쌓을수록 더 좋은 성능이 나온다. 따라서 돈을 들이면 성능이 보장된다. 그 예시가 LLM이다. 그러나 Efficiecy 관점에서 충분히 고민을 해봐야할 문제가 많다. RAM 사이즈, NPU 퍼포먼스, Cache 사이즈 등 고려해야 할 사항이 많다. On-device LLM에 대한 사이즈가 어느정도 적절할까?...</p></div><a class=entry-link aria-label="post link to [논문] Layer Sharing for Parameter-Efficient Transformer" href=https://russellgeum.github.io/posts/review/2024-04-04/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Multimodal Models</h2></header><div class=entry-content><p>개요 최근 대형 언어 모델은 멀티모달과 결합한 방향으로 변하고 있다. 구현 방식에 몇 가지 유형이 있지만, 공통적으로 멀티모달 데이터 임베딩을 자연어 임베딩 공간으로 매핑한 후, 이를 언어 모델 추론을 위한 입력으로 활용한다. 대형 멀티모달 모델의 큰 접근은 아래와 같다.
중요한 트렌드 멀티모달 이해에서 생성으로 그리고 모달리티 간의 변환 (Any-to-Any)
(예시: MiniGPT-4 → MiniGPT-5 → NExT-GPT) Pre-Training - Supervised Fine-Tuning - RLHF으로의 훈련 파이프라인
(예시: BLIP-2 → InstructBLIP → DRESS) 다양한 모달리티으로의 확장...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Multimodal Models" href=https://russellgeum.github.io/posts/review/2024-02-04/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Efficient Large Language Models</h2></header><div class=entry-content><p>개요 대규모 언어 모델은 자연어 이해, 생성, 복잡한 추론과 같은 작업에서 뛰어난 능력을 보여주었다. 그러나 대규모 언어 모델은 막대한 하드웨어 리소스가 필요하고, 효율성을 위한 기술 개발의 니즈가 발생하였다. 이 기술 동향은 효율적인 대규모 언어 모델을 위해 몇 가지 기술 분류와 최근 동향을 제안한다.
Model Compression Weight-Only Quantization (PTQ) GPTQ: Accurate Quantization for Generative Pre-trained Transformers, [Paper] [Code] ICLR, 2023
QuIP: 2-Bit Quantization of Large Language Models With Guarantees, [Paper] [Code] arXiv, 2023...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Efficient Large Language Models" href=https://russellgeum.github.io/posts/review/2024-01-07/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] ICCV 2023 관심 논문 리스트업</h2></header><div class=entry-content><p>ICCV 2023 ICCV 2023 Link
Papers ICCV 2023이 열리고 있다. NeRF, Multimodal/VQA, Model Compression 위주로 트래킹한다.
(일부 특이한 연구도 포함)
Neural Radiance Fields NeRF-MS: Neural Radiance Fields with Multi-Sequence
Peihao Li et al.
Re-ReND: Real-time Rendering of NeRFs across Devices
Sara Rojas et al.
CLNeRF: Continual Learning Meets NeRF
Zhipeng Cai, Matthias Muller
Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction
Hansheng Chen et al.
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields...</p></div><a class=entry-link aria-label="post link to [논문] ICCV 2023 관심 논문 리스트업" href=https://russellgeum.github.io/posts/review/2023-10-03/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Language Models Compression</h2></header><div class=entry-content><p>대규모 언어 모델의 경량화 동향 Abstract LLM은 거대한 크기와 계산량으로 인해, 리소스 제한적인 환경에서의 배포를 어렵게 만듬 LLM의 압축이 중요한 분야임. 이 서베이는 LLM 압축 기술의 많은 자료를 제공함 Quantization, Pruning, KD 등 다양한 방법론을 탐구하며, 최신 연구와 접근법을 보여줌 압축된 LLM을 평가하기 위한 메트릭에 대한 조사도 진행함 Introduction & Method 대규모 언어 모델은 다양한 태스크에서 뛰어난 능력을 보여주고 있다. 그럼에도 모델의 방대한 크기와 요구되는 계산량때문에 배포에서 많은 어려움이 따른다. 2020년의 GPT-175B 모델은 1,750억 개 파라미터이다....</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Language Models Compression" href=https://russellgeum.github.io/posts/review/2023-08-29/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Survey: Large Language Models</h2></header><div class=entry-content><p>LLM 동향 LLM 모델 그 자체부터 응용/생산성을 위한 갖가지 방법을 섞은 모델, 기술들이 계속 나오고 있다. 그러다가 근래에는 그 정도가 사그라든 느낌이 드는데, 이 틈이 딱 공부하기 좋은 시기라고 생각한다. LLM에 관련한 모든 논문은 볼 수 없어도, 히스토리나 최근의 동향을 볼 수 있는 서베이 논문이 많이 나와서 리스트업한다. 시간날 때 읽어보면 각 분야의 개별 연구자 및 개발자들이 어떤 시각으로 LLM을 활용하거나 바라보는지 최신 연구들을 추적할 기회이다.
Large Language Models Survey on Large Language Models...</p></div><a class=entry-link aria-label="post link to [논문] Survey: Large Language Models" href=https://russellgeum.github.io/posts/review/2023-08-28/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] ICML 2023 관심 논문 리스트업</h2></header><div class=entry-content><p>ICML 2023 Papers ICML 2023이 열리고 있다.
Distillation, Quantization, HW-aware Deep Learning 위주로 트래킹 중이다.
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models
Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan
DIVISION: Memory Efficient Training via Dual Activation Precision
Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Na Zou, Xia Hu
Fast Private Kernel Density Estimation via Locality Sensitive Quantization
Tal Wagner, Yonatan Naamad, Nina Mishra...</p></div><a class=entry-link aria-label="post link to [논문] ICML 2023 관심 논문 리스트업" href=https://russellgeum.github.io/posts/review/2023-07-25/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Pruning vs Quantization: Which is Better?</h2></header><div class=entry-content><p>Paper Link Andrey Kuzmin et al (Qualcomm AI Research)
Introduction 이 논문은 딥러닝 모델 압축에서 Quantization과 Pruning이 무엇이 어떤 경우에 더 우수한지의 정량적 실험 결과를 리포트한다.
Motivation 양자화와 프루닝은 모두 비슷한 시기에 시작되어 발전하였다. 그러나 아직까지 올바른 비교는 (저자가 아는 한) 없었다고 주장한다. 본 연구의 리포트가 앞으로 딥러닝 추론 하드웨어 디자인 결정에 도움이 되기를 희망한다. 이 논문은 실험을 위해 몇 가지 강력한 가정을 사용한다. 먼저 FP16 데이터 타입을 기준으로 삼는다. 일반적으로 딥러닝 추론 성능의 정확도를 떨어트리지 않는 마지노선이라 주장하고, 신경망은 매우 흔하게 FP16 타입에서 학습되기 때문이다....</p></div><a class=entry-link aria-label="post link to [논문] Pruning vs Quantization: Which is Better?" href=https://russellgeum.github.io/posts/review/2023-07-23/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] SqueezeLLM: Dense-and-Sparse Quantization</h2></header><div class=entry-content><p>Paper Link Sehoon Kim et al (UC Berkeley)
Introduction This paper proposes a Psuedo-PTQ method considering the weight distribution of LLM and outliers.
Motivation Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. Deploying LLMs for inference has been a significant challenge due to their unprecedented resource requirements. AThis has forced existing deployment frameworks to use multi-GPU inference pipelines, or to use smaller and less performant models....</p></div><a class=entry-link aria-label="post link to [논문] SqueezeLLM: Dense-and-Sparse Quantization" href=https://russellgeum.github.io/posts/review/2023-07-19/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>[논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions</h2></header><div class=entry-content><p>개요 호모그래피는 스테레오 비전의 근본이다. 영상이 대략 회전 모션이거나 장면이 평면 표면에 가까우면 호모그래피 행렬을 근사할 수 있다. 장면이 제약 조건을 만족하면 직접 호모그래피를 계산할 수 있다. 시맨텍 어웨어하고 러버스트한 호모그래피 추정 딥러닝 알고리즘을 개발 이전 연구의 한계 생략
제안 방법 두 이미지를 인코더
레즈넷34 백본을 받아서 3x3, 8DoF의 호모그래피 행렬을 추정
호모그래피 추정을 위해 Triplet Loss를 사용
호모그래피 추정이 완벽하다면 호모그래피를 통한 와핑이 잘 되어야 함 그래서 와핑한 피처 혹은 이미지가 타겟 피처 또는 이미지와 잘 얼라인 되어야 함 두 번째 로스 텀은 잘 모르겠음 호모그래피 a→b에서 b→a는 identity로 레귤라이저를 추가함 Content-aware prob map...</p></div><a class=entry-link aria-label="post link to [논문] Content-aware Unsupervised Deep Homography Estimation and Its Enxtensions" href=https://russellgeum.github.io/posts/review/2022-07-31/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://russellgeum.github.io/categories/%EB%85%BC%EB%AC%B8/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://russellgeum.github.io/>Oppenheimer's BLOG</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>